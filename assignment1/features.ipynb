{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image features exercise\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "We have seen that we can achieve reasonable performance on an image classification task by training a linear classifier on the pixels of the input image. In this exercise we will show that we can improve our classification performance by training linear classifiers not on raw pixels but on features that are computed from the raw pixels.\n",
    "\n",
    "All of your work for this exercise will be done in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "Similar to previous exercises, we will load CIFAR-10 data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "from cs231n.features import color_histogram_hsv, hog_feature\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # Subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features\n",
    "For each image we will compute a Histogram of Oriented\n",
    "Gradients (HOG) as well as a color histogram using the hue channel in HSV\n",
    "color space. We form our final feature vector for each image by concatenating\n",
    "the HOG and color histogram feature vectors.\n",
    "\n",
    "Roughly speaking, HOG should capture the texture of the image while ignoring\n",
    "color information, and the color histogram represents the color of the input\n",
    "image while ignoring texture. As a result, we expect that using both together\n",
    "ought to work better than using either alone. Verifying this assumption would\n",
    "be a good thing to try for the bonus section.\n",
    "\n",
    "The `hog_feature` and `color_histogram_hsv` functions both operate on a single\n",
    "image and return a feature vector for that image. The extract_features\n",
    "function takes a set of images and a list of feature functions and evaluates\n",
    "each feature function on each image, storing the results in a matrix where\n",
    "each column is the concatenation of all feature vectors for a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done extracting features for 1000 / 49000 images\n",
      "Done extracting features for 2000 / 49000 images\n",
      "Done extracting features for 3000 / 49000 images\n",
      "Done extracting features for 4000 / 49000 images\n",
      "Done extracting features for 5000 / 49000 images\n",
      "Done extracting features for 6000 / 49000 images\n",
      "Done extracting features for 7000 / 49000 images\n",
      "Done extracting features for 8000 / 49000 images\n",
      "Done extracting features for 9000 / 49000 images\n",
      "Done extracting features for 10000 / 49000 images\n",
      "Done extracting features for 11000 / 49000 images\n",
      "Done extracting features for 12000 / 49000 images\n",
      "Done extracting features for 13000 / 49000 images\n",
      "Done extracting features for 14000 / 49000 images\n",
      "Done extracting features for 15000 / 49000 images\n",
      "Done extracting features for 16000 / 49000 images\n",
      "Done extracting features for 17000 / 49000 images\n",
      "Done extracting features for 18000 / 49000 images\n",
      "Done extracting features for 19000 / 49000 images\n",
      "Done extracting features for 20000 / 49000 images\n",
      "Done extracting features for 21000 / 49000 images\n",
      "Done extracting features for 22000 / 49000 images\n",
      "Done extracting features for 23000 / 49000 images\n",
      "Done extracting features for 24000 / 49000 images\n",
      "Done extracting features for 25000 / 49000 images\n",
      "Done extracting features for 26000 / 49000 images\n",
      "Done extracting features for 27000 / 49000 images\n",
      "Done extracting features for 28000 / 49000 images\n",
      "Done extracting features for 29000 / 49000 images\n",
      "Done extracting features for 30000 / 49000 images\n",
      "Done extracting features for 31000 / 49000 images\n",
      "Done extracting features for 32000 / 49000 images\n",
      "Done extracting features for 33000 / 49000 images\n",
      "Done extracting features for 34000 / 49000 images\n",
      "Done extracting features for 35000 / 49000 images\n",
      "Done extracting features for 36000 / 49000 images\n",
      "Done extracting features for 37000 / 49000 images\n",
      "Done extracting features for 38000 / 49000 images\n",
      "Done extracting features for 39000 / 49000 images\n",
      "Done extracting features for 40000 / 49000 images\n",
      "Done extracting features for 41000 / 49000 images\n",
      "Done extracting features for 42000 / 49000 images\n",
      "Done extracting features for 43000 / 49000 images\n",
      "Done extracting features for 44000 / 49000 images\n",
      "Done extracting features for 45000 / 49000 images\n",
      "Done extracting features for 46000 / 49000 images\n",
      "Done extracting features for 47000 / 49000 images\n",
      "Done extracting features for 48000 / 49000 images\n"
     ]
    }
   ],
   "source": [
    "from cs231n.features import *\n",
    "\n",
    "num_color_bins = 10 # Number of bins in the color histogram\n",
    "feature_fns = [hog_feature, lambda img: color_histogram_hsv(img, nbin=num_color_bins)]\n",
    "X_train_feats = extract_features(X_train, feature_fns, verbose=True)\n",
    "X_val_feats = extract_features(X_val, feature_fns)\n",
    "X_test_feats = extract_features(X_test, feature_fns)\n",
    "\n",
    "# Preprocessing: Subtract the mean feature\n",
    "mean_feat = np.mean(X_train_feats, axis=0, keepdims=True)\n",
    "X_train_feats -= mean_feat\n",
    "X_val_feats -= mean_feat\n",
    "X_test_feats -= mean_feat\n",
    "\n",
    "# Preprocessing: Divide by standard deviation. This ensures that each feature\n",
    "# has roughly the same scale.\n",
    "std_feat = np.std(X_train_feats, axis=0, keepdims=True)\n",
    "X_train_feats /= std_feat\n",
    "X_val_feats /= std_feat\n",
    "X_test_feats /= std_feat\n",
    "\n",
    "# Preprocessing: Add a bias dimension\n",
    "X_train_feats = np.hstack([X_train_feats, np.ones((X_train_feats.shape[0], 1))])\n",
    "X_val_feats = np.hstack([X_val_feats, np.ones((X_val_feats.shape[0], 1))])\n",
    "X_test_feats = np.hstack([X_test_feats, np.ones((X_test_feats.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SVM on features\n",
    "Using the multiclass SVM code developed earlier in the assignment, train SVMs on top of the features extracted above; this should achieve better results than training SVMs directly on top of raw pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration with rate 4.862602e-07 and regstrength 2.488512e+06\n",
      "iteration 0 / 1000: loss 3847.354235\n",
      "iteration 100 / 1000: loss 11211418199382964575621342849662976.000000\n",
      "iteration 200 / 1000: loss 32747247803862622162651429851889752710978613872528113015132782592.000000\n",
      "iteration 300 / 1000: loss 95650899793088183258736119657243467928302758173929928621905415195215652818617478142072788615168.000000\n",
      "iteration 400 / 1000: loss 279385146685464034325494603600159951833595076103691382204705808831377854063955561762537553482114382355616698616287730457378816.000000\n",
      "iteration 500 / 1000: loss 816051499330470777592078825712060356776096257034803485791828842449664524513492921140109815152701039903295096569044077275913582994481642878244424457585164288.000000\n",
      "iteration 600 / 1000: loss 2383591459531793578373019059142070108106344348238057845801004113955251663987684581102796104510346900461497064203687523987390045861959926993710595347363128721270982882866843259276921667584.000000\n",
      "iteration 700 / 1000: loss 6962193256938194046878817496892034825803589137936938878961103037323169653132457246150671677664310475495823846309730171700024548435634922332346722427472603069138415502546274207617216351334925615616715982737815961075712.000000\n",
      "iteration 800 / 1000: loss 20335756261048604665218744950339459132876380868839043780127278271809231619640525632468416309833153937641028406090321353743643910456222210448893480208992255269393279084258502424231211777414582201934495928595464500650607032482504643880586889975037952.000000\n",
      "iteration 900 / 1000: loss 59398377414568313904293698266840408731458150616553672946319122199133619004085885844172641591559257321725956113721570151933706820406918740852390470682202511548414444620135012593991121882385593044461058105718487765550695574368573589347494358680541495528932500785331314002730418176.000000\n",
      "training accuracy: 0.088612\n",
      "validation accuracy: 0.086000\n",
      "\n",
      "\n",
      "Starting iteration with rate 3.853529e-05 and regstrength 1.238538e+06\n",
      "iteration 0 / 1000: loss 1992.111867\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bram/Documents/courses/cs231n_spring_2017_stanford/assignment1/cs231n/classifiers/linear_svm.py:86: RuntimeWarning: overflow encountered in double_scalars\n",
      "  regloss = reg*np.sum(W*W)\n",
      "/home/bram/install/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/bram/Documents/courses/cs231n_spring_2017_stanford/assignment1/cs231n/classifiers/linear_svm.py:86: RuntimeWarning: overflow encountered in multiply\n",
      "  regloss = reg*np.sum(W*W)\n",
      "/home/bram/Documents/courses/cs231n_spring_2017_stanford/assignment1/cs231n/classifiers/linear_svm.py:112: RuntimeWarning: overflow encountered in multiply\n",
      "  dW += reg*2*W\n",
      "/home/bram/Documents/courses/cs231n_spring_2017_stanford/assignment1/cs231n/classifiers/linear_classifier.py:75: RuntimeWarning: invalid value encountered in subtract\n",
      "  self.W -= learning_rate*grad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 7.742637e-07 and regstrength 1.028256e+06\n",
      "iteration 0 / 1000: loss 1567.970834\n",
      "iteration 100 / 1000: loss 9.000037\n",
      "iteration 200 / 1000: loss 9.000059\n",
      "iteration 300 / 1000: loss 9.000046\n",
      "iteration 400 / 1000: loss 9.000052\n",
      "iteration 500 / 1000: loss 9.000040\n",
      "iteration 600 / 1000: loss 9.000038\n",
      "iteration 700 / 1000: loss 9.000055\n",
      "iteration 800 / 1000: loss 9.000057\n",
      "iteration 900 / 1000: loss 9.000049\n",
      "training accuracy: 0.227980\n",
      "validation accuracy: 0.213000\n",
      "\n",
      "\n",
      "Starting iteration with rate 3.678380e-07 and regstrength 2.607004e+06\n",
      "iteration 0 / 1000: loss 4382.761609\n",
      "iteration 100 / 1000: loss 9.000313\n",
      "iteration 200 / 1000: loss 9.000153\n",
      "iteration 300 / 1000: loss 9.000147\n",
      "iteration 400 / 1000: loss 9.000150\n",
      "iteration 500 / 1000: loss 9.000152\n",
      "iteration 600 / 1000: loss 9.000147\n",
      "iteration 700 / 1000: loss 9.000141\n",
      "iteration 800 / 1000: loss 9.000147\n",
      "iteration 900 / 1000: loss 9.000137\n",
      "training accuracy: 0.214735\n",
      "validation accuracy: 0.214000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.353048e-06 and regstrength 3.871318e+05\n",
      "iteration 0 / 1000: loss 654.189927\n",
      "iteration 100 / 1000: loss 9.000013\n",
      "iteration 200 / 1000: loss 9.000011\n",
      "iteration 300 / 1000: loss 8.999998\n",
      "iteration 400 / 1000: loss 9.000007\n",
      "iteration 500 / 1000: loss 8.999996\n",
      "iteration 600 / 1000: loss 9.000005\n",
      "iteration 700 / 1000: loss 9.000019\n",
      "iteration 800 / 1000: loss 8.999991\n",
      "iteration 900 / 1000: loss 9.000005\n",
      "training accuracy: 0.341000\n",
      "validation accuracy: 0.354000\n",
      "\n",
      "\n",
      "Starting iteration with rate 9.545485e-06 and regstrength 5.000000e+04\n",
      "iteration 0 / 1000: loss 84.088213\n",
      "iteration 100 / 1000: loss 8.999861\n",
      "iteration 200 / 1000: loss 8.999946\n",
      "iteration 300 / 1000: loss 8.999974\n",
      "iteration 400 / 1000: loss 8.999942\n",
      "iteration 500 / 1000: loss 8.999948\n",
      "iteration 600 / 1000: loss 8.999900\n",
      "iteration 700 / 1000: loss 8.999941\n",
      "iteration 800 / 1000: loss 8.999898\n",
      "iteration 900 / 1000: loss 8.999949\n",
      "training accuracy: 0.327245\n",
      "validation accuracy: 0.319000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.047616e-05 and regstrength 4.055654e+05\n",
      "iteration 0 / 1000: loss 647.607433\n",
      "iteration 100 / 1000: loss 6150746449100342464190746191609568515183103128279491547309629978356714137912459469086608636911838059766368950922561884866066266824304481414531517761089064714332200488063389925376.000000\n",
      "iteration 200 / 1000: loss inf\n",
      "iteration 300 / 1000: loss inf\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.232847e-06 and regstrength 4.663017e+05\n",
      "iteration 0 / 1000: loss 723.954265\n",
      "iteration 100 / 1000: loss 9.000031\n",
      "iteration 200 / 1000: loss 9.000007\n",
      "iteration 300 / 1000: loss 9.000011\n",
      "iteration 400 / 1000: loss 9.000023\n",
      "iteration 500 / 1000: loss 9.000020\n",
      "iteration 600 / 1000: loss 9.000003\n",
      "iteration 700 / 1000: loss 9.000014\n",
      "iteration 800 / 1000: loss 9.000011\n",
      "iteration 900 / 1000: loss 9.000008\n",
      "training accuracy: 0.319000\n",
      "validation accuracy: 0.327000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.321941e-07 and regstrength 2.164381e+06\n",
      "iteration 0 / 1000: loss 3402.090120\n",
      "iteration 100 / 1000: loss 8.999997\n",
      "iteration 200 / 1000: loss 8.999992\n",
      "iteration 300 / 1000: loss 8.999996\n",
      "iteration 400 / 1000: loss 8.999996\n",
      "iteration 500 / 1000: loss 8.999996\n",
      "iteration 600 / 1000: loss 8.999994\n",
      "iteration 700 / 1000: loss 8.999995\n",
      "iteration 800 / 1000: loss 8.999995\n",
      "iteration 900 / 1000: loss 8.999994\n",
      "training accuracy: 0.366612\n",
      "validation accuracy: 0.390000\n",
      "\n",
      "\n",
      "Starting iteration with rate 4.430621e-07 and regstrength 1.882468e+06\n",
      "iteration 0 / 1000: loss 3088.048578\n",
      "iteration 100 / 1000: loss 9.000033\n",
      "iteration 200 / 1000: loss 9.000037\n",
      "iteration 300 / 1000: loss 9.000039\n",
      "iteration 400 / 1000: loss 9.000028\n",
      "iteration 500 / 1000: loss 9.000045\n",
      "iteration 600 / 1000: loss 9.000033\n",
      "iteration 700 / 1000: loss 9.000036\n",
      "iteration 800 / 1000: loss 9.000035\n",
      "iteration 900 / 1000: loss 9.000037\n",
      "training accuracy: 0.234980\n",
      "validation accuracy: 0.236000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.629751e-06 and regstrength 2.114621e+05\n",
      "iteration 0 / 1000: loss 349.771031\n",
      "iteration 100 / 1000: loss 8.999960\n",
      "iteration 200 / 1000: loss 8.999953\n",
      "iteration 300 / 1000: loss 8.999948\n",
      "iteration 400 / 1000: loss 8.999966\n",
      "iteration 500 / 1000: loss 8.999952\n",
      "iteration 600 / 1000: loss 8.999956\n",
      "iteration 700 / 1000: loss 8.999967\n",
      "iteration 800 / 1000: loss 8.999962\n",
      "iteration 900 / 1000: loss 8.999976\n",
      "training accuracy: 0.352347\n",
      "validation accuracy: 0.381000\n",
      "\n",
      "\n",
      "Starting iteration with rate 5.994843e-06 and regstrength 3.695361e+05\n",
      "iteration 0 / 1000: loss 557.384760\n",
      "iteration 100 / 1000: loss 65109753863261928529889357946152666279036973903943745222205794827710894303775635612459355540738966071858429952.000000\n",
      "iteration 200 / 1000: loss 7730471606343120415448156489601084430471150774159200252290734636370860190803218302207224208246831352148604428626187535611814270107983210542861042590900882232024191415331820198285105529719467925663383345793552057630720.000000\n",
      "iteration 300 / 1000: loss inf\n",
      "iteration 400 / 1000: loss inf\n",
      "iteration 500 / 1000: loss inf\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bram/Documents/courses/cs231n_spring_2017_stanford/assignment1/cs231n/classifiers/linear_svm.py:81: RuntimeWarning: invalid value encountered in subtract\n",
      "  summand = np.maximum(indiv_scores.transpose()-correct_scores+1,0).transpose()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 4.229243e-05 and regstrength 2.066006e+06\n",
      "iteration 0 / 1000: loss 3284.105212\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 2.848036e-06 and regstrength 7.599555e+04\n",
      "iteration 0 / 1000: loss 129.690869\n",
      "iteration 100 / 1000: loss 8.999822\n",
      "iteration 200 / 1000: loss 8.999831\n",
      "iteration 300 / 1000: loss 8.999826\n",
      "iteration 400 / 1000: loss 8.999828\n",
      "iteration 500 / 1000: loss 8.999805\n",
      "iteration 600 / 1000: loss 8.999840\n",
      "iteration 700 / 1000: loss 8.999796\n",
      "iteration 800 / 1000: loss 8.999815\n",
      "iteration 900 / 1000: loss 8.999774\n",
      "training accuracy: 0.375939\n",
      "validation accuracy: 0.404000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.417474e-04 and regstrength 2.668350e+05\n",
      "iteration 0 / 1000: loss 390.691676\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.555676e-04 and regstrength 6.164234e+05\n",
      "iteration 0 / 1000: loss 941.942414\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 3.199267e-05 and regstrength 8.340503e+04\n",
      "iteration 0 / 1000: loss 134.193509\n",
      "iteration 100 / 1000: loss 3384146174325433334833518198721565534783148492971365494187055704913210353048793011158243789977119957935035607499967409490065620992.000000\n",
      "iteration 200 / 1000: loss 91476536564667556050327909035981649165672439562762390476435715341101712787710724155157973291942089513733453731262323827396123987381917237046646208948101954499791153751555002964336109364213355523110514634869108186348577262972591096236374187625946645279014912.000000\n",
      "iteration 300 / 1000: loss inf\n",
      "iteration 400 / 1000: loss inf\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 5.094138e-05 and regstrength 4.451075e+05\n",
      "iteration 0 / 1000: loss 652.064765\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.149757e-05 and regstrength 1.457527e+05\n",
      "iteration 0 / 1000: loss 252.979765\n",
      "iteration 100 / 1000: loss 45725170764495256177788860446908878126221429824798702615187148246466544271360.000000\n",
      "iteration 200 / 1000: loss 8569264631993638597453559647986742691418347823346480087707230045089784071691709309568775256225536042336587950007139382914584262040284412898703898050560.000000\n",
      "iteration 300 / 1000: loss 1605949088989644094986019910193185873193267598894617886618343378518815878643661130758870606684862706730694519562848645083986884981010565114992655200580443245334502175650326973867250140388382281808581173950282502747075265953792.000000\n",
      "iteration 400 / 1000: loss 300967771119777698225989627202877710259900772612142900379174881411830667912472904863537007017248334965091339201668913460593704733085725492746406532065114652520510379472492515627995691981272191311752382557070148605733218932638808655671174612037451171922462775949004885792033597750805750428050655281152.000000\n",
      "iteration 500 / 1000: loss inf\n",
      "iteration 600 / 1000: loss inf\n",
      "iteration 700 / 1000: loss inf\n",
      "iteration 800 / 1000: loss inf\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.176812e-04 and regstrength 5.616620e+05\n",
      "iteration 0 / 1000: loss 899.295952\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.830738e-05 and regstrength 1.715235e+06\n",
      "iteration 0 / 1000: loss 2743.915153\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 3.125716e-06 and regstrength 5.748785e+04\n",
      "iteration 0 / 1000: loss 103.721735\n",
      "iteration 100 / 1000: loss 8.999809\n",
      "iteration 200 / 1000: loss 8.999704\n",
      "iteration 300 / 1000: loss 8.999720\n",
      "iteration 400 / 1000: loss 8.999811\n",
      "iteration 500 / 1000: loss 8.999769\n",
      "iteration 600 / 1000: loss 8.999703\n",
      "iteration 700 / 1000: loss 8.999752\n",
      "iteration 800 / 1000: loss 8.999778\n",
      "iteration 900 / 1000: loss 8.999831\n",
      "training accuracy: 0.395837\n",
      "validation accuracy: 0.389000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.072267e-04 and regstrength 2.431301e+05\n",
      "iteration 0 / 1000: loss 369.481632\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.873817e-04 and regstrength 1.004617e+05\n",
      "iteration 0 / 1000: loss 160.212846\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 4.037017e-07 and regstrength 1.102565e+05\n",
      "iteration 0 / 1000: loss 179.773655\n",
      "iteration 100 / 1000: loss 8.999813\n",
      "iteration 200 / 1000: loss 8.999870\n",
      "iteration 300 / 1000: loss 8.999824\n",
      "iteration 400 / 1000: loss 8.999820\n",
      "iteration 500 / 1000: loss 8.999907\n",
      "iteration 600 / 1000: loss 8.999895\n",
      "iteration 700 / 1000: loss 8.999845\n",
      "iteration 800 / 1000: loss 8.999884\n",
      "iteration 900 / 1000: loss 8.999811\n",
      "training accuracy: 0.415857\n",
      "validation accuracy: 0.414000\n",
      "\n",
      "\n",
      "Starting iteration with rate 9.111628e-04 and regstrength 1.491824e+06\n",
      "iteration 0 / 1000: loss 2436.396413\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.747528e-07 and regstrength 2.215311e+05\n",
      "iteration 0 / 1000: loss 346.037556\n",
      "iteration 100 / 1000: loss 8.999968\n",
      "iteration 200 / 1000: loss 8.999937\n",
      "iteration 300 / 1000: loss 8.999929\n",
      "iteration 400 / 1000: loss 8.999921\n",
      "iteration 500 / 1000: loss 8.999915\n",
      "iteration 600 / 1000: loss 8.999926\n",
      "iteration 700 / 1000: loss 8.999925\n",
      "iteration 800 / 1000: loss 8.999919\n",
      "iteration 900 / 1000: loss 8.999924\n",
      "training accuracy: 0.413327\n",
      "validation accuracy: 0.426000\n",
      "\n",
      "\n",
      "Starting iteration with rate 6.892612e-04 and regstrength 3.527401e+05\n",
      "iteration 0 / 1000: loss 545.809172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 5.590810e-05 and regstrength 9.815203e+05\n",
      "iteration 0 / 1000: loss 1550.936742\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.291550e-04 and regstrength 4.772742e+06\n",
      "iteration 0 / 1000: loss 7297.535268\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 8.902151e-05 and regstrength 8.737642e+04\n",
      "iteration 0 / 1000: loss 140.265880\n",
      "iteration 100 / 1000: loss 5384045225970288864110315772104587881707013289164805811703974852195043321490803990645548822867337029717530124851885350130461606546871622657450503331174221736616155976199805708841201971145461660476659358873973294602742870935900876963840.000000\n",
      "iteration 200 / 1000: loss inf\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 2.595024e-06 and regstrength 3.367075e+05\n",
      "iteration 0 / 1000: loss 539.863224\n",
      "iteration 100 / 1000: loss 9.000257\n",
      "iteration 200 / 1000: loss 9.000350\n",
      "iteration 300 / 1000: loss 9.000338\n",
      "iteration 400 / 1000: loss 9.000317\n",
      "iteration 500 / 1000: loss 9.000272\n",
      "iteration 600 / 1000: loss 9.000281\n",
      "iteration 700 / 1000: loss 9.000288\n",
      "iteration 800 / 1000: loss 9.000307\n",
      "iteration 900 / 1000: loss 9.000305\n",
      "training accuracy: 0.229327\n",
      "validation accuracy: 0.233000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.261857e-05 and regstrength 2.320794e+05\n",
      "iteration 0 / 1000: loss 377.007423\n",
      "iteration 100 / 1000: loss 69159636483634746594938141347435462899558946829277658431198336259745855349753343961631373183884769054636735429571966576281611721051314585600.000000\n",
      "iteration 200 / 1000: loss 12997076943172717867198588477773462544934445761388765970014389681225096133115764556041024024413077060452198315600229159493682167013811738912687798288771030425832584368500135490203707659053436367743959195291929509346245733887675360254521814824246291613883399338604694606324957184.000000\n",
      "iteration 300 / 1000: loss inf\n",
      "iteration 400 / 1000: loss inf\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 2.420128e-05 and regstrength 3.446306e+06\n",
      "iteration 0 / 1000: loss 5265.141895\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.450829e-07 and regstrength 1.562858e+06\n",
      "iteration 0 / 1000: loss 2438.597176\n",
      "iteration 100 / 1000: loss 8.999991\n",
      "iteration 200 / 1000: loss 8.999993\n",
      "iteration 300 / 1000: loss 8.999991\n",
      "iteration 400 / 1000: loss 8.999992\n",
      "iteration 500 / 1000: loss 8.999991\n",
      "iteration 600 / 1000: loss 8.999992\n",
      "iteration 700 / 1000: loss 8.999994\n",
      "iteration 800 / 1000: loss 8.999989\n",
      "iteration 900 / 1000: loss 8.999992\n",
      "training accuracy: 0.370592\n",
      "validation accuracy: 0.359000\n",
      "\n",
      "\n",
      "Starting iteration with rate 2.257020e-04 and regstrength 6.457748e+05\n",
      "iteration 0 / 1000: loss 993.265674\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 2.009233e-05 and regstrength 3.289666e+06\n",
      "iteration 0 / 1000: loss 5111.298642\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 7.220809e-06 and regstrength 1.926764e+05\n",
      "iteration 0 / 1000: loss 299.065737\n",
      "iteration 100 / 1000: loss 46913250959921006661967042402414490500292586562912256.000000\n",
      "iteration 200 / 1000: loss 7587483818984767674214310564583932270640961728180190587293255118365289280383272564051262694618138935296.000000\n",
      "iteration 300 / 1000: loss 1227156709999460061949978181241778331453783302494902052413389013969112926301256577261775969544915383301910629210347894084394804044948570951332584977596416.000000\n",
      "iteration 400 / 1000: loss 198473384171011753037715431495439580681111387052594896702840464714768660908189653041169947310258436599445285287301664284043348450901707118135113552403227693598503813588025322986757050766990667306030333952.000000\n",
      "iteration 500 / 1000: loss 32099962379141740813237173483840864209191157977821232056558313436219381426388835227872991127382214441478159122872442899000951467702847660225800862972136691963945382296337215522768845831794824723600438571804774189295322786327860007617387760640897971126272.000000\n",
      "iteration 600 / 1000: loss 5191666323654152339647618646677425973496554024880847608775483038986638796040636128948759957537938152405430127134533477052240270394266055243910047029321357132980789116429540755147407918122893889667618799650406164517257749618989854918728787555687732266619567875014458598673769929319528079084111533153714176.000000\n",
      "iteration 700 / 1000: loss inf\n",
      "iteration 800 / 1000: loss inf\n",
      "iteration 900 / 1000: loss inf\n",
      "training accuracy: 0.079265\n",
      "validation accuracy: 0.076000\n",
      "\n",
      "\n",
      "Starting iteration with rate 7.564633e-04 and regstrength 1.128510e+06\n",
      "iteration 0 / 1000: loss 1756.474419\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 5.462277e-06 and regstrength 1.359294e+06\n",
      "iteration 0 / 1000: loss 2059.902943\n",
      "iteration 100 / 1000: loss 3980242765332215701259599118368277361021640293352706097859757473160118871347664571725585703529192506721822344471786175399848921477712727386063556140931678348437398604872493445187380988729197726516108102306154556087918941740562644992.000000\n",
      "iteration 200 / 1000: loss inf\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 2.154435e-06 and regstrength 1.297512e+06\n",
      "iteration 0 / 1000: loss 2063.433466\n",
      "iteration 100 / 1000: loss 4904105990378246880400677767912159275995068229497172204428648235119435528582244579903186565027707356044124727300997260955384961220739072.000000\n",
      "iteration 200 / 1000: loss 11706500733456204337208037411124441041056950540200657568190224049632362346392396490375982528629546610613817822209281037523372476540341091235644216366158813821769699495523942901042372948836063636977263533206376959545395996654625502356213662313389819074072867013209882624.000000\n",
      "iteration 300 / 1000: loss inf\n",
      "iteration 400 / 1000: loss inf\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 2.656088e-05 and regstrength 9.153691e+04\n",
      "iteration 0 / 1000: loss 158.132339\n",
      "iteration 100 / 1000: loss 354468325542972562320504939583336353154424043290229319163047941897136136448778951659967203454086760273026955555349987328.000000\n",
      "iteration 200 / 1000: loss 842481157363225748869284976177144650620089626614175750449965399764153117101521404094995084153319638626200020668306633261510260249540978408155647417384383078082880782603470420026702325524134928653493382778106943638231102978297665249345536.000000\n",
      "iteration 300 / 1000: loss inf\n",
      "iteration 400 / 1000: loss inf\n",
      "iteration 500 / 1000: loss inf\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.123324e-06 and regstrength 8.943248e+05\n",
      "iteration 0 / 1000: loss 1429.451631\n",
      "iteration 100 / 1000: loss 8935.925768\n",
      "iteration 200 / 1000: loss 56112.596301\n",
      "iteration 300 / 1000: loss 352604.118631\n",
      "iteration 400 / 1000: loss 2216054.896377\n",
      "iteration 500 / 1000: loss 13928414.113217\n",
      "iteration 600 / 1000: loss 87541135.276202\n",
      "iteration 700 / 1000: loss 550186976.934595\n",
      "iteration 800 / 1000: loss 3457823998.705810\n",
      "iteration 900 / 1000: loss 21731673360.943233\n",
      "training accuracy: 0.116327\n",
      "validation accuracy: 0.117000\n",
      "\n",
      "\n",
      "Starting iteration with rate 3.430469e-06 and regstrength 6.309284e+04\n",
      "iteration 0 / 1000: loss 104.178433\n",
      "iteration 100 / 1000: loss 8.999851\n",
      "iteration 200 / 1000: loss 8.999798\n",
      "iteration 300 / 1000: loss 8.999844\n",
      "iteration 400 / 1000: loss 8.999779\n",
      "iteration 500 / 1000: loss 8.999801\n",
      "iteration 600 / 1000: loss 8.999884\n",
      "iteration 700 / 1000: loss 8.999867\n",
      "iteration 800 / 1000: loss 8.999830\n",
      "iteration 900 / 1000: loss 8.999725\n",
      "training accuracy: 0.373673\n",
      "validation accuracy: 0.372000\n",
      "\n",
      "\n",
      "Starting iteration with rate 2.782559e-07 and regstrength 1.637275e+06\n",
      "iteration 0 / 1000: loss 2616.133301\n",
      "iteration 100 / 1000: loss 8.999999\n",
      "iteration 200 / 1000: loss 8.999997\n",
      "iteration 300 / 1000: loss 8.999996\n",
      "iteration 400 / 1000: loss 8.999997\n",
      "iteration 500 / 1000: loss 8.999997\n",
      "iteration 600 / 1000: loss 8.999996\n",
      "iteration 700 / 1000: loss 8.999998\n",
      "iteration 800 / 1000: loss 9.000000\n",
      "iteration 900 / 1000: loss 9.000000\n",
      "training accuracy: 0.339857\n",
      "validation accuracy: 0.326000\n",
      "\n",
      "\n",
      "Starting iteration with rate 6.135907e-05 and regstrength 1.972103e+06\n",
      "iteration 0 / 1000: loss 3035.352545\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 3.764936e-06 and regstrength 4.555814e+06\n",
      "iteration 0 / 1000: loss 7182.318087\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss inf\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 7.924829e-06 and regstrength 8.148754e+05\n",
      "iteration 0 / 1000: loss 1292.587228\n",
      "iteration 100 / 1000: loss 214217597496261159786285201239172109591662346557101505793680403593859805334240752198640553870201269547533350418718577755742167015720701366482430577500208512132005208455115333736426928239274316265976730679557677199130624.000000\n",
      "iteration 200 / 1000: loss inf\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 4.641589e-05 and regstrength 7.961414e+04\n",
      "iteration 0 / 1000: loss 138.544466\n",
      "iteration 100 / 1000: loss 16688538410802377805204655443610368502704409372818123039968514593224990655164635388257764367302199695760163098000411994483715140174588019186099986072159045443125248.000000\n",
      "iteration 200 / 1000: loss inf\n",
      "iteration 300 / 1000: loss inf\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 4.534879e-06 and regstrength 1.155065e+05\n",
      "iteration 0 / 1000: loss 184.225078\n",
      "iteration 100 / 1000: loss 9.000067\n",
      "iteration 200 / 1000: loss 9.000017\n",
      "iteration 300 / 1000: loss 8.999998\n",
      "iteration 400 / 1000: loss 8.999955\n",
      "iteration 500 / 1000: loss 8.999941\n",
      "iteration 600 / 1000: loss 9.000024\n",
      "iteration 700 / 1000: loss 8.999964\n",
      "iteration 800 / 1000: loss 8.999986\n",
      "iteration 900 / 1000: loss 9.000078\n",
      "training accuracy: 0.312776\n",
      "validation accuracy: 0.309000\n",
      "\n",
      "\n",
      "Starting iteration with rate 7.390722e-05 and regstrength 1.328044e+05\n",
      "iteration 0 / 1000: loss 210.956230\n",
      "iteration 100 / 1000: loss 22373011541572722079842843423644753808826249135523435816014215332233386317304728439678848115835691762643796575873257107978754671702646808640416849069707582016444478930397374295210181111175970271567794757825266781829584118316039637459017342044512447893602304.000000\n",
      "iteration 200 / 1000: loss inf\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 3.351603e-07 and regstrength 7.254144e+04\n",
      "iteration 0 / 1000: loss 129.019424\n",
      "iteration 100 / 1000: loss 9.005500\n",
      "iteration 200 / 1000: loss 8.999742\n",
      "iteration 300 / 1000: loss 8.999777\n",
      "iteration 400 / 1000: loss 8.999732\n",
      "iteration 500 / 1000: loss 8.999759\n",
      "iteration 600 / 1000: loss 8.999763\n",
      "iteration 700 / 1000: loss 8.999732\n",
      "iteration 800 / 1000: loss 8.999764\n",
      "iteration 900 / 1000: loss 8.999771\n",
      "training accuracy: 0.409408\n",
      "validation accuracy: 0.413000\n",
      "\n",
      "\n",
      "Starting iteration with rate 2.056512e-04 and regstrength 1.424018e+06\n",
      "iteration 0 / 1000: loss 2207.916095\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.204504e-07 and regstrength 1.599634e+05\n",
      "iteration 0 / 1000: loss 240.392708\n",
      "iteration 100 / 1000: loss 9.089185\n",
      "iteration 200 / 1000: loss 8.999915\n",
      "iteration 300 / 1000: loss 8.999901\n",
      "iteration 400 / 1000: loss 8.999890\n",
      "iteration 500 / 1000: loss 8.999889\n",
      "iteration 600 / 1000: loss 8.999916\n",
      "iteration 700 / 1000: loss 8.999925\n",
      "iteration 800 / 1000: loss 8.999905\n",
      "iteration 900 / 1000: loss 8.999906\n",
      "training accuracy: 0.412796\n",
      "validation accuracy: 0.408000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.707353e-04 and regstrength 9.589551e+04\n",
      "iteration 0 / 1000: loss 159.650354\n",
      "iteration 100 / 1000: loss 326878855371658115971890461564471695716503327389372395317952844174622849739836399154299257370001989310390843923349870879450419347756349695839045088667953335510204953141525425128347323585389296205773264548444339922167148340314179480861272335939841616332040091219258674484114235369463543038037930215997440.000000\n",
      "iteration 200 / 1000: loss inf\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 5.722368e-04 and regstrength 3.782317e+06\n",
      "iteration 0 / 1000: loss 6014.427420\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.097499e-07 and regstrength 7.087371e+05\n",
      "iteration 0 / 1000: loss 1071.929329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1000: loss 8.999981\n",
      "iteration 200 / 1000: loss 8.999976\n",
      "iteration 300 / 1000: loss 8.999987\n",
      "iteration 400 / 1000: loss 8.999983\n",
      "iteration 500 / 1000: loss 8.999981\n",
      "iteration 600 / 1000: loss 8.999976\n",
      "iteration 700 / 1000: loss 8.999979\n",
      "iteration 800 / 1000: loss 8.999974\n",
      "iteration 900 / 1000: loss 8.999979\n",
      "training accuracy: 0.386408\n",
      "validation accuracy: 0.365000\n",
      "\n",
      "\n",
      "Starting iteration with rate 3.511192e-05 and regstrength 2.795405e+05\n",
      "iteration 0 / 1000: loss 451.773226\n",
      "iteration 100 / 1000: loss 49051009015657985505459197653393005867206996259954018528412879929848047484021608234288511671637390064658963314702098410463538186646217808437276694187458489742066765945732085817749914827680703646512845128680904375042507424104549788296314290984450080049725440.000000\n",
      "iteration 200 / 1000: loss inf\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 9.770100e-05 and regstrength 1.210064e+05\n",
      "iteration 0 / 1000: loss 199.384985\n",
      "iteration 100 / 1000: loss 1877997147415552659658821629989575611125488814042629678106284859620929110024167486996854628386266829206428124781933195384077074960881187436354698557184617474765091394017121176281909638207843987977288450571136877284312511931703881725643849186610634826371467688182074992230400.000000\n",
      "iteration 200 / 1000: loss inf\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 4.328761e-04 and regstrength 1.391280e+05\n",
      "iteration 0 / 1000: loss 238.991556\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 4.977024e-06 and regstrength 5.000000e+06\n",
      "iteration 0 / 1000: loss 7318.998942\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 6.734151e-05 and regstrength 2.997421e+06\n",
      "iteration 0 / 1000: loss 4817.147061\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.000000e-03 and regstrength 5.117655e+05\n",
      "iteration 0 / 1000: loss 831.159352\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 5.857021e-07 and regstrength 1.526928e+05\n",
      "iteration 0 / 1000: loss 261.970750\n",
      "iteration 100 / 1000: loss 8.999904\n",
      "iteration 200 / 1000: loss 8.999923\n",
      "iteration 300 / 1000: loss 8.999892\n",
      "iteration 400 / 1000: loss 8.999909\n",
      "iteration 500 / 1000: loss 8.999881\n",
      "iteration 600 / 1000: loss 8.999916\n",
      "iteration 700 / 1000: loss 8.999882\n",
      "iteration 800 / 1000: loss 8.999910\n",
      "iteration 900 / 1000: loss 8.999910\n",
      "training accuracy: 0.394755\n",
      "validation accuracy: 0.380000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.023531e-06 and regstrength 5.884060e+05\n",
      "iteration 0 / 1000: loss 911.388056\n",
      "iteration 100 / 1000: loss 9.000016\n",
      "iteration 200 / 1000: loss 9.000009\n",
      "iteration 300 / 1000: loss 9.000014\n",
      "iteration 400 / 1000: loss 9.000024\n",
      "iteration 500 / 1000: loss 9.000010\n",
      "iteration 600 / 1000: loss 9.000009\n",
      "iteration 700 / 1000: loss 9.000013\n",
      "iteration 800 / 1000: loss 9.000017\n",
      "iteration 900 / 1000: loss 9.000020\n",
      "training accuracy: 0.306347\n",
      "validation accuracy: 0.300000\n",
      "\n",
      "\n",
      "Starting iteration with rate 2.364489e-06 and regstrength 5.361336e+05\n",
      "iteration 0 / 1000: loss 837.848504\n",
      "iteration 100 / 1000: loss 14479937934025401930072827598325524987904.000000\n",
      "iteration 200 / 1000: loss 252967847098143583152470873284021280402255946320446119966887356625062859898880.000000\n",
      "iteration 300 / 1000: loss 4419406488966892712574597375737060576640022837252353968132817512322427505543802259133552508577430736942106859798528.000000\n",
      "iteration 400 / 1000: loss 77208048132477524303253205104768304685322480012070418779874703536818828639012986116405693615502563498415549771739807478308383918157865790729104864051200.000000\n",
      "iteration 500 / 1000: loss 1348842364084156832145157419137124077188573029807298155494436375266326300013429862746263619600533569860458784360995720627838788624958556788407761384815907123678822678513210719448737945485312.000000\n",
      "iteration 600 / 1000: loss 23564586427911747849357350744897961008246904127113140054554805386282496819283070476909266433136074842672384462352888920543252574512015619514793031597008928621296961256013764256475892029623480211554612607877831079988646710870016.000000\n",
      "iteration 700 / 1000: loss 411678746386020916769073107964213190644801114390336803412134866754140578789957722575718246768259077867449917074549353283165191409618225850803456062991291536498031559823408483099154772996586905402851819097765870213794614427279790696739171342463631444581917315301376.000000\n",
      "iteration 800 / 1000: loss 7192122414048440322479188727804270360789405388464207493600485382125014991454758617223751486336767804847598535703012037324691523903226806727783422554210541507976239212987412893218647046377987374910912042494376706097949648062295623146196607775651492722876197770845131097206935837331243214411660942901248.000000\n",
      "iteration 900 / 1000: loss inf\n",
      "training accuracy: 0.088490\n",
      "validation accuracy: 0.105000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.963041e-06 and regstrength 3.067954e+05\n",
      "iteration 0 / 1000: loss 487.464050\n",
      "iteration 100 / 1000: loss 9.000032\n",
      "iteration 200 / 1000: loss 9.000010\n",
      "iteration 300 / 1000: loss 9.000024\n",
      "iteration 400 / 1000: loss 9.000034\n",
      "iteration 500 / 1000: loss 9.000028\n",
      "iteration 600 / 1000: loss 9.000031\n",
      "iteration 700 / 1000: loss 9.000027\n",
      "iteration 800 / 1000: loss 9.000037\n",
      "iteration 900 / 1000: loss 9.000014\n",
      "training accuracy: 0.292286\n",
      "validation accuracy: 0.291000\n",
      "\n",
      "\n",
      "Starting iteration with rate 3.944206e-04 and regstrength 3.962414e+06\n",
      "iteration 0 / 1000: loss 5985.531776\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 3.053856e-07 and regstrength 6.022518e+04\n",
      "iteration 0 / 1000: loss 99.145508\n",
      "iteration 100 / 1000: loss 9.049818\n",
      "iteration 200 / 1000: loss 8.999741\n",
      "iteration 300 / 1000: loss 8.999734\n",
      "iteration 400 / 1000: loss 8.999745\n",
      "iteration 500 / 1000: loss 8.999737\n",
      "iteration 600 / 1000: loss 8.999706\n",
      "iteration 700 / 1000: loss 8.999743\n",
      "iteration 800 / 1000: loss 8.999728\n",
      "iteration 900 / 1000: loss 8.999696\n",
      "training accuracy: 0.408143\n",
      "validation accuracy: 0.402000\n",
      "\n",
      "\n",
      "Starting iteration with rate 2.718588e-04 and regstrength 1.755596e+05\n",
      "iteration 0 / 1000: loss 296.351369\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 2.310130e-07 and regstrength 1.077217e+06\n",
      "iteration 0 / 1000: loss 1670.946595\n",
      "iteration 100 / 1000: loss 8.999989\n",
      "iteration 200 / 1000: loss 8.999986\n",
      "iteration 300 / 1000: loss 8.999989\n",
      "iteration 400 / 1000: loss 8.999987\n",
      "iteration 500 / 1000: loss 8.999988\n",
      "iteration 600 / 1000: loss 8.999991\n",
      "iteration 700 / 1000: loss 8.999992\n",
      "iteration 800 / 1000: loss 8.999987\n",
      "iteration 900 / 1000: loss 8.999993\n",
      "training accuracy: 0.373796\n",
      "validation accuracy: 0.365000\n",
      "\n",
      "\n",
      "Starting iteration with rate 8.302176e-04 and regstrength 2.018509e+05\n",
      "iteration 0 / 1000: loss 318.804234\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.384886e-05 and regstrength 2.861184e+06\n",
      "iteration 0 / 1000: loss 4421.654580\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.917910e-07 and regstrength 2.731139e+06\n",
      "iteration 0 / 1000: loss 3935.692907\n",
      "iteration 100 / 1000: loss 9.000002\n",
      "iteration 200 / 1000: loss 8.999999\n",
      "iteration 300 / 1000: loss 9.000002\n",
      "iteration 400 / 1000: loss 9.000002\n",
      "iteration 500 / 1000: loss 9.000001\n",
      "iteration 600 / 1000: loss 8.999999\n",
      "iteration 700 / 1000: loss 9.000002\n",
      "iteration 800 / 1000: loss 9.000000\n",
      "iteration 900 / 1000: loss 8.999999\n",
      "training accuracy: 0.306449\n",
      "validation accuracy: 0.337000\n",
      "\n",
      "\n",
      "Starting iteration with rate 6.579332e-06 and regstrength 9.369087e+05\n",
      "iteration 0 / 1000: loss 1506.954165\n",
      "iteration 100 / 1000: loss 10226584405208118401208975218474165567959286275717315040893742546394707060164681952258854099952731664901198965232834141612798175872300891999651726101310701908855158722068694160430986641959900089770424470135086514176.000000\n",
      "iteration 200 / 1000: loss inf\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.519911e-05 and regstrength 7.424841e+05\n",
      "iteration 0 / 1000: loss 1173.788274\n",
      "iteration 100 / 1000: loss 687179871330299218021853154137792285031702541521735716888610487761191158827734862457963035344766377257337206781021914906802214564481386726345111657020558300249898648663304977988367648051016755113789892442695152065531172982849302928684470264854969226409202823580016967680.000000\n",
      "iteration 200 / 1000: loss inf\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.788650e-06 and regstrength 4.885050e+05\n",
      "iteration 0 / 1000: loss 795.793885\n",
      "iteration 100 / 1000: loss 9.000194\n",
      "iteration 200 / 1000: loss 9.000221\n",
      "iteration 300 / 1000: loss 9.000198\n",
      "iteration 400 / 1000: loss 9.000204\n",
      "iteration 500 / 1000: loss 9.000203\n",
      "iteration 600 / 1000: loss 9.000205\n",
      "iteration 700 / 1000: loss 9.000186\n",
      "iteration 800 / 1000: loss 9.000177\n",
      "iteration 900 / 1000: loss 9.000212\n",
      "training accuracy: 0.239816\n",
      "validation accuracy: 0.244000\n",
      "\n",
      "\n",
      "Starting iteration with rate 9.326033e-07 and regstrength 8.536763e+05\n",
      "iteration 0 / 1000: loss 1291.222948\n",
      "iteration 100 / 1000: loss 9.000053\n",
      "iteration 200 / 1000: loss 9.000058\n",
      "iteration 300 / 1000: loss 9.000059\n",
      "iteration 400 / 1000: loss 9.000051\n",
      "iteration 500 / 1000: loss 9.000064\n",
      "iteration 600 / 1000: loss 9.000064\n",
      "iteration 700 / 1000: loss 9.000061\n",
      "iteration 800 / 1000: loss 9.000065\n",
      "iteration 900 / 1000: loss 9.000057\n",
      "training accuracy: 0.291265\n",
      "validation accuracy: 0.312000\n",
      "\n",
      "\n",
      "Starting iteration with rate 7.054802e-07 and regstrength 1.052452e+05\n",
      "iteration 0 / 1000: loss 178.398707\n",
      "iteration 100 / 1000: loss 8.999849\n",
      "iteration 200 / 1000: loss 8.999875\n",
      "iteration 300 / 1000: loss 8.999865\n",
      "iteration 400 / 1000: loss 8.999862\n",
      "iteration 500 / 1000: loss 8.999877\n",
      "iteration 600 / 1000: loss 8.999861\n",
      "iteration 700 / 1000: loss 8.999813\n",
      "iteration 800 / 1000: loss 8.999784\n",
      "iteration 900 / 1000: loss 8.999850\n",
      "training accuracy: 0.403673\n",
      "validation accuracy: 0.396000\n",
      "\n",
      "\n",
      "Starting iteration with rate 6.280291e-04 and regstrength 2.547069e+05\n",
      "iteration 0 / 1000: loss 405.399712\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 2.205131e-05 and regstrength 4.248767e+05\n",
      "iteration 0 / 1000: loss 676.933159\n",
      "iteration 100 / 1000: loss 4041248201353581879213877650142997214113599946922769991615427596753387970307435085921809217163304611413258165347385144501863631083532703769395844724225597060629785555632586559333498215809287885858811648340938336220286880008558422873670482727388764438528.000000\n",
      "iteration 200 / 1000: loss inf\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 4.750810e-04 and regstrength 3.610405e+06\n",
      "iteration 0 / 1000: loss 5472.922782\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.484968e-06 and regstrength 2.267439e+06\n",
      "iteration 0 / 1000: loss 3585.418383\n",
      "iteration 100 / 1000: loss 176713007518568678054524796104169629073936556032312613338275535580330251090009766664117591943756481254282947040746014224590601540258917828348439865948372992.000000\n",
      "iteration 200 / 1000: loss 8731490402404466296955250091859373313684812950707845840754417016867787948115097978867285593059616447994223400026947266792289037821184352432627648932214487244702706602603312969110194282283541818579872682489846270337863562720661352369045201114587353237017438838343181824609118410735406939232297083466787323904.000000\n",
      "iteration 300 / 1000: loss inf\n",
      "iteration 400 / 1000: loss inf\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 6.428073e-07 and regstrength 3.140146e+06\n",
      "iteration 0 / 1000: loss 4861.079976\n",
      "iteration 100 / 1000: loss 14975448841094472479977013996692056980612529267216997332836734064234732028756697073408203236552212480.000000\n",
      "iteration 200 / 1000: loss 46220201013516221377313001647561980772463240166212708436260324907881736933920395123519836331506046463395318515392318850442424142305126854629295299366156905705630285323870260705077749806628203397120.000000\n",
      "iteration 300 / 1000: loss 142653953440617864722377168481461317305867048452505297832771831474452730354836456015071783496625369383196969006472523475558877250952693708477302955159618202924382448999262231426670730457354120345781623471773854521797492027499776815924984733616998851341812994481636135772477344834971665866686464.000000\n",
      "iteration 400 / 1000: loss inf\n",
      "iteration 500 / 1000: loss inf\n",
      "iteration 600 / 1000: loss inf\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 5.336699e-07 and regstrength 4.348745e+06\n",
      "iteration 0 / 1000: loss 6530.440732\n",
      "iteration 100 / 1000: loss 118174972215401299925229781572721470135241815430028268139257148557235638531020891835726748461900107601594239791661056.000000\n",
      "iteration 200 / 1000: loss 2141446580956684652977872263486502331195580320186211748919793202671801589675527566380856110863823946748654680866040744876982062737263897068282716935740137936593313150574124660858606408197434670986270522569535733667597372977315840.000000\n",
      "iteration 300 / 1000: loss inf\n",
      "iteration 400 / 1000: loss inf\n",
      "iteration 500 / 1000: loss inf\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.000000e-07 and regstrength 1.182245e+06\n",
      "iteration 0 / 1000: loss 1897.472758\n",
      "iteration 100 / 1000: loss 8.999985\n",
      "iteration 200 / 1000: loss 8.999986\n",
      "iteration 300 / 1000: loss 8.999986\n",
      "iteration 400 / 1000: loss 8.999986\n",
      "iteration 500 / 1000: loss 8.999985\n",
      "iteration 600 / 1000: loss 8.999986\n",
      "iteration 700 / 1000: loss 8.999985\n",
      "iteration 800 / 1000: loss 8.999988\n",
      "iteration 900 / 1000: loss 8.999985\n",
      "training accuracy: 0.390735\n",
      "validation accuracy: 0.380000\n",
      "\n",
      "\n",
      "Starting iteration with rate 3.274549e-04 and regstrength 4.151088e+06\n",
      "iteration 0 / 1000: loss 6475.962974\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 2.915053e-05 and regstrength 2.928510e+05\n",
      "iteration 0 / 1000: loss 458.164417\n",
      "iteration 100 / 1000: loss 7492722832974407091491871614215207701189138853606836672787501155732175817904026741214251805355644813601474990251085782704134552258231363666764207838563006411265186940702221879108895074232288492866996976159693455151485712270070375672484489854976.000000\n",
      "iteration 200 / 1000: loss inf\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 2.477076e-04 and regstrength 1.839190e+05\n",
      "iteration 0 / 1000: loss 279.989319\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 2.104904e-07 and regstrength 3.214037e+05\n",
      "iteration 0 / 1000: loss 512.328664\n",
      "iteration 100 / 1000: loss 8.999956\n",
      "iteration 200 / 1000: loss 8.999944\n",
      "iteration 300 / 1000: loss 8.999951\n",
      "iteration 400 / 1000: loss 8.999955\n",
      "iteration 500 / 1000: loss 8.999945\n",
      "iteration 600 / 1000: loss 8.999941\n",
      "iteration 700 / 1000: loss 8.999949\n",
      "iteration 800 / 1000: loss 8.999956\n",
      "iteration 900 / 1000: loss 8.999956\n",
      "training accuracy: 0.406061\n",
      "validation accuracy: 0.388000\n",
      "\n",
      "\n",
      "Starting iteration with rate 5.214008e-04 and regstrength 1.796907e+06\n",
      "iteration 0 / 1000: loss 2816.277188\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.592283e-07 and regstrength 7.778381e+05\n",
      "iteration 0 / 1000: loss 1256.369224\n",
      "iteration 100 / 1000: loss 8.999980\n",
      "iteration 200 / 1000: loss 8.999977\n",
      "iteration 300 / 1000: loss 8.999980\n",
      "iteration 400 / 1000: loss 8.999978\n",
      "iteration 500 / 1000: loss 8.999982\n",
      "iteration 600 / 1000: loss 8.999975\n",
      "iteration 700 / 1000: loss 8.999982\n",
      "iteration 800 / 1000: loss 8.999978\n",
      "iteration 900 / 1000: loss 8.999982\n",
      "training accuracy: 0.381531\n",
      "validation accuracy: 0.373000\n",
      "\n",
      "\n",
      "Starting iteration with rate 4.132012e-06 and regstrength 6.609706e+04\n",
      "iteration 0 / 1000: loss 113.483701\n",
      "iteration 100 / 1000: loss 8.999796\n",
      "iteration 200 / 1000: loss 8.999811\n",
      "iteration 300 / 1000: loss 8.999835\n",
      "iteration 400 / 1000: loss 8.999772\n",
      "iteration 500 / 1000: loss 8.999867\n",
      "iteration 600 / 1000: loss 8.999835\n",
      "iteration 700 / 1000: loss 8.999815\n",
      "iteration 800 / 1000: loss 8.999858\n",
      "iteration 900 / 1000: loss 8.999846\n",
      "training accuracy: 0.376612\n",
      "validation accuracy: 0.376000\n",
      "\n",
      "\n",
      "Starting iteration with rate 8.497534e-07 and regstrength 2.375405e+06\n",
      "iteration 0 / 1000: loss 3389.026522\n",
      "iteration 100 / 1000: loss 10432104008748550524836564356481661922999856419742254908494512634107584600670514529528743730054430720.000000\n",
      "iteration 200 / 1000: loss 32197628892087847642225837575484904763417723430739226361477109402531904543936937964336422319329206158007756143374343136822985808751441469954511784378148218716705359363591183823363893144747803410432.000000\n",
      "iteration 300 / 1000: loss 99374709589093987750797092499887520700985007028280143887441677894924804628932999262637591881558767733792819004263052098826020174102060912504545240571279221868289031576188984212336361460084126548403393324237475884049943007023600929406758850230021963868201332747498054081980283036678360741707776.000000\n",
      "iteration 400 / 1000: loss inf\n",
      "iteration 500 / 1000: loss inf\n",
      "iteration 600 / 1000: loss inf\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 3.593814e-04 and regstrength 1.675801e+05\n",
      "iteration 0 / 1000: loss 267.506967\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 2.535364e-07 and regstrength 1.267682e+05\n",
      "iteration 0 / 1000: loss 192.141234\n",
      "iteration 100 / 1000: loss 9.000138\n",
      "iteration 200 / 1000: loss 8.999873\n",
      "iteration 300 / 1000: loss 8.999841\n",
      "iteration 400 / 1000: loss 8.999811\n",
      "iteration 500 / 1000: loss 8.999873\n",
      "iteration 600 / 1000: loss 8.999872\n",
      "iteration 700 / 1000: loss 8.999866\n",
      "iteration 800 / 1000: loss 8.999857\n",
      "iteration 900 / 1000: loss 8.999862\n",
      "training accuracy: 0.405102\n",
      "validation accuracy: 0.405000\n",
      "\n",
      "\n",
      "Starting iteration with rate 8.697490e-06 and regstrength 5.487494e+04\n",
      "iteration 0 / 1000: loss 94.809842\n",
      "iteration 100 / 1000: loss 8.999917\n",
      "iteration 200 / 1000: loss 8.999972\n",
      "iteration 300 / 1000: loss 9.000037\n",
      "iteration 400 / 1000: loss 8.999951\n",
      "iteration 500 / 1000: loss 8.999947\n",
      "iteration 600 / 1000: loss 8.999986\n",
      "iteration 700 / 1000: loss 9.000032\n",
      "iteration 800 / 1000: loss 9.000035\n",
      "iteration 900 / 1000: loss 9.000041\n",
      "training accuracy: 0.348061\n",
      "validation accuracy: 0.331000\n",
      "\n",
      "\n",
      "Starting iteration with rate 8.111308e-05 and regstrength 6.924432e+04\n",
      "iteration 0 / 1000: loss 114.048500\n",
      "iteration 100 / 1000: loss 1056876085913078989419366329052357230212525843688298805217313211134968969099901250345747251436599597370605316404498958506726360978685954207580767037202961829081413320006732245270902370043404912946296389632.000000\n",
      "iteration 200 / 1000: loss inf\n",
      "iteration 300 / 1000: loss inf\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 2.983647e-04 and regstrength 6.765239e+05\n",
      "iteration 0 / 1000: loss 1077.793593\n",
      "iteration 100 / 1000: loss inf\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.668101e-05 and regstrength 5.238079e+04\n",
      "iteration 0 / 1000: loss 90.757072\n",
      "iteration 100 / 1000: loss 9.001615\n",
      "iteration 200 / 1000: loss 9.001824\n",
      "iteration 300 / 1000: loss 9.001952\n",
      "iteration 400 / 1000: loss 9.002497\n",
      "iteration 500 / 1000: loss 9.001953\n",
      "iteration 600 / 1000: loss 9.001950\n",
      "iteration 700 / 1000: loss 9.002049\n",
      "iteration 800 / 1000: loss 9.001675\n",
      "iteration 900 / 1000: loss 9.001964\n",
      "training accuracy: 0.233837\n",
      "validation accuracy: 0.222000\n",
      "\n",
      "\n",
      "lr 1.000000e-07 reg 1.182245e+06 train accuracy: 0.390735 val accuracy: 0.380000\n",
      "lr 1.097499e-07 reg 7.087371e+05 train accuracy: 0.386408 val accuracy: 0.365000\n",
      "lr 1.204504e-07 reg 1.599634e+05 train accuracy: 0.412796 val accuracy: 0.408000\n",
      "lr 1.321941e-07 reg 2.164381e+06 train accuracy: 0.366612 val accuracy: 0.390000\n",
      "lr 1.450829e-07 reg 1.562858e+06 train accuracy: 0.370592 val accuracy: 0.359000\n",
      "lr 1.592283e-07 reg 7.778381e+05 train accuracy: 0.381531 val accuracy: 0.373000\n",
      "lr 1.747528e-07 reg 2.215311e+05 train accuracy: 0.413327 val accuracy: 0.426000\n",
      "lr 1.917910e-07 reg 2.731139e+06 train accuracy: 0.306449 val accuracy: 0.337000\n",
      "lr 2.104904e-07 reg 3.214037e+05 train accuracy: 0.406061 val accuracy: 0.388000\n",
      "lr 2.310130e-07 reg 1.077217e+06 train accuracy: 0.373796 val accuracy: 0.365000\n",
      "lr 2.535364e-07 reg 1.267682e+05 train accuracy: 0.405102 val accuracy: 0.405000\n",
      "lr 2.782559e-07 reg 1.637275e+06 train accuracy: 0.339857 val accuracy: 0.326000\n",
      "lr 3.053856e-07 reg 6.022518e+04 train accuracy: 0.408143 val accuracy: 0.402000\n",
      "lr 3.351603e-07 reg 7.254144e+04 train accuracy: 0.409408 val accuracy: 0.413000\n",
      "lr 3.678380e-07 reg 2.607004e+06 train accuracy: 0.214735 val accuracy: 0.214000\n",
      "lr 4.037017e-07 reg 1.102565e+05 train accuracy: 0.415857 val accuracy: 0.414000\n",
      "lr 4.430621e-07 reg 1.882468e+06 train accuracy: 0.234980 val accuracy: 0.236000\n",
      "lr 4.862602e-07 reg 2.488512e+06 train accuracy: 0.088612 val accuracy: 0.086000\n",
      "lr 5.336699e-07 reg 4.348745e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.857021e-07 reg 1.526928e+05 train accuracy: 0.394755 val accuracy: 0.380000\n",
      "lr 6.428073e-07 reg 3.140146e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.054802e-07 reg 1.052452e+05 train accuracy: 0.403673 val accuracy: 0.396000\n",
      "lr 7.742637e-07 reg 1.028256e+06 train accuracy: 0.227980 val accuracy: 0.213000\n",
      "lr 8.497534e-07 reg 2.375405e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 9.326033e-07 reg 8.536763e+05 train accuracy: 0.291265 val accuracy: 0.312000\n",
      "lr 1.023531e-06 reg 5.884060e+05 train accuracy: 0.306347 val accuracy: 0.300000\n",
      "lr 1.123324e-06 reg 8.943248e+05 train accuracy: 0.116327 val accuracy: 0.117000\n",
      "lr 1.232847e-06 reg 4.663017e+05 train accuracy: 0.319000 val accuracy: 0.327000\n",
      "lr 1.353048e-06 reg 3.871318e+05 train accuracy: 0.341000 val accuracy: 0.354000\n",
      "lr 1.484968e-06 reg 2.267439e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.629751e-06 reg 2.114621e+05 train accuracy: 0.352347 val accuracy: 0.381000\n",
      "lr 1.788650e-06 reg 4.885050e+05 train accuracy: 0.239816 val accuracy: 0.244000\n",
      "lr 1.963041e-06 reg 3.067954e+05 train accuracy: 0.292286 val accuracy: 0.291000\n",
      "lr 2.154435e-06 reg 1.297512e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.364489e-06 reg 5.361336e+05 train accuracy: 0.088490 val accuracy: 0.105000\n",
      "lr 2.595024e-06 reg 3.367075e+05 train accuracy: 0.229327 val accuracy: 0.233000\n",
      "lr 2.848036e-06 reg 7.599555e+04 train accuracy: 0.375939 val accuracy: 0.404000\n",
      "lr 3.125716e-06 reg 5.748785e+04 train accuracy: 0.395837 val accuracy: 0.389000\n",
      "lr 3.430469e-06 reg 6.309284e+04 train accuracy: 0.373673 val accuracy: 0.372000\n",
      "lr 3.764936e-06 reg 4.555814e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.132012e-06 reg 6.609706e+04 train accuracy: 0.376612 val accuracy: 0.376000\n",
      "lr 4.534879e-06 reg 1.155065e+05 train accuracy: 0.312776 val accuracy: 0.309000\n",
      "lr 4.977024e-06 reg 5.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.462277e-06 reg 1.359294e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e-06 reg 3.695361e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 6.579332e-06 reg 9.369087e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.220809e-06 reg 1.926764e+05 train accuracy: 0.079265 val accuracy: 0.076000\n",
      "lr 7.924829e-06 reg 8.148754e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 8.697490e-06 reg 5.487494e+04 train accuracy: 0.348061 val accuracy: 0.331000\n",
      "lr 9.545485e-06 reg 5.000000e+04 train accuracy: 0.327245 val accuracy: 0.319000\n",
      "lr 1.047616e-05 reg 4.055654e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.149757e-05 reg 1.457527e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.261857e-05 reg 2.320794e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.384886e-05 reg 2.861184e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.519911e-05 reg 7.424841e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.668101e-05 reg 5.238079e+04 train accuracy: 0.233837 val accuracy: 0.222000\n",
      "lr 1.830738e-05 reg 1.715235e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.009233e-05 reg 3.289666e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.205131e-05 reg 4.248767e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.420128e-05 reg 3.446306e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.656088e-05 reg 9.153691e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.915053e-05 reg 2.928510e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.199267e-05 reg 8.340503e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.511192e-05 reg 2.795405e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.853529e-05 reg 1.238538e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.229243e-05 reg 2.066006e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-05 reg 7.961414e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.094138e-05 reg 4.451075e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.590810e-05 reg 9.815203e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 6.135907e-05 reg 1.972103e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 6.734151e-05 reg 2.997421e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.390722e-05 reg 1.328044e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 8.111308e-05 reg 6.924432e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 8.902151e-05 reg 8.737642e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 9.770100e-05 reg 1.210064e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.072267e-04 reg 2.431301e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.176812e-04 reg 5.616620e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e-04 reg 4.772742e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.417474e-04 reg 2.668350e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.555676e-04 reg 6.164234e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.707353e-04 reg 9.589551e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.873817e-04 reg 1.004617e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.056512e-04 reg 1.424018e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.257020e-04 reg 6.457748e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.477076e-04 reg 1.839190e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.718588e-04 reg 1.755596e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.983647e-04 reg 6.765239e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.274549e-04 reg 4.151088e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e-04 reg 1.675801e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.944206e-04 reg 3.962414e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.328761e-04 reg 1.391280e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.750810e-04 reg 3.610405e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.214008e-04 reg 1.796907e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.722368e-04 reg 3.782317e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 6.280291e-04 reg 2.547069e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 6.892612e-04 reg 3.527401e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.564633e-04 reg 1.128510e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 8.302176e-04 reg 2.018509e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 9.111628e-04 reg 1.491824e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-03 reg 5.117655e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "best validation accuracy achieved during cross-validation: 0.426000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune the learning rate and regularization strength\n",
    "\n",
    "from cs231n.classifiers.linear_classifier import LinearSVM\n",
    "\n",
    "learning_rates = np.log10([1e-7, 1e-3])\n",
    "regularization_strengths = np.log10([5e4, 5e6])\n",
    "\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_svm = None\n",
    "\n",
    "pass\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained classifer in best_svm. You might also want to play          #\n",
    "# with different numbers of bins in the color histogram. If you are careful    #\n",
    "# you should be able to get accuracy of near 0.44 on the validation set.       #\n",
    "################################################################################\n",
    "\n",
    "num_params = 100\n",
    "range_lr = np.logspace(learning_rates[0],learning_rates[1],num_params)\n",
    "range_reg = np.logspace(regularization_strengths[0],regularization_strengths[1],num_params)\n",
    "\n",
    "range_lr = range_lr[np.random.permutation(num_params)]\n",
    "range_reg = range_reg[np.random.permutation(num_params)]\n",
    "\n",
    "for learning_rate, regularization_strength in zip(range_lr, range_reg):\n",
    "    #train\n",
    "    print(\"Starting iteration with rate %e and regstrength %e\"%(learning_rate,regularization_strength))\n",
    "    svm = LinearSVM()\n",
    "    loss_hist = svm.train(X_train_feats, y_train, learning_rate=learning_rate, reg=regularization_strength,\n",
    "                          num_iters=1000, verbose=True)\n",
    "    \n",
    "    y_train_feats_pred = svm.predict(X_train_feats)\n",
    "    train_acc = np.mean(y_train == y_train_feats_pred)\n",
    "    print('training accuracy: %f' % train_acc)\n",
    "    y_val_feats_pred = svm.predict(X_val_feats)\n",
    "    val_acc = np.mean(y_val == y_val_feats_pred)\n",
    "    print('validation accuracy: %f' % val_acc)\n",
    "    print('\\n')\n",
    "    \n",
    "    results[(learning_rate,regularization_strength)]=(train_acc,val_acc)\n",
    "    if val_acc > best_val:\n",
    "        best_val = val_acc\n",
    "        best_svm = svm\n",
    "\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "\n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41\n"
     ]
    }
   ],
   "source": [
    "# Evaluate your trained SVM on the test set\n",
    "y_test_pred = best_svm.predict(X_test_feats)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 80 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# An important way to gain intuition about how an algorithm works is to\n",
    "# visualize the mistakes that it makes. In this visualization, we show examples\n",
    "# of images that are misclassified by our current system. The first column\n",
    "# shows images that our system labeled as \"plane\" but whose true label is\n",
    "# something other than \"plane\".\n",
    "\n",
    "examples_per_class = 8\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for cls, cls_name in enumerate(classes):\n",
    "    idxs = np.where((y_test != cls) & (y_test_pred == cls))[0]\n",
    "    idxs = np.random.choice(idxs, examples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt.subplot(examples_per_class, len(classes), i * len(classes) + cls + 1)\n",
    "        plt.imshow(X_test[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inline question 1:\n",
    "Describe the misclassification results that you see. Do they make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network on image features\n",
    "Earlier in this assigment we saw that training a two-layer neural network on raw pixels achieved better classification performance than linear classifiers on raw pixels. In this notebook we have seen that linear classifiers on image features outperform linear classifiers on raw pixels. \n",
    "\n",
    "For completeness, we should also try training a neural network on image features. This approach should outperform all previous approaches: you should easily be able to achieve over 55% classification accuracy on the test set; our best model achieves about 60% classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49000, 155)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration with rate 3.759879e-01, regstrength 1.325711e-03 and hidden layer size 14\n",
      "iteration 0 / 10000: loss 2.302585\n",
      "iteration 100 / 10000: loss 1.798546\n",
      "iteration 200 / 10000: loss 1.439739\n",
      "iteration 300 / 10000: loss 1.308191\n",
      "iteration 400 / 10000: loss 1.470945\n",
      "iteration 500 / 10000: loss 1.373103\n",
      "iteration 600 / 10000: loss 1.452514\n",
      "iteration 700 / 10000: loss 1.405214\n",
      "iteration 800 / 10000: loss 1.535544\n",
      "iteration 900 / 10000: loss 1.564349\n",
      "iteration 1000 / 10000: loss 1.321581\n",
      "iteration 1100 / 10000: loss 1.347691\n",
      "iteration 1200 / 10000: loss 1.352269\n",
      "iteration 1300 / 10000: loss 1.397684\n",
      "iteration 1400 / 10000: loss 1.359335\n",
      "iteration 1500 / 10000: loss 1.416243\n",
      "iteration 1600 / 10000: loss 1.380701\n",
      "iteration 1700 / 10000: loss 1.509528\n",
      "iteration 1800 / 10000: loss 1.471577\n",
      "iteration 1900 / 10000: loss 1.345650\n",
      "iteration 2000 / 10000: loss 1.585643\n",
      "iteration 2100 / 10000: loss 1.553012\n",
      "iteration 2200 / 10000: loss 1.271631\n",
      "iteration 2300 / 10000: loss 1.437356\n",
      "iteration 2400 / 10000: loss 1.369358\n",
      "iteration 2500 / 10000: loss 1.337443\n",
      "iteration 2600 / 10000: loss 1.330269\n",
      "iteration 2700 / 10000: loss 1.293554\n",
      "iteration 2800 / 10000: loss 1.335832\n",
      "iteration 2900 / 10000: loss 1.336954\n",
      "iteration 3000 / 10000: loss 1.331236\n",
      "iteration 3100 / 10000: loss 1.285423\n",
      "iteration 3200 / 10000: loss 1.424721\n",
      "iteration 3300 / 10000: loss 1.358280\n",
      "iteration 3400 / 10000: loss 1.370228\n",
      "iteration 3500 / 10000: loss 1.348723\n",
      "iteration 3600 / 10000: loss 1.354578\n",
      "iteration 3700 / 10000: loss 1.405466\n",
      "iteration 3800 / 10000: loss 1.400735\n",
      "iteration 3900 / 10000: loss 1.435974\n",
      "iteration 4000 / 10000: loss 1.531892\n",
      "iteration 4100 / 10000: loss 1.474990\n",
      "iteration 4200 / 10000: loss 1.489757\n",
      "iteration 4300 / 10000: loss 1.460184\n",
      "iteration 4400 / 10000: loss 1.436995\n",
      "iteration 4500 / 10000: loss 1.431416\n",
      "iteration 4600 / 10000: loss 1.380019\n",
      "iteration 4700 / 10000: loss 1.280686\n",
      "iteration 4800 / 10000: loss 1.305223\n",
      "iteration 4900 / 10000: loss 1.333744\n",
      "iteration 5000 / 10000: loss 1.442641\n",
      "iteration 5100 / 10000: loss 1.344078\n",
      "iteration 5200 / 10000: loss 1.417643\n",
      "iteration 5300 / 10000: loss 1.229625\n",
      "iteration 5400 / 10000: loss 1.333321\n",
      "iteration 5500 / 10000: loss 1.441829\n",
      "iteration 5600 / 10000: loss 1.489466\n",
      "iteration 5700 / 10000: loss 1.331882\n",
      "iteration 5800 / 10000: loss 1.325126\n",
      "iteration 5900 / 10000: loss 1.205649\n",
      "iteration 6000 / 10000: loss 1.476452\n",
      "iteration 6100 / 10000: loss 1.408839\n",
      "iteration 6200 / 10000: loss 1.394317\n",
      "iteration 6300 / 10000: loss 1.454953\n",
      "iteration 6400 / 10000: loss 1.406637\n",
      "iteration 6500 / 10000: loss 1.392485\n",
      "iteration 6600 / 10000: loss 1.434362\n",
      "iteration 6700 / 10000: loss 1.363869\n",
      "iteration 6800 / 10000: loss 1.328440\n",
      "iteration 6900 / 10000: loss 1.342974\n",
      "iteration 7000 / 10000: loss 1.331250\n",
      "iteration 7100 / 10000: loss 1.266146\n",
      "iteration 7200 / 10000: loss 1.417940\n",
      "iteration 7300 / 10000: loss 1.348369\n",
      "iteration 7400 / 10000: loss 1.337853\n",
      "iteration 7500 / 10000: loss 1.381506\n",
      "iteration 7600 / 10000: loss 1.342966\n",
      "iteration 7700 / 10000: loss 1.349390\n",
      "iteration 7800 / 10000: loss 1.395687\n",
      "iteration 7900 / 10000: loss 1.240774\n",
      "iteration 8000 / 10000: loss 1.207554\n",
      "iteration 8100 / 10000: loss 1.358953\n",
      "iteration 8200 / 10000: loss 1.359079\n",
      "iteration 8300 / 10000: loss 1.363960\n",
      "iteration 8400 / 10000: loss 1.402736\n",
      "iteration 8500 / 10000: loss 1.258869\n",
      "iteration 8600 / 10000: loss 1.290282\n",
      "iteration 8700 / 10000: loss 1.377629\n",
      "iteration 8800 / 10000: loss 1.328127\n",
      "iteration 8900 / 10000: loss 1.444255\n",
      "iteration 9000 / 10000: loss 1.356865\n",
      "iteration 9100 / 10000: loss 1.323460\n",
      "iteration 9200 / 10000: loss 1.366445\n",
      "iteration 9300 / 10000: loss 1.344125\n",
      "iteration 9400 / 10000: loss 1.324945\n",
      "iteration 9500 / 10000: loss 1.364326\n",
      "iteration 9600 / 10000: loss 1.327513\n",
      "iteration 9700 / 10000: loss 1.462059\n",
      "iteration 9800 / 10000: loss 1.489011\n",
      "iteration 9900 / 10000: loss 1.304059\n",
      "Validation accuracy:  0.52\n",
      "\n",
      "\n",
      "prev best val: 0.000000e+00, next best val: 5.200000e-01\n",
      "Starting iteration with rate 6.385230e-02, regstrength 1.000000e-03 and hidden layer size 38\n",
      "iteration 0 / 10000: loss 2.302585\n",
      "iteration 100 / 10000: loss 2.301329\n",
      "iteration 200 / 10000: loss 2.302778\n",
      "iteration 300 / 10000: loss 2.282659\n",
      "iteration 400 / 10000: loss 2.031474\n",
      "iteration 500 / 10000: loss 1.864004\n",
      "iteration 600 / 10000: loss 1.701629\n",
      "iteration 700 / 10000: loss 1.570551\n",
      "iteration 800 / 10000: loss 1.692798\n",
      "iteration 900 / 10000: loss 1.496526\n",
      "iteration 1000 / 10000: loss 1.426567\n",
      "iteration 1100 / 10000: loss 1.391267\n",
      "iteration 1200 / 10000: loss 1.521018\n",
      "iteration 1300 / 10000: loss 1.369862\n",
      "iteration 1400 / 10000: loss 1.427483\n",
      "iteration 1500 / 10000: loss 1.400099\n",
      "iteration 1600 / 10000: loss 1.447018\n",
      "iteration 1700 / 10000: loss 1.371444\n",
      "iteration 1800 / 10000: loss 1.403842\n",
      "iteration 1900 / 10000: loss 1.323307\n",
      "iteration 2000 / 10000: loss 1.276318\n",
      "iteration 2100 / 10000: loss 1.344171\n",
      "iteration 2200 / 10000: loss 1.253845\n",
      "iteration 2300 / 10000: loss 1.458387\n",
      "iteration 2400 / 10000: loss 1.403400\n",
      "iteration 2500 / 10000: loss 1.339591\n",
      "iteration 2600 / 10000: loss 1.316631\n",
      "iteration 2700 / 10000: loss 1.375853\n",
      "iteration 2800 / 10000: loss 1.425968\n",
      "iteration 2900 / 10000: loss 1.286497\n",
      "iteration 3000 / 10000: loss 1.333565\n",
      "iteration 3100 / 10000: loss 1.395560\n",
      "iteration 3200 / 10000: loss 1.166783\n",
      "iteration 3300 / 10000: loss 1.368153\n",
      "iteration 3400 / 10000: loss 1.205419\n",
      "iteration 3500 / 10000: loss 1.440863\n",
      "iteration 3600 / 10000: loss 1.385292\n",
      "iteration 3700 / 10000: loss 1.243539\n",
      "iteration 3800 / 10000: loss 1.350852\n",
      "iteration 3900 / 10000: loss 1.381027\n",
      "iteration 4000 / 10000: loss 1.226668\n",
      "iteration 4100 / 10000: loss 1.229122\n",
      "iteration 4200 / 10000: loss 1.201103\n",
      "iteration 4300 / 10000: loss 1.263875\n",
      "iteration 4400 / 10000: loss 1.201815\n",
      "iteration 4500 / 10000: loss 1.214723\n",
      "iteration 4600 / 10000: loss 1.304380\n",
      "iteration 4700 / 10000: loss 1.375838\n",
      "iteration 4800 / 10000: loss 1.278727\n",
      "iteration 4900 / 10000: loss 1.333796\n",
      "iteration 5000 / 10000: loss 1.342048\n",
      "iteration 5100 / 10000: loss 1.148688\n",
      "iteration 5200 / 10000: loss 1.330768\n",
      "iteration 5300 / 10000: loss 1.318997\n",
      "iteration 5400 / 10000: loss 1.355225\n",
      "iteration 5500 / 10000: loss 1.206830\n",
      "iteration 5600 / 10000: loss 1.287220\n",
      "iteration 5700 / 10000: loss 1.327969\n",
      "iteration 5800 / 10000: loss 1.177285\n",
      "iteration 5900 / 10000: loss 1.395422\n",
      "iteration 6000 / 10000: loss 1.304620\n",
      "iteration 6100 / 10000: loss 1.277928\n",
      "iteration 6200 / 10000: loss 1.233297\n",
      "iteration 6300 / 10000: loss 1.187041\n",
      "iteration 6400 / 10000: loss 1.388755\n",
      "iteration 6500 / 10000: loss 1.107286\n",
      "iteration 6600 / 10000: loss 1.198003\n",
      "iteration 6700 / 10000: loss 1.228450\n",
      "iteration 6800 / 10000: loss 1.400877\n",
      "iteration 6900 / 10000: loss 1.091549\n",
      "iteration 7000 / 10000: loss 1.294707\n",
      "iteration 7100 / 10000: loss 1.194784\n",
      "iteration 7200 / 10000: loss 1.186139\n",
      "iteration 7300 / 10000: loss 1.180331\n",
      "iteration 7400 / 10000: loss 1.205736\n",
      "iteration 7500 / 10000: loss 1.304770\n",
      "iteration 7600 / 10000: loss 1.230329\n",
      "iteration 7700 / 10000: loss 1.328788\n",
      "iteration 7800 / 10000: loss 1.265663\n",
      "iteration 7900 / 10000: loss 1.289935\n",
      "iteration 8000 / 10000: loss 1.213856\n",
      "iteration 8100 / 10000: loss 1.265992\n",
      "iteration 8200 / 10000: loss 1.398260\n",
      "iteration 8300 / 10000: loss 1.150051\n",
      "iteration 8400 / 10000: loss 1.302910\n",
      "iteration 8500 / 10000: loss 1.388173\n",
      "iteration 8600 / 10000: loss 1.233476\n",
      "iteration 8700 / 10000: loss 1.303377\n",
      "iteration 8800 / 10000: loss 1.247004\n",
      "iteration 8900 / 10000: loss 1.186634\n",
      "iteration 9000 / 10000: loss 1.343831\n",
      "iteration 9100 / 10000: loss 1.199102\n",
      "iteration 9200 / 10000: loss 1.151918\n",
      "iteration 9300 / 10000: loss 1.271477\n",
      "iteration 9400 / 10000: loss 1.240591\n",
      "iteration 9500 / 10000: loss 1.255787\n",
      "iteration 9600 / 10000: loss 1.349693\n",
      "iteration 9700 / 10000: loss 1.217169\n",
      "iteration 9800 / 10000: loss 1.240734\n",
      "iteration 9900 / 10000: loss 1.249559\n",
      "Validation accuracy:  0.574\n",
      "\n",
      "\n",
      "prev best val: 5.200000e-01, next best val: 5.740000e-01\n",
      "Starting iteration with rate 7.830572e-01, regstrength 6.250552e-03 and hidden layer size 31\n",
      "iteration 0 / 10000: loss 2.302585\n",
      "iteration 100 / 10000: loss 1.721923\n",
      "iteration 200 / 10000: loss 1.716951\n",
      "iteration 300 / 10000: loss 1.685176\n",
      "iteration 400 / 10000: loss 1.615686\n",
      "iteration 500 / 10000: loss 1.513085\n",
      "iteration 600 / 10000: loss 1.471957\n",
      "iteration 700 / 10000: loss 1.557637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 800 / 10000: loss 1.601673\n",
      "iteration 900 / 10000: loss 1.537863\n",
      "iteration 1000 / 10000: loss 1.546000\n",
      "iteration 1100 / 10000: loss 1.554494\n",
      "iteration 1200 / 10000: loss 1.535748\n",
      "iteration 1300 / 10000: loss 1.563875\n",
      "iteration 1400 / 10000: loss 1.508488\n",
      "iteration 1500 / 10000: loss 1.542553\n",
      "iteration 1600 / 10000: loss 1.695247\n",
      "iteration 1700 / 10000: loss 1.600893\n",
      "iteration 1800 / 10000: loss 1.445977\n",
      "iteration 1900 / 10000: loss 1.588069\n",
      "iteration 2000 / 10000: loss 1.516197\n",
      "iteration 2100 / 10000: loss 1.662731\n",
      "iteration 2200 / 10000: loss 1.470607\n",
      "iteration 2300 / 10000: loss 1.543173\n",
      "iteration 2400 / 10000: loss 1.483731\n",
      "iteration 2500 / 10000: loss 1.450765\n",
      "iteration 2600 / 10000: loss 1.603380\n",
      "iteration 2700 / 10000: loss 1.502098\n",
      "iteration 2800 / 10000: loss 1.545946\n",
      "iteration 2900 / 10000: loss 1.527564\n",
      "iteration 3000 / 10000: loss 1.534474\n",
      "iteration 3100 / 10000: loss 1.401210\n",
      "iteration 3200 / 10000: loss 1.494369\n",
      "iteration 3300 / 10000: loss 1.536333\n",
      "iteration 3400 / 10000: loss 1.491231\n",
      "iteration 3500 / 10000: loss 1.496931\n",
      "iteration 3600 / 10000: loss 1.578038\n",
      "iteration 3700 / 10000: loss 1.572103\n",
      "iteration 3800 / 10000: loss 1.434451\n",
      "iteration 3900 / 10000: loss 1.476662\n",
      "iteration 4000 / 10000: loss 1.653274\n",
      "iteration 4100 / 10000: loss 1.563414\n",
      "iteration 4200 / 10000: loss 1.534336\n",
      "iteration 4300 / 10000: loss 1.569227\n",
      "iteration 4400 / 10000: loss 1.545914\n",
      "iteration 4500 / 10000: loss 1.605297\n",
      "iteration 4600 / 10000: loss 1.569482\n",
      "iteration 4700 / 10000: loss 1.548333\n",
      "iteration 4800 / 10000: loss 1.455754\n",
      "iteration 4900 / 10000: loss 1.515114\n",
      "iteration 5000 / 10000: loss 1.481114\n",
      "iteration 5100 / 10000: loss 1.486569\n",
      "iteration 5200 / 10000: loss 1.516670\n",
      "iteration 5300 / 10000: loss 1.449713\n",
      "iteration 5400 / 10000: loss 1.465487\n",
      "iteration 5500 / 10000: loss 1.604642\n",
      "iteration 5600 / 10000: loss 1.557219\n",
      "iteration 5700 / 10000: loss 1.454617\n",
      "iteration 5800 / 10000: loss 1.477047\n",
      "iteration 5900 / 10000: loss 1.550795\n",
      "iteration 6000 / 10000: loss 1.468841\n",
      "iteration 6100 / 10000: loss 1.580305\n",
      "iteration 6200 / 10000: loss 1.565419\n",
      "iteration 6300 / 10000: loss 1.608925\n",
      "iteration 6400 / 10000: loss 1.439036\n",
      "iteration 6500 / 10000: loss 1.550000\n",
      "iteration 6600 / 10000: loss 1.473682\n",
      "iteration 6700 / 10000: loss 1.427444\n",
      "iteration 6800 / 10000: loss 1.456569\n",
      "iteration 6900 / 10000: loss 1.308410\n",
      "iteration 7000 / 10000: loss 1.399979\n",
      "iteration 7100 / 10000: loss 1.558645\n",
      "iteration 7200 / 10000: loss 1.623897\n",
      "iteration 7300 / 10000: loss 1.534281\n",
      "iteration 7400 / 10000: loss 1.425266\n",
      "iteration 7500 / 10000: loss 1.521555\n",
      "iteration 7600 / 10000: loss 1.559717\n",
      "iteration 7700 / 10000: loss 1.612707\n",
      "iteration 7800 / 10000: loss 1.571227\n",
      "iteration 7900 / 10000: loss 1.640792\n",
      "iteration 8000 / 10000: loss 1.546941\n",
      "iteration 8100 / 10000: loss 1.398566\n",
      "iteration 8200 / 10000: loss 1.528427\n",
      "iteration 8300 / 10000: loss 1.522418\n",
      "iteration 8400 / 10000: loss 1.573224\n",
      "iteration 8500 / 10000: loss 1.462509\n",
      "iteration 8600 / 10000: loss 1.560636\n",
      "iteration 8700 / 10000: loss 1.488323\n",
      "iteration 8800 / 10000: loss 1.475072\n",
      "iteration 8900 / 10000: loss 1.535241\n",
      "iteration 9000 / 10000: loss 1.428229\n",
      "iteration 9100 / 10000: loss 1.517698\n",
      "iteration 9200 / 10000: loss 1.513142\n",
      "iteration 9300 / 10000: loss 1.600594\n",
      "iteration 9400 / 10000: loss 1.552518\n",
      "iteration 9500 / 10000: loss 1.395192\n",
      "iteration 9600 / 10000: loss 1.602961\n",
      "iteration 9700 / 10000: loss 1.530199\n",
      "iteration 9800 / 10000: loss 1.452092\n",
      "iteration 9900 / 10000: loss 1.601570\n",
      "Validation accuracy:  0.555\n",
      "\n",
      "\n",
      "Starting iteration with rate 7.366172e-01, regstrength 3.727594e-01 and hidden layer size 41\n",
      "iteration 0 / 10000: loss 2.302611\n",
      "iteration 100 / 10000: loss 2.303401\n",
      "iteration 200 / 10000: loss 2.302327\n",
      "iteration 300 / 10000: loss 2.302572\n",
      "iteration 400 / 10000: loss 2.304579\n",
      "iteration 500 / 10000: loss 2.304321\n",
      "iteration 600 / 10000: loss 2.305758\n",
      "iteration 700 / 10000: loss 2.306318\n",
      "iteration 800 / 10000: loss 2.304175\n",
      "iteration 900 / 10000: loss 2.304830\n",
      "iteration 1000 / 10000: loss 2.302369\n",
      "iteration 1100 / 10000: loss 2.306693\n",
      "iteration 1200 / 10000: loss 2.302533\n",
      "iteration 1300 / 10000: loss 2.304024\n",
      "iteration 1400 / 10000: loss 2.302029\n",
      "iteration 1500 / 10000: loss 2.301976\n",
      "iteration 1600 / 10000: loss 2.301652\n",
      "iteration 1700 / 10000: loss 2.303281\n",
      "iteration 1800 / 10000: loss 2.304433\n",
      "iteration 1900 / 10000: loss 2.304565\n",
      "iteration 2000 / 10000: loss 2.302226\n",
      "iteration 2100 / 10000: loss 2.302272\n",
      "iteration 2200 / 10000: loss 2.304409\n",
      "iteration 2300 / 10000: loss 2.306654\n",
      "iteration 2400 / 10000: loss 2.302490\n",
      "iteration 2500 / 10000: loss 2.300689\n",
      "iteration 2600 / 10000: loss 2.301547\n",
      "iteration 2700 / 10000: loss 2.303934\n",
      "iteration 2800 / 10000: loss 2.304331\n",
      "iteration 2900 / 10000: loss 2.303044\n",
      "iteration 3000 / 10000: loss 2.302308\n",
      "iteration 3100 / 10000: loss 2.303541\n",
      "iteration 3200 / 10000: loss 2.303799\n",
      "iteration 3300 / 10000: loss 2.303434\n",
      "iteration 3400 / 10000: loss 2.305357\n",
      "iteration 3500 / 10000: loss 2.304109\n",
      "iteration 3600 / 10000: loss 2.301293\n",
      "iteration 3700 / 10000: loss 2.303846\n",
      "iteration 3800 / 10000: loss 2.303575\n",
      "iteration 3900 / 10000: loss 2.303985\n",
      "iteration 4000 / 10000: loss 2.302678\n",
      "iteration 4100 / 10000: loss 2.304294\n",
      "iteration 4200 / 10000: loss 2.304815\n",
      "iteration 4300 / 10000: loss 2.303634\n",
      "iteration 4400 / 10000: loss 2.302912\n",
      "iteration 4500 / 10000: loss 2.301799\n",
      "iteration 4600 / 10000: loss 2.302937\n",
      "iteration 4700 / 10000: loss 2.301881\n",
      "iteration 4800 / 10000: loss 2.304151\n",
      "iteration 4900 / 10000: loss 2.302826\n",
      "iteration 5000 / 10000: loss 2.306152\n",
      "iteration 5100 / 10000: loss 2.302607\n",
      "iteration 5200 / 10000: loss 2.302713\n",
      "iteration 5300 / 10000: loss 2.303355\n",
      "iteration 5400 / 10000: loss 2.304019\n",
      "iteration 5500 / 10000: loss 2.302426\n",
      "iteration 5600 / 10000: loss 2.304043\n",
      "iteration 5700 / 10000: loss 2.303100\n",
      "iteration 5800 / 10000: loss 2.302304\n",
      "iteration 5900 / 10000: loss 2.304091\n",
      "iteration 6000 / 10000: loss 2.303705\n",
      "iteration 6100 / 10000: loss 2.302362\n",
      "iteration 6200 / 10000: loss 2.303349\n",
      "iteration 6300 / 10000: loss 2.302982\n",
      "iteration 6400 / 10000: loss 2.304170\n",
      "iteration 6500 / 10000: loss 2.304558\n",
      "iteration 6600 / 10000: loss 2.302428\n",
      "iteration 6700 / 10000: loss 2.302655\n",
      "iteration 6800 / 10000: loss 2.301951\n",
      "iteration 6900 / 10000: loss 2.303497\n",
      "iteration 7000 / 10000: loss 2.302855\n",
      "iteration 7100 / 10000: loss 2.303575\n",
      "iteration 7200 / 10000: loss 2.303237\n",
      "iteration 7300 / 10000: loss 2.302028\n",
      "iteration 7400 / 10000: loss 2.302909\n",
      "iteration 7500 / 10000: loss 2.304103\n",
      "iteration 7600 / 10000: loss 2.303121\n",
      "iteration 7700 / 10000: loss 2.302383\n",
      "iteration 7800 / 10000: loss 2.302309\n",
      "iteration 7900 / 10000: loss 2.302692\n",
      "iteration 8000 / 10000: loss 2.302519\n",
      "iteration 8100 / 10000: loss 2.303823\n",
      "iteration 8200 / 10000: loss 2.302672\n",
      "iteration 8300 / 10000: loss 2.302710\n",
      "iteration 8400 / 10000: loss 2.305803\n",
      "iteration 8500 / 10000: loss 2.302066\n",
      "iteration 8600 / 10000: loss 2.302710\n",
      "iteration 8700 / 10000: loss 2.302486\n",
      "iteration 8800 / 10000: loss 2.303033\n",
      "iteration 8900 / 10000: loss 2.303555\n",
      "iteration 9000 / 10000: loss 2.301063\n",
      "iteration 9100 / 10000: loss 2.302041\n",
      "iteration 9200 / 10000: loss 2.302279\n",
      "iteration 9300 / 10000: loss 2.302772\n",
      "iteration 9400 / 10000: loss 2.303406\n",
      "iteration 9500 / 10000: loss 2.303614\n",
      "iteration 9600 / 10000: loss 2.302228\n",
      "iteration 9700 / 10000: loss 2.302605\n",
      "iteration 9800 / 10000: loss 2.302572\n",
      "iteration 9900 / 10000: loss 2.302653\n",
      "Validation accuracy:  0.098\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.041333e-01, regstrength 1.930698e-02 and hidden layer size 25\n",
      "iteration 0 / 10000: loss 2.302586\n",
      "iteration 100 / 10000: loss 2.302501\n",
      "iteration 200 / 10000: loss 2.288107\n",
      "iteration 300 / 10000: loss 2.106262\n",
      "iteration 400 / 10000: loss 1.917271\n",
      "iteration 500 / 10000: loss 1.906713\n",
      "iteration 600 / 10000: loss 1.724439\n",
      "iteration 700 / 10000: loss 1.732085\n",
      "iteration 800 / 10000: loss 1.763519\n",
      "iteration 900 / 10000: loss 1.794394\n",
      "iteration 1000 / 10000: loss 1.608869\n",
      "iteration 1100 / 10000: loss 1.803292\n",
      "iteration 1200 / 10000: loss 1.776524\n",
      "iteration 1300 / 10000: loss 1.749328\n",
      "iteration 1400 / 10000: loss 1.769725\n",
      "iteration 1500 / 10000: loss 1.685374\n",
      "iteration 1600 / 10000: loss 1.686692\n",
      "iteration 1700 / 10000: loss 1.776294\n",
      "iteration 1800 / 10000: loss 1.723227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1900 / 10000: loss 1.806458\n",
      "iteration 2000 / 10000: loss 1.712170\n",
      "iteration 2100 / 10000: loss 1.637752\n",
      "iteration 2200 / 10000: loss 1.635110\n",
      "iteration 2300 / 10000: loss 1.795557\n",
      "iteration 2400 / 10000: loss 1.697637\n",
      "iteration 2500 / 10000: loss 1.678320\n",
      "iteration 2600 / 10000: loss 1.757365\n",
      "iteration 2700 / 10000: loss 1.734027\n",
      "iteration 2800 / 10000: loss 1.774444\n",
      "iteration 2900 / 10000: loss 1.728576\n",
      "iteration 3000 / 10000: loss 1.744804\n",
      "iteration 3100 / 10000: loss 1.774035\n",
      "iteration 3200 / 10000: loss 1.723337\n",
      "iteration 3300 / 10000: loss 1.727210\n",
      "iteration 3400 / 10000: loss 1.760016\n",
      "iteration 3500 / 10000: loss 1.693898\n",
      "iteration 3600 / 10000: loss 1.689227\n",
      "iteration 3700 / 10000: loss 1.598735\n",
      "iteration 3800 / 10000: loss 1.673847\n",
      "iteration 3900 / 10000: loss 1.796223\n",
      "iteration 4000 / 10000: loss 1.770644\n",
      "iteration 4100 / 10000: loss 1.757629\n",
      "iteration 4200 / 10000: loss 1.727983\n",
      "iteration 4300 / 10000: loss 1.726941\n",
      "iteration 4400 / 10000: loss 1.782807\n",
      "iteration 4500 / 10000: loss 1.669170\n",
      "iteration 4600 / 10000: loss 1.817394\n",
      "iteration 4700 / 10000: loss 1.745708\n",
      "iteration 4800 / 10000: loss 1.685337\n",
      "iteration 4900 / 10000: loss 1.754848\n",
      "iteration 5000 / 10000: loss 1.730234\n",
      "iteration 5100 / 10000: loss 1.757101\n",
      "iteration 5200 / 10000: loss 1.742391\n",
      "iteration 5300 / 10000: loss 1.856375\n",
      "iteration 5400 / 10000: loss 1.696518\n",
      "iteration 5500 / 10000: loss 1.759268\n",
      "iteration 5600 / 10000: loss 1.706222\n",
      "iteration 5700 / 10000: loss 1.668700\n",
      "iteration 5800 / 10000: loss 1.690081\n",
      "iteration 5900 / 10000: loss 1.801190\n",
      "iteration 6000 / 10000: loss 1.720108\n",
      "iteration 6100 / 10000: loss 1.710430\n",
      "iteration 6200 / 10000: loss 1.767025\n",
      "iteration 6300 / 10000: loss 1.658030\n",
      "iteration 6400 / 10000: loss 1.774458\n",
      "iteration 6500 / 10000: loss 1.750987\n",
      "iteration 6600 / 10000: loss 1.860077\n",
      "iteration 6700 / 10000: loss 1.692139\n",
      "iteration 6800 / 10000: loss 1.707098\n",
      "iteration 6900 / 10000: loss 1.724895\n",
      "iteration 7000 / 10000: loss 1.762361\n",
      "iteration 7100 / 10000: loss 1.683383\n",
      "iteration 7200 / 10000: loss 1.706838\n",
      "iteration 7300 / 10000: loss 1.688740\n",
      "iteration 7400 / 10000: loss 1.849881\n",
      "iteration 7500 / 10000: loss 1.648948\n",
      "iteration 7600 / 10000: loss 1.696728\n",
      "iteration 7700 / 10000: loss 1.616256\n",
      "iteration 7800 / 10000: loss 1.807953\n",
      "iteration 7900 / 10000: loss 1.826784\n",
      "iteration 8000 / 10000: loss 1.791979\n",
      "iteration 8100 / 10000: loss 1.824903\n",
      "iteration 8200 / 10000: loss 1.690694\n",
      "iteration 8300 / 10000: loss 1.677855\n",
      "iteration 8400 / 10000: loss 1.637237\n",
      "iteration 8500 / 10000: loss 1.741354\n",
      "iteration 8600 / 10000: loss 1.921907\n",
      "iteration 8700 / 10000: loss 1.843224\n",
      "iteration 8800 / 10000: loss 1.784776\n",
      "iteration 8900 / 10000: loss 1.688637\n",
      "iteration 9000 / 10000: loss 1.694875\n",
      "iteration 9100 / 10000: loss 1.760139\n",
      "iteration 9200 / 10000: loss 1.799972\n",
      "iteration 9300 / 10000: loss 1.733102\n",
      "iteration 9400 / 10000: loss 1.811099\n",
      "iteration 9500 / 10000: loss 1.687803\n",
      "iteration 9600 / 10000: loss 1.791542\n",
      "iteration 9700 / 10000: loss 1.716973\n",
      "iteration 9800 / 10000: loss 1.707958\n",
      "iteration 9900 / 10000: loss 1.753676\n",
      "Validation accuracy:  0.507\n",
      "\n",
      "\n",
      "Starting iteration with rate 6.929313e-01, regstrength 4.941713e-01 and hidden layer size 11\n",
      "iteration 0 / 10000: loss 2.302594\n",
      "iteration 100 / 10000: loss 2.303217\n",
      "iteration 200 / 10000: loss 2.303031\n",
      "iteration 300 / 10000: loss 2.304129\n",
      "iteration 400 / 10000: loss 2.303220\n",
      "iteration 500 / 10000: loss 2.302493\n",
      "iteration 600 / 10000: loss 2.303857\n",
      "iteration 700 / 10000: loss 2.304078\n",
      "iteration 800 / 10000: loss 2.303613\n",
      "iteration 900 / 10000: loss 2.305224\n",
      "iteration 1000 / 10000: loss 2.303587\n",
      "iteration 1100 / 10000: loss 2.302713\n",
      "iteration 1200 / 10000: loss 2.302615\n",
      "iteration 1300 / 10000: loss 2.302960\n",
      "iteration 1400 / 10000: loss 2.304209\n",
      "iteration 1500 / 10000: loss 2.302132\n",
      "iteration 1600 / 10000: loss 2.303434\n",
      "iteration 1700 / 10000: loss 2.303900\n",
      "iteration 1800 / 10000: loss 2.302324\n",
      "iteration 1900 / 10000: loss 2.303563\n",
      "iteration 2000 / 10000: loss 2.305039\n",
      "iteration 2100 / 10000: loss 2.303408\n",
      "iteration 2200 / 10000: loss 2.306584\n",
      "iteration 2300 / 10000: loss 2.302796\n",
      "iteration 2400 / 10000: loss 2.303098\n",
      "iteration 2500 / 10000: loss 2.304313\n",
      "iteration 2600 / 10000: loss 2.302689\n",
      "iteration 2700 / 10000: loss 2.304979\n",
      "iteration 2800 / 10000: loss 2.304021\n",
      "iteration 2900 / 10000: loss 2.303441\n",
      "iteration 3000 / 10000: loss 2.302448\n",
      "iteration 3100 / 10000: loss 2.303408\n",
      "iteration 3200 / 10000: loss 2.302361\n",
      "iteration 3300 / 10000: loss 2.303490\n",
      "iteration 3400 / 10000: loss 2.302440\n",
      "iteration 3500 / 10000: loss 2.302223\n",
      "iteration 3600 / 10000: loss 2.303318\n",
      "iteration 3700 / 10000: loss 2.303088\n",
      "iteration 3800 / 10000: loss 2.304103\n",
      "iteration 3900 / 10000: loss 2.303124\n",
      "iteration 4000 / 10000: loss 2.302650\n",
      "iteration 4100 / 10000: loss 2.302905\n",
      "iteration 4200 / 10000: loss 2.303511\n",
      "iteration 4300 / 10000: loss 2.303100\n",
      "iteration 4400 / 10000: loss 2.303020\n",
      "iteration 4500 / 10000: loss 2.302377\n",
      "iteration 4600 / 10000: loss 2.305734\n",
      "iteration 4700 / 10000: loss 2.302764\n",
      "iteration 4800 / 10000: loss 2.303287\n",
      "iteration 4900 / 10000: loss 2.302893\n",
      "iteration 5000 / 10000: loss 2.304281\n",
      "iteration 5100 / 10000: loss 2.301440\n",
      "iteration 5200 / 10000: loss 2.302583\n",
      "iteration 5300 / 10000: loss 2.301552\n",
      "iteration 5400 / 10000: loss 2.302437\n",
      "iteration 5500 / 10000: loss 2.302951\n",
      "iteration 5600 / 10000: loss 2.302639\n",
      "iteration 5700 / 10000: loss 2.302504\n",
      "iteration 5800 / 10000: loss 2.301799\n",
      "iteration 5900 / 10000: loss 2.303528\n",
      "iteration 6000 / 10000: loss 2.302810\n",
      "iteration 6100 / 10000: loss 2.302837\n",
      "iteration 6200 / 10000: loss 2.303668\n",
      "iteration 6300 / 10000: loss 2.302532\n",
      "iteration 6400 / 10000: loss 2.303073\n",
      "iteration 6500 / 10000: loss 2.303381\n",
      "iteration 6600 / 10000: loss 2.302669\n",
      "iteration 6700 / 10000: loss 2.302883\n",
      "iteration 6800 / 10000: loss 2.303327\n",
      "iteration 6900 / 10000: loss 2.304331\n",
      "iteration 7000 / 10000: loss 2.302515\n",
      "iteration 7100 / 10000: loss 2.302572\n",
      "iteration 7200 / 10000: loss 2.302679\n",
      "iteration 7300 / 10000: loss 2.304656\n",
      "iteration 7400 / 10000: loss 2.303153\n",
      "iteration 7500 / 10000: loss 2.303208\n",
      "iteration 7600 / 10000: loss 2.302707\n",
      "iteration 7700 / 10000: loss 2.303061\n",
      "iteration 7800 / 10000: loss 2.301539\n",
      "iteration 7900 / 10000: loss 2.302320\n",
      "iteration 8000 / 10000: loss 2.302230\n",
      "iteration 8100 / 10000: loss 2.304600\n",
      "iteration 8200 / 10000: loss 2.302319\n",
      "iteration 8300 / 10000: loss 2.302849\n",
      "iteration 8400 / 10000: loss 2.303144\n",
      "iteration 8500 / 10000: loss 2.303232\n",
      "iteration 8600 / 10000: loss 2.303058\n",
      "iteration 8700 / 10000: loss 2.303388\n",
      "iteration 8800 / 10000: loss 2.302691\n",
      "iteration 8900 / 10000: loss 2.301950\n",
      "iteration 9000 / 10000: loss 2.302697\n",
      "iteration 9100 / 10000: loss 2.302086\n",
      "iteration 9200 / 10000: loss 2.303517\n",
      "iteration 9300 / 10000: loss 2.302455\n",
      "iteration 9400 / 10000: loss 2.302786\n",
      "iteration 9500 / 10000: loss 2.303137\n",
      "iteration 9600 / 10000: loss 2.303957\n",
      "iteration 9700 / 10000: loss 2.302417\n",
      "iteration 9800 / 10000: loss 2.302684\n",
      "iteration 9900 / 10000: loss 2.302906\n",
      "Validation accuracy:  0.113\n",
      "\n",
      "\n",
      "Starting iteration with rate 2.769591e-01, regstrength 3.393222e-02 and hidden layer size 43\n",
      "iteration 0 / 10000: loss 2.302588\n",
      "iteration 100 / 10000: loss 2.220365\n",
      "iteration 200 / 10000: loss 1.999626\n",
      "iteration 300 / 10000: loss 1.883767\n",
      "iteration 400 / 10000: loss 1.870978\n",
      "iteration 500 / 10000: loss 1.907843\n",
      "iteration 600 / 10000: loss 1.935702\n",
      "iteration 700 / 10000: loss 1.942615\n",
      "iteration 800 / 10000: loss 1.950742\n",
      "iteration 900 / 10000: loss 1.903590\n",
      "iteration 1000 / 10000: loss 1.931494\n",
      "iteration 1100 / 10000: loss 1.935060\n",
      "iteration 1200 / 10000: loss 1.936342\n",
      "iteration 1300 / 10000: loss 1.913020\n",
      "iteration 1400 / 10000: loss 1.885090\n",
      "iteration 1500 / 10000: loss 1.918452\n",
      "iteration 1600 / 10000: loss 1.881236\n",
      "iteration 1700 / 10000: loss 2.000221\n",
      "iteration 1800 / 10000: loss 1.882597\n",
      "iteration 1900 / 10000: loss 1.916402\n",
      "iteration 2000 / 10000: loss 1.981355\n",
      "iteration 2100 / 10000: loss 1.935848\n",
      "iteration 2200 / 10000: loss 1.923284\n",
      "iteration 2300 / 10000: loss 1.876220\n",
      "iteration 2400 / 10000: loss 1.885677\n",
      "iteration 2500 / 10000: loss 1.985533\n",
      "iteration 2600 / 10000: loss 1.925266\n",
      "iteration 2700 / 10000: loss 1.970318\n",
      "iteration 2800 / 10000: loss 1.832652\n",
      "iteration 2900 / 10000: loss 1.915003\n",
      "iteration 3000 / 10000: loss 1.913666\n",
      "iteration 3100 / 10000: loss 1.952380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3200 / 10000: loss 1.912164\n",
      "iteration 3300 / 10000: loss 1.945976\n",
      "iteration 3400 / 10000: loss 1.935706\n",
      "iteration 3500 / 10000: loss 1.960854\n",
      "iteration 3600 / 10000: loss 1.873214\n",
      "iteration 3700 / 10000: loss 1.925722\n",
      "iteration 3800 / 10000: loss 1.847467\n",
      "iteration 3900 / 10000: loss 1.948808\n",
      "iteration 4000 / 10000: loss 1.928083\n",
      "iteration 4100 / 10000: loss 1.922455\n",
      "iteration 4200 / 10000: loss 1.918507\n",
      "iteration 4300 / 10000: loss 1.901086\n",
      "iteration 4400 / 10000: loss 1.920947\n",
      "iteration 4500 / 10000: loss 1.934200\n",
      "iteration 4600 / 10000: loss 1.948593\n",
      "iteration 4700 / 10000: loss 1.870778\n",
      "iteration 4800 / 10000: loss 2.000305\n",
      "iteration 4900 / 10000: loss 1.894586\n",
      "iteration 5000 / 10000: loss 1.944341\n",
      "iteration 5100 / 10000: loss 1.955208\n",
      "iteration 5200 / 10000: loss 1.906679\n",
      "iteration 5300 / 10000: loss 1.924538\n",
      "iteration 5400 / 10000: loss 1.876699\n",
      "iteration 5500 / 10000: loss 1.993937\n",
      "iteration 5600 / 10000: loss 1.860334\n",
      "iteration 5700 / 10000: loss 1.889683\n",
      "iteration 5800 / 10000: loss 1.894035\n",
      "iteration 5900 / 10000: loss 1.932997\n",
      "iteration 6000 / 10000: loss 1.880940\n",
      "iteration 6100 / 10000: loss 1.914523\n",
      "iteration 6200 / 10000: loss 1.880286\n",
      "iteration 6300 / 10000: loss 1.873153\n",
      "iteration 6400 / 10000: loss 1.927386\n",
      "iteration 6500 / 10000: loss 1.811300\n",
      "iteration 6600 / 10000: loss 1.979121\n",
      "iteration 6700 / 10000: loss 1.881965\n",
      "iteration 6800 / 10000: loss 1.942568\n",
      "iteration 6900 / 10000: loss 1.908699\n",
      "iteration 7000 / 10000: loss 1.946429\n",
      "iteration 7100 / 10000: loss 1.899453\n",
      "iteration 7200 / 10000: loss 1.852776\n",
      "iteration 7300 / 10000: loss 1.932264\n",
      "iteration 7400 / 10000: loss 1.951213\n",
      "iteration 7500 / 10000: loss 1.921000\n",
      "iteration 7600 / 10000: loss 1.886982\n",
      "iteration 7700 / 10000: loss 1.822664\n",
      "iteration 7800 / 10000: loss 1.992680\n",
      "iteration 7900 / 10000: loss 1.812658\n",
      "iteration 8000 / 10000: loss 1.969055\n",
      "iteration 8100 / 10000: loss 1.887022\n",
      "iteration 8200 / 10000: loss 1.924497\n",
      "iteration 8300 / 10000: loss 1.898879\n",
      "iteration 8400 / 10000: loss 1.891338\n",
      "iteration 8500 / 10000: loss 1.874466\n",
      "iteration 8600 / 10000: loss 1.896819\n",
      "iteration 8700 / 10000: loss 1.913966\n",
      "iteration 8800 / 10000: loss 1.834628\n",
      "iteration 8900 / 10000: loss 1.892304\n",
      "iteration 9000 / 10000: loss 1.930080\n",
      "iteration 9100 / 10000: loss 1.873584\n",
      "iteration 9200 / 10000: loss 1.985608\n",
      "iteration 9300 / 10000: loss 1.956210\n",
      "iteration 9400 / 10000: loss 1.953633\n",
      "iteration 9500 / 10000: loss 1.915130\n",
      "iteration 9600 / 10000: loss 1.933744\n",
      "iteration 9700 / 10000: loss 1.915390\n",
      "iteration 9800 / 10000: loss 1.859197\n",
      "iteration 9900 / 10000: loss 1.915535\n",
      "Validation accuracy:  0.464\n",
      "\n",
      "\n",
      "Starting iteration with rate 7.215722e-02, regstrength 9.540955e-03 and hidden layer size 10\n",
      "iteration 0 / 10000: loss 2.302585\n",
      "iteration 100 / 10000: loss 2.302755\n",
      "iteration 200 / 10000: loss 2.301869\n",
      "iteration 300 / 10000: loss 2.293691\n",
      "iteration 400 / 10000: loss 2.115103\n",
      "iteration 500 / 10000: loss 1.964166\n",
      "iteration 600 / 10000: loss 1.778313\n",
      "iteration 700 / 10000: loss 1.808690\n",
      "iteration 800 / 10000: loss 1.832217\n",
      "iteration 900 / 10000: loss 1.821891\n",
      "iteration 1000 / 10000: loss 1.681968\n",
      "iteration 1100 / 10000: loss 1.685894\n",
      "iteration 1200 / 10000: loss 1.667722\n",
      "iteration 1300 / 10000: loss 1.540333\n",
      "iteration 1400 / 10000: loss 1.567070\n",
      "iteration 1500 / 10000: loss 1.673271\n",
      "iteration 1600 / 10000: loss 1.693526\n",
      "iteration 1700 / 10000: loss 1.617520\n",
      "iteration 1800 / 10000: loss 1.545255\n",
      "iteration 1900 / 10000: loss 1.539101\n",
      "iteration 2000 / 10000: loss 1.580186\n",
      "iteration 2100 / 10000: loss 1.578284\n",
      "iteration 2200 / 10000: loss 1.695463\n",
      "iteration 2300 / 10000: loss 1.629042\n",
      "iteration 2400 / 10000: loss 1.641097\n",
      "iteration 2500 / 10000: loss 1.609015\n",
      "iteration 2600 / 10000: loss 1.590905\n",
      "iteration 2700 / 10000: loss 1.473344\n",
      "iteration 2800 / 10000: loss 1.730627\n",
      "iteration 2900 / 10000: loss 1.584004\n",
      "iteration 3000 / 10000: loss 1.587534\n",
      "iteration 3100 / 10000: loss 1.571164\n",
      "iteration 3200 / 10000: loss 1.674642\n",
      "iteration 3300 / 10000: loss 1.673091\n",
      "iteration 3400 / 10000: loss 1.629318\n",
      "iteration 3500 / 10000: loss 1.514971\n",
      "iteration 3600 / 10000: loss 1.569272\n",
      "iteration 3700 / 10000: loss 1.606680\n",
      "iteration 3800 / 10000: loss 1.627720\n",
      "iteration 3900 / 10000: loss 1.568672\n",
      "iteration 4000 / 10000: loss 1.569905\n",
      "iteration 4100 / 10000: loss 1.623409\n",
      "iteration 4200 / 10000: loss 1.510668\n",
      "iteration 4300 / 10000: loss 1.628961\n",
      "iteration 4400 / 10000: loss 1.474645\n",
      "iteration 4500 / 10000: loss 1.610726\n",
      "iteration 4600 / 10000: loss 1.519385\n",
      "iteration 4700 / 10000: loss 1.519648\n",
      "iteration 4800 / 10000: loss 1.506257\n",
      "iteration 4900 / 10000: loss 1.596595\n",
      "iteration 5000 / 10000: loss 1.706201\n",
      "iteration 5100 / 10000: loss 1.665016\n",
      "iteration 5200 / 10000: loss 1.528363\n",
      "iteration 5300 / 10000: loss 1.635040\n",
      "iteration 5400 / 10000: loss 1.465911\n",
      "iteration 5500 / 10000: loss 1.596738\n",
      "iteration 5600 / 10000: loss 1.611161\n",
      "iteration 5700 / 10000: loss 1.573473\n",
      "iteration 5800 / 10000: loss 1.578227\n",
      "iteration 5900 / 10000: loss 1.677483\n",
      "iteration 6000 / 10000: loss 1.531170\n",
      "iteration 6100 / 10000: loss 1.577001\n",
      "iteration 6200 / 10000: loss 1.596353\n",
      "iteration 6300 / 10000: loss 1.547730\n",
      "iteration 6400 / 10000: loss 1.539149\n",
      "iteration 6500 / 10000: loss 1.581805\n",
      "iteration 6600 / 10000: loss 1.621995\n",
      "iteration 6700 / 10000: loss 1.568597\n",
      "iteration 6800 / 10000: loss 1.644238\n",
      "iteration 6900 / 10000: loss 1.657235\n",
      "iteration 7000 / 10000: loss 1.698754\n",
      "iteration 7100 / 10000: loss 1.549110\n",
      "iteration 7200 / 10000: loss 1.654510\n",
      "iteration 7300 / 10000: loss 1.427074\n",
      "iteration 7400 / 10000: loss 1.618878\n",
      "iteration 7500 / 10000: loss 1.554574\n",
      "iteration 7600 / 10000: loss 1.571027\n",
      "iteration 7700 / 10000: loss 1.421850\n",
      "iteration 7800 / 10000: loss 1.519951\n",
      "iteration 7900 / 10000: loss 1.614405\n",
      "iteration 8000 / 10000: loss 1.566183\n",
      "iteration 8100 / 10000: loss 1.556163\n",
      "iteration 8200 / 10000: loss 1.518408\n",
      "iteration 8300 / 10000: loss 1.505294\n",
      "iteration 8400 / 10000: loss 1.630039\n",
      "iteration 8500 / 10000: loss 1.563512\n",
      "iteration 8600 / 10000: loss 1.616736\n",
      "iteration 8700 / 10000: loss 1.628564\n",
      "iteration 8800 / 10000: loss 1.540941\n",
      "iteration 8900 / 10000: loss 1.530276\n",
      "iteration 9000 / 10000: loss 1.730195\n",
      "iteration 9100 / 10000: loss 1.621296\n",
      "iteration 9200 / 10000: loss 1.536655\n",
      "iteration 9300 / 10000: loss 1.537046\n",
      "iteration 9400 / 10000: loss 1.588370\n",
      "iteration 9500 / 10000: loss 1.573101\n",
      "iteration 9600 / 10000: loss 1.606364\n",
      "iteration 9700 / 10000: loss 1.475033\n",
      "iteration 9800 / 10000: loss 1.508325\n",
      "iteration 9900 / 10000: loss 1.632949\n",
      "Validation accuracy:  0.507\n",
      "\n",
      "\n",
      "Starting iteration with rate 8.849052e-01, regstrength 1.098541e-02 and hidden layer size 32\n",
      "iteration 0 / 10000: loss 2.302586\n",
      "iteration 100 / 10000: loss 1.716625\n",
      "iteration 200 / 10000: loss 1.658672\n",
      "iteration 300 / 10000: loss 1.716043\n",
      "iteration 400 / 10000: loss 1.568853\n",
      "iteration 500 / 10000: loss 1.731425\n",
      "iteration 600 / 10000: loss 1.712843\n",
      "iteration 700 / 10000: loss 1.689638\n",
      "iteration 800 / 10000: loss 1.876814\n",
      "iteration 900 / 10000: loss 1.669668\n",
      "iteration 1000 / 10000: loss 1.652674\n",
      "iteration 1100 / 10000: loss 1.792649\n",
      "iteration 1200 / 10000: loss 1.667489\n",
      "iteration 1300 / 10000: loss 1.678512\n",
      "iteration 1400 / 10000: loss 1.662378\n",
      "iteration 1500 / 10000: loss 1.711439\n",
      "iteration 1600 / 10000: loss 1.524257\n",
      "iteration 1700 / 10000: loss 1.726186\n",
      "iteration 1800 / 10000: loss 1.629370\n",
      "iteration 1900 / 10000: loss 1.716679\n",
      "iteration 2000 / 10000: loss 1.561550\n",
      "iteration 2100 / 10000: loss 1.582449\n",
      "iteration 2200 / 10000: loss 1.743808\n",
      "iteration 2300 / 10000: loss 1.653201\n",
      "iteration 2400 / 10000: loss 1.679107\n",
      "iteration 2500 / 10000: loss 1.704573\n",
      "iteration 2600 / 10000: loss 1.721860\n",
      "iteration 2700 / 10000: loss 1.716847\n",
      "iteration 2800 / 10000: loss 1.660212\n",
      "iteration 2900 / 10000: loss 1.702458\n",
      "iteration 3000 / 10000: loss 1.769336\n",
      "iteration 3100 / 10000: loss 1.696585\n",
      "iteration 3200 / 10000: loss 1.681916\n",
      "iteration 3300 / 10000: loss 1.730341\n",
      "iteration 3400 / 10000: loss 1.621875\n",
      "iteration 3500 / 10000: loss 1.586623\n",
      "iteration 3600 / 10000: loss 1.692557\n",
      "iteration 3700 / 10000: loss 1.646224\n",
      "iteration 3800 / 10000: loss 1.592714\n",
      "iteration 3900 / 10000: loss 1.607351\n",
      "iteration 4000 / 10000: loss 1.665701\n",
      "iteration 4100 / 10000: loss 1.557272\n",
      "iteration 4200 / 10000: loss 1.692541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4300 / 10000: loss 1.662875\n",
      "iteration 4400 / 10000: loss 1.554330\n",
      "iteration 4500 / 10000: loss 1.627646\n",
      "iteration 4600 / 10000: loss 1.804463\n",
      "iteration 4700 / 10000: loss 1.748987\n",
      "iteration 4800 / 10000: loss 1.832916\n",
      "iteration 4900 / 10000: loss 1.658842\n",
      "iteration 5000 / 10000: loss 1.699528\n",
      "iteration 5100 / 10000: loss 1.627606\n",
      "iteration 5200 / 10000: loss 1.670922\n",
      "iteration 5300 / 10000: loss 1.609705\n",
      "iteration 5400 / 10000: loss 1.602327\n",
      "iteration 5500 / 10000: loss 1.685319\n",
      "iteration 5600 / 10000: loss 1.705113\n",
      "iteration 5700 / 10000: loss 1.748627\n",
      "iteration 5800 / 10000: loss 1.692840\n",
      "iteration 5900 / 10000: loss 1.562604\n",
      "iteration 6000 / 10000: loss 1.499549\n",
      "iteration 6100 / 10000: loss 1.618408\n",
      "iteration 6200 / 10000: loss 1.732779\n",
      "iteration 6300 / 10000: loss 1.597167\n",
      "iteration 6400 / 10000: loss 1.576392\n",
      "iteration 6500 / 10000: loss 1.682907\n",
      "iteration 6600 / 10000: loss 1.741641\n",
      "iteration 6700 / 10000: loss 1.584417\n",
      "iteration 6800 / 10000: loss 1.697772\n",
      "iteration 6900 / 10000: loss 1.645735\n",
      "iteration 7000 / 10000: loss 1.526211\n",
      "iteration 7100 / 10000: loss 1.789817\n",
      "iteration 7200 / 10000: loss 1.686929\n",
      "iteration 7300 / 10000: loss 1.676530\n",
      "iteration 7400 / 10000: loss 1.559622\n",
      "iteration 7500 / 10000: loss 1.696208\n",
      "iteration 7600 / 10000: loss 1.511577\n",
      "iteration 7700 / 10000: loss 1.513965\n",
      "iteration 7800 / 10000: loss 1.869283\n",
      "iteration 7900 / 10000: loss 1.568256\n",
      "iteration 8000 / 10000: loss 1.503686\n",
      "iteration 8100 / 10000: loss 1.650649\n",
      "iteration 8200 / 10000: loss 1.697295\n",
      "iteration 8300 / 10000: loss 1.664023\n",
      "iteration 8400 / 10000: loss 1.701177\n",
      "iteration 8500 / 10000: loss 1.605083\n",
      "iteration 8600 / 10000: loss 1.657713\n",
      "iteration 8700 / 10000: loss 1.528646\n",
      "iteration 8800 / 10000: loss 1.720832\n",
      "iteration 8900 / 10000: loss 1.538030\n",
      "iteration 9000 / 10000: loss 1.783868\n",
      "iteration 9100 / 10000: loss 1.636199\n",
      "iteration 9200 / 10000: loss 1.616201\n",
      "iteration 9300 / 10000: loss 1.591763\n",
      "iteration 9400 / 10000: loss 1.598841\n",
      "iteration 9500 / 10000: loss 1.633116\n",
      "iteration 9600 / 10000: loss 1.575243\n",
      "iteration 9700 / 10000: loss 1.661360\n",
      "iteration 9800 / 10000: loss 1.578133\n",
      "iteration 9900 / 10000: loss 1.543558\n",
      "Validation accuracy:  0.491\n",
      "\n",
      "\n",
      "Starting iteration with rate 6.787786e-02, regstrength 3.906940e-02 and hidden layer size 11\n",
      "iteration 0 / 10000: loss 2.302586\n",
      "iteration 100 / 10000: loss 2.301980\n",
      "iteration 200 / 10000: loss 2.302634\n",
      "iteration 300 / 10000: loss 2.302005\n",
      "iteration 400 / 10000: loss 2.285690\n",
      "iteration 500 / 10000: loss 2.243896\n",
      "iteration 600 / 10000: loss 2.144612\n",
      "iteration 700 / 10000: loss 2.110307\n",
      "iteration 800 / 10000: loss 2.024849\n",
      "iteration 900 / 10000: loss 2.035184\n",
      "iteration 1000 / 10000: loss 1.959453\n",
      "iteration 1100 / 10000: loss 2.013776\n",
      "iteration 1200 / 10000: loss 1.973660\n",
      "iteration 1300 / 10000: loss 2.037673\n",
      "iteration 1400 / 10000: loss 2.050565\n",
      "iteration 1500 / 10000: loss 2.033824\n",
      "iteration 1600 / 10000: loss 1.995319\n",
      "iteration 1700 / 10000: loss 1.976726\n",
      "iteration 1800 / 10000: loss 2.000333\n",
      "iteration 1900 / 10000: loss 1.983168\n",
      "iteration 2000 / 10000: loss 1.983700\n",
      "iteration 2100 / 10000: loss 1.893142\n",
      "iteration 2200 / 10000: loss 1.978411\n",
      "iteration 2300 / 10000: loss 1.937432\n",
      "iteration 2400 / 10000: loss 1.966483\n",
      "iteration 2500 / 10000: loss 1.908462\n",
      "iteration 2600 / 10000: loss 1.904841\n",
      "iteration 2700 / 10000: loss 2.069150\n",
      "iteration 2800 / 10000: loss 1.967251\n",
      "iteration 2900 / 10000: loss 1.887937\n",
      "iteration 3000 / 10000: loss 1.974184\n",
      "iteration 3100 / 10000: loss 1.961186\n",
      "iteration 3200 / 10000: loss 2.019782\n",
      "iteration 3300 / 10000: loss 1.901858\n",
      "iteration 3400 / 10000: loss 1.948230\n",
      "iteration 3500 / 10000: loss 2.000051\n",
      "iteration 3600 / 10000: loss 1.975508\n",
      "iteration 3700 / 10000: loss 1.933201\n",
      "iteration 3800 / 10000: loss 1.996307\n",
      "iteration 3900 / 10000: loss 1.996194\n",
      "iteration 4000 / 10000: loss 1.914795\n",
      "iteration 4100 / 10000: loss 1.926649\n",
      "iteration 4200 / 10000: loss 2.041760\n",
      "iteration 4300 / 10000: loss 1.878202\n",
      "iteration 4400 / 10000: loss 1.930983\n",
      "iteration 4500 / 10000: loss 2.007878\n",
      "iteration 4600 / 10000: loss 1.904009\n",
      "iteration 4700 / 10000: loss 1.908217\n",
      "iteration 4800 / 10000: loss 1.938655\n",
      "iteration 4900 / 10000: loss 1.961218\n",
      "iteration 5000 / 10000: loss 1.935121\n",
      "iteration 5100 / 10000: loss 1.923889\n",
      "iteration 5200 / 10000: loss 2.062308\n",
      "iteration 5300 / 10000: loss 2.015301\n",
      "iteration 5400 / 10000: loss 1.902070\n",
      "iteration 5500 / 10000: loss 1.958278\n",
      "iteration 5600 / 10000: loss 1.927802\n",
      "iteration 5700 / 10000: loss 1.942480\n",
      "iteration 5800 / 10000: loss 1.953463\n",
      "iteration 5900 / 10000: loss 1.905929\n",
      "iteration 6000 / 10000: loss 1.912870\n",
      "iteration 6100 / 10000: loss 1.950384\n",
      "iteration 6200 / 10000: loss 1.906159\n",
      "iteration 6300 / 10000: loss 2.037248\n",
      "iteration 6400 / 10000: loss 2.033738\n",
      "iteration 6500 / 10000: loss 2.008495\n",
      "iteration 6600 / 10000: loss 1.949241\n",
      "iteration 6700 / 10000: loss 2.040836\n",
      "iteration 6800 / 10000: loss 1.935365\n",
      "iteration 6900 / 10000: loss 1.946394\n",
      "iteration 7000 / 10000: loss 1.948798\n",
      "iteration 7100 / 10000: loss 1.941045\n",
      "iteration 7200 / 10000: loss 1.901912\n",
      "iteration 7300 / 10000: loss 2.002883\n",
      "iteration 7400 / 10000: loss 1.977827\n",
      "iteration 7500 / 10000: loss 1.924085\n",
      "iteration 7600 / 10000: loss 1.992978\n",
      "iteration 7700 / 10000: loss 1.945652\n",
      "iteration 7800 / 10000: loss 1.919412\n",
      "iteration 7900 / 10000: loss 1.958130\n",
      "iteration 8000 / 10000: loss 1.903264\n",
      "iteration 8100 / 10000: loss 1.964960\n",
      "iteration 8200 / 10000: loss 1.935557\n",
      "iteration 8300 / 10000: loss 1.978588\n",
      "iteration 8400 / 10000: loss 1.984110\n",
      "iteration 8500 / 10000: loss 1.923779\n",
      "iteration 8600 / 10000: loss 1.915518\n",
      "iteration 8700 / 10000: loss 1.954120\n",
      "iteration 8800 / 10000: loss 2.012137\n",
      "iteration 8900 / 10000: loss 1.948763\n",
      "iteration 9000 / 10000: loss 1.982778\n",
      "iteration 9100 / 10000: loss 1.986992\n",
      "iteration 9200 / 10000: loss 1.946893\n",
      "iteration 9300 / 10000: loss 1.932189\n",
      "iteration 9400 / 10000: loss 1.942433\n",
      "iteration 9500 / 10000: loss 1.955707\n",
      "iteration 9600 / 10000: loss 1.968528\n",
      "iteration 9700 / 10000: loss 1.931913\n",
      "iteration 9800 / 10000: loss 1.878393\n",
      "iteration 9900 / 10000: loss 1.951296\n",
      "Validation accuracy:  0.453\n",
      "\n",
      "\n",
      "Starting iteration with rate 5.426049e-01, regstrength 5.179475e-02 and hidden layer size 42\n",
      "iteration 0 / 10000: loss 2.302588\n",
      "iteration 100 / 10000: loss 2.130937\n",
      "iteration 200 / 10000: loss 2.046757\n",
      "iteration 300 / 10000: loss 2.085449\n",
      "iteration 400 / 10000: loss 2.092116\n",
      "iteration 500 / 10000: loss 2.070928\n",
      "iteration 600 / 10000: loss 2.147225\n",
      "iteration 700 / 10000: loss 2.144915\n",
      "iteration 800 / 10000: loss 2.049917\n",
      "iteration 900 / 10000: loss 2.043776\n",
      "iteration 1000 / 10000: loss 2.049752\n",
      "iteration 1100 / 10000: loss 2.019657\n",
      "iteration 1200 / 10000: loss 2.082515\n",
      "iteration 1300 / 10000: loss 2.028690\n",
      "iteration 1400 / 10000: loss 2.059770\n",
      "iteration 1500 / 10000: loss 2.035598\n",
      "iteration 1600 / 10000: loss 2.038701\n",
      "iteration 1700 / 10000: loss 2.068122\n",
      "iteration 1800 / 10000: loss 2.016459\n",
      "iteration 1900 / 10000: loss 2.097003\n",
      "iteration 2000 / 10000: loss 2.049834\n",
      "iteration 2100 / 10000: loss 2.082891\n",
      "iteration 2200 / 10000: loss 2.097506\n",
      "iteration 2300 / 10000: loss 2.038519\n",
      "iteration 2400 / 10000: loss 2.118540\n",
      "iteration 2500 / 10000: loss 2.059785\n",
      "iteration 2600 / 10000: loss 1.921472\n",
      "iteration 2700 / 10000: loss 2.081854\n",
      "iteration 2800 / 10000: loss 2.019457\n",
      "iteration 2900 / 10000: loss 2.052470\n",
      "iteration 3000 / 10000: loss 2.026951\n",
      "iteration 3100 / 10000: loss 2.172800\n",
      "iteration 3200 / 10000: loss 2.009871\n",
      "iteration 3300 / 10000: loss 2.048891\n",
      "iteration 3400 / 10000: loss 2.121344\n",
      "iteration 3500 / 10000: loss 2.085061\n",
      "iteration 3600 / 10000: loss 2.045125\n",
      "iteration 3700 / 10000: loss 2.075790\n",
      "iteration 3800 / 10000: loss 2.051583\n",
      "iteration 3900 / 10000: loss 2.045287\n",
      "iteration 4000 / 10000: loss 2.066741\n",
      "iteration 4100 / 10000: loss 2.095062\n",
      "iteration 4200 / 10000: loss 2.081248\n",
      "iteration 4300 / 10000: loss 2.063385\n",
      "iteration 4400 / 10000: loss 2.072626\n",
      "iteration 4500 / 10000: loss 2.143398\n",
      "iteration 4600 / 10000: loss 2.056242\n",
      "iteration 4700 / 10000: loss 2.052340\n",
      "iteration 4800 / 10000: loss 2.020477\n",
      "iteration 4900 / 10000: loss 2.099764\n",
      "iteration 5000 / 10000: loss 2.042548\n",
      "iteration 5100 / 10000: loss 2.066631\n",
      "iteration 5200 / 10000: loss 2.095751\n",
      "iteration 5300 / 10000: loss 1.993539\n",
      "iteration 5400 / 10000: loss 2.081266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5500 / 10000: loss 1.999386\n",
      "iteration 5600 / 10000: loss 2.054691\n",
      "iteration 5700 / 10000: loss 2.056935\n",
      "iteration 5800 / 10000: loss 2.036479\n",
      "iteration 5900 / 10000: loss 2.035891\n",
      "iteration 6000 / 10000: loss 2.097838\n",
      "iteration 6100 / 10000: loss 2.069636\n",
      "iteration 6200 / 10000: loss 2.049448\n",
      "iteration 6300 / 10000: loss 2.066068\n",
      "iteration 6400 / 10000: loss 2.083193\n",
      "iteration 6500 / 10000: loss 2.065060\n",
      "iteration 6600 / 10000: loss 2.044704\n",
      "iteration 6700 / 10000: loss 2.055471\n",
      "iteration 6800 / 10000: loss 2.119990\n",
      "iteration 6900 / 10000: loss 2.062756\n",
      "iteration 7000 / 10000: loss 1.965946\n",
      "iteration 7100 / 10000: loss 2.103150\n",
      "iteration 7200 / 10000: loss 2.106364\n",
      "iteration 7300 / 10000: loss 2.076521\n",
      "iteration 7400 / 10000: loss 1.987541\n",
      "iteration 7500 / 10000: loss 2.013139\n",
      "iteration 7600 / 10000: loss 2.095809\n",
      "iteration 7700 / 10000: loss 2.075985\n",
      "iteration 7800 / 10000: loss 2.056999\n",
      "iteration 7900 / 10000: loss 1.987151\n",
      "iteration 8000 / 10000: loss 2.055094\n",
      "iteration 8100 / 10000: loss 2.039623\n",
      "iteration 8200 / 10000: loss 2.148931\n",
      "iteration 8300 / 10000: loss 2.086932\n",
      "iteration 8400 / 10000: loss 2.115068\n",
      "iteration 8500 / 10000: loss 2.083421\n",
      "iteration 8600 / 10000: loss 2.018707\n",
      "iteration 8700 / 10000: loss 2.040455\n",
      "iteration 8800 / 10000: loss 2.098516\n",
      "iteration 8900 / 10000: loss 2.063201\n",
      "iteration 9000 / 10000: loss 2.052690\n",
      "iteration 9100 / 10000: loss 2.105925\n",
      "iteration 9200 / 10000: loss 2.085802\n",
      "iteration 9300 / 10000: loss 2.065101\n",
      "iteration 9400 / 10000: loss 2.079980\n",
      "iteration 9500 / 10000: loss 2.022579\n",
      "iteration 9600 / 10000: loss 2.094505\n",
      "iteration 9700 / 10000: loss 2.034718\n",
      "iteration 9800 / 10000: loss 2.073868\n",
      "iteration 9900 / 10000: loss 2.060332\n",
      "Validation accuracy:  0.427\n",
      "\n",
      "\n",
      "Starting iteration with rate 6.518363e-01, regstrength 2.682696e-03 and hidden layer size 34\n",
      "iteration 0 / 10000: loss 2.302585\n",
      "iteration 100 / 10000: loss 1.501102\n",
      "iteration 200 / 10000: loss 1.435827\n",
      "iteration 300 / 10000: loss 1.537814\n",
      "iteration 400 / 10000: loss 1.425642\n",
      "iteration 500 / 10000: loss 1.462060\n",
      "iteration 600 / 10000: loss 1.411023\n",
      "iteration 700 / 10000: loss 1.448934\n",
      "iteration 800 / 10000: loss 1.604250\n",
      "iteration 900 / 10000: loss 1.373378\n",
      "iteration 1000 / 10000: loss 1.370791\n",
      "iteration 1100 / 10000: loss 1.698168\n",
      "iteration 1200 / 10000: loss 1.408780\n",
      "iteration 1300 / 10000: loss 1.392629\n",
      "iteration 1400 / 10000: loss 1.417809\n",
      "iteration 1500 / 10000: loss 1.599785\n",
      "iteration 1600 / 10000: loss 1.406204\n",
      "iteration 1700 / 10000: loss 1.404060\n",
      "iteration 1800 / 10000: loss 1.418168\n",
      "iteration 1900 / 10000: loss 1.400659\n",
      "iteration 2000 / 10000: loss 1.310400\n",
      "iteration 2100 / 10000: loss 1.440680\n",
      "iteration 2200 / 10000: loss 1.394965\n",
      "iteration 2300 / 10000: loss 1.344151\n",
      "iteration 2400 / 10000: loss 1.338942\n",
      "iteration 2500 / 10000: loss 1.427364\n",
      "iteration 2600 / 10000: loss 1.411034\n",
      "iteration 2700 / 10000: loss 1.380519\n",
      "iteration 2800 / 10000: loss 1.408692\n",
      "iteration 2900 / 10000: loss 1.284874\n",
      "iteration 3000 / 10000: loss 1.516389\n",
      "iteration 3100 / 10000: loss 1.364600\n",
      "iteration 3200 / 10000: loss 1.343735\n",
      "iteration 3300 / 10000: loss 1.428919\n",
      "iteration 3400 / 10000: loss 1.429332\n",
      "iteration 3500 / 10000: loss 1.253659\n",
      "iteration 3600 / 10000: loss 1.368435\n",
      "iteration 3700 / 10000: loss 1.357881\n",
      "iteration 3800 / 10000: loss 1.472322\n",
      "iteration 3900 / 10000: loss 1.439849\n",
      "iteration 4000 / 10000: loss 1.315278\n",
      "iteration 4100 / 10000: loss 1.388224\n",
      "iteration 4200 / 10000: loss 1.380489\n",
      "iteration 4300 / 10000: loss 1.330723\n",
      "iteration 4400 / 10000: loss 1.467084\n",
      "iteration 4500 / 10000: loss 1.337236\n",
      "iteration 4600 / 10000: loss 1.398528\n",
      "iteration 4700 / 10000: loss 1.452335\n",
      "iteration 4800 / 10000: loss 1.289703\n",
      "iteration 4900 / 10000: loss 1.441493\n",
      "iteration 5000 / 10000: loss 1.334209\n",
      "iteration 5100 / 10000: loss 1.327058\n",
      "iteration 5200 / 10000: loss 1.373664\n",
      "iteration 5300 / 10000: loss 1.487091\n",
      "iteration 5400 / 10000: loss 1.290938\n",
      "iteration 5500 / 10000: loss 1.282777\n",
      "iteration 5600 / 10000: loss 1.405579\n",
      "iteration 5700 / 10000: loss 1.219663\n",
      "iteration 5800 / 10000: loss 1.376835\n",
      "iteration 5900 / 10000: loss 1.206944\n",
      "iteration 6000 / 10000: loss 1.402997\n",
      "iteration 6100 / 10000: loss 1.418158\n",
      "iteration 6200 / 10000: loss 1.436645\n",
      "iteration 6300 / 10000: loss 1.409795\n",
      "iteration 6400 / 10000: loss 1.409007\n",
      "iteration 6500 / 10000: loss 1.411526\n",
      "iteration 6600 / 10000: loss 1.405798\n",
      "iteration 6700 / 10000: loss 1.395021\n",
      "iteration 6800 / 10000: loss 1.333395\n",
      "iteration 6900 / 10000: loss 1.349112\n",
      "iteration 7000 / 10000: loss 1.391924\n",
      "iteration 7100 / 10000: loss 1.369988\n",
      "iteration 7200 / 10000: loss 1.341207\n",
      "iteration 7300 / 10000: loss 1.408823\n",
      "iteration 7400 / 10000: loss 1.342228\n",
      "iteration 7500 / 10000: loss 1.382449\n",
      "iteration 7600 / 10000: loss 1.529132\n",
      "iteration 7700 / 10000: loss 1.358673\n",
      "iteration 7800 / 10000: loss 1.309076\n",
      "iteration 7900 / 10000: loss 1.379762\n",
      "iteration 8000 / 10000: loss 1.352988\n",
      "iteration 8100 / 10000: loss 1.349575\n",
      "iteration 8200 / 10000: loss 1.412317\n",
      "iteration 8300 / 10000: loss 1.333516\n",
      "iteration 8400 / 10000: loss 1.391680\n",
      "iteration 8500 / 10000: loss 1.402021\n",
      "iteration 8600 / 10000: loss 1.279823\n",
      "iteration 8700 / 10000: loss 1.438013\n",
      "iteration 8800 / 10000: loss 1.339113\n",
      "iteration 8900 / 10000: loss 1.347250\n",
      "iteration 9000 / 10000: loss 1.406062\n",
      "iteration 9100 / 10000: loss 1.293388\n",
      "iteration 9200 / 10000: loss 1.319411\n",
      "iteration 9300 / 10000: loss 1.265469\n",
      "iteration 9400 / 10000: loss 1.360514\n",
      "iteration 9500 / 10000: loss 1.242036\n",
      "iteration 9600 / 10000: loss 1.359620\n",
      "iteration 9700 / 10000: loss 1.419165\n",
      "iteration 9800 / 10000: loss 1.385653\n",
      "iteration 9900 / 10000: loss 1.311999\n",
      "Validation accuracy:  0.565\n",
      "\n",
      "\n",
      "Starting iteration with rate 9.214809e-02, regstrength 1.048113e-01 and hidden layer size 19\n",
      "iteration 0 / 10000: loss 2.302588\n",
      "iteration 100 / 10000: loss 2.300834\n",
      "iteration 200 / 10000: loss 2.302830\n",
      "iteration 300 / 10000: loss 2.301631\n",
      "iteration 400 / 10000: loss 2.299602\n",
      "iteration 500 / 10000: loss 2.278514\n",
      "iteration 600 / 10000: loss 2.259058\n",
      "iteration 700 / 10000: loss 2.279068\n",
      "iteration 800 / 10000: loss 2.267699\n",
      "iteration 900 / 10000: loss 2.201127\n",
      "iteration 1000 / 10000: loss 2.294668\n",
      "iteration 1100 / 10000: loss 2.232904\n",
      "iteration 1200 / 10000: loss 2.204111\n",
      "iteration 1300 / 10000: loss 2.249055\n",
      "iteration 1400 / 10000: loss 2.266901\n",
      "iteration 1500 / 10000: loss 2.258557\n",
      "iteration 1600 / 10000: loss 2.286700\n",
      "iteration 1700 / 10000: loss 2.222466\n",
      "iteration 1800 / 10000: loss 2.222795\n",
      "iteration 1900 / 10000: loss 2.237537\n",
      "iteration 2000 / 10000: loss 2.269419\n",
      "iteration 2100 / 10000: loss 2.246808\n",
      "iteration 2200 / 10000: loss 2.235718\n",
      "iteration 2300 / 10000: loss 2.261955\n",
      "iteration 2400 / 10000: loss 2.265787\n",
      "iteration 2500 / 10000: loss 2.291876\n",
      "iteration 2600 / 10000: loss 2.252160\n",
      "iteration 2700 / 10000: loss 2.198620\n",
      "iteration 2800 / 10000: loss 2.262595\n",
      "iteration 2900 / 10000: loss 2.225913\n",
      "iteration 3000 / 10000: loss 2.255326\n",
      "iteration 3100 / 10000: loss 2.231414\n",
      "iteration 3200 / 10000: loss 2.261831\n",
      "iteration 3300 / 10000: loss 2.278960\n",
      "iteration 3400 / 10000: loss 2.232267\n",
      "iteration 3500 / 10000: loss 2.210908\n",
      "iteration 3600 / 10000: loss 2.219821\n",
      "iteration 3700 / 10000: loss 2.242692\n",
      "iteration 3800 / 10000: loss 2.274681\n",
      "iteration 3900 / 10000: loss 2.218045\n",
      "iteration 4000 / 10000: loss 2.235063\n",
      "iteration 4100 / 10000: loss 2.259419\n",
      "iteration 4200 / 10000: loss 2.229134\n",
      "iteration 4300 / 10000: loss 2.259734\n",
      "iteration 4400 / 10000: loss 2.232076\n",
      "iteration 4500 / 10000: loss 2.259724\n",
      "iteration 4600 / 10000: loss 2.233571\n",
      "iteration 4700 / 10000: loss 2.232815\n",
      "iteration 4800 / 10000: loss 2.206464\n",
      "iteration 4900 / 10000: loss 2.254247\n",
      "iteration 5000 / 10000: loss 2.253973\n",
      "iteration 5100 / 10000: loss 2.278600\n",
      "iteration 5200 / 10000: loss 2.213209\n",
      "iteration 5300 / 10000: loss 2.226841\n",
      "iteration 5400 / 10000: loss 2.257112\n",
      "iteration 5500 / 10000: loss 2.228052\n",
      "iteration 5600 / 10000: loss 2.240500\n",
      "iteration 5700 / 10000: loss 2.261015\n",
      "iteration 5800 / 10000: loss 2.247360\n",
      "iteration 5900 / 10000: loss 2.271747\n",
      "iteration 6000 / 10000: loss 2.265216\n",
      "iteration 6100 / 10000: loss 2.247930\n",
      "iteration 6200 / 10000: loss 2.256177\n",
      "iteration 6300 / 10000: loss 2.232590\n",
      "iteration 6400 / 10000: loss 2.269001\n",
      "iteration 6500 / 10000: loss 2.272267\n",
      "iteration 6600 / 10000: loss 2.235493\n",
      "iteration 6700 / 10000: loss 2.209934\n",
      "iteration 6800 / 10000: loss 2.310356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6900 / 10000: loss 2.275642\n",
      "iteration 7000 / 10000: loss 2.262900\n",
      "iteration 7100 / 10000: loss 2.269500\n",
      "iteration 7200 / 10000: loss 2.243643\n",
      "iteration 7300 / 10000: loss 2.256243\n",
      "iteration 7400 / 10000: loss 2.254993\n",
      "iteration 7500 / 10000: loss 2.263414\n",
      "iteration 7600 / 10000: loss 2.252812\n",
      "iteration 7700 / 10000: loss 2.275145\n",
      "iteration 7800 / 10000: loss 2.263201\n",
      "iteration 7900 / 10000: loss 2.272461\n",
      "iteration 8000 / 10000: loss 2.226455\n",
      "iteration 8100 / 10000: loss 2.250130\n",
      "iteration 8200 / 10000: loss 2.271657\n",
      "iteration 8300 / 10000: loss 2.244089\n",
      "iteration 8400 / 10000: loss 2.221595\n",
      "iteration 8500 / 10000: loss 2.234816\n",
      "iteration 8600 / 10000: loss 2.253165\n",
      "iteration 8700 / 10000: loss 2.285706\n",
      "iteration 8800 / 10000: loss 2.249561\n",
      "iteration 8900 / 10000: loss 2.245254\n",
      "iteration 9000 / 10000: loss 2.282892\n",
      "iteration 9100 / 10000: loss 2.229094\n",
      "iteration 9200 / 10000: loss 2.233124\n",
      "iteration 9300 / 10000: loss 2.272438\n",
      "iteration 9400 / 10000: loss 2.283150\n",
      "iteration 9500 / 10000: loss 2.251180\n",
      "iteration 9600 / 10000: loss 2.275759\n",
      "iteration 9700 / 10000: loss 2.237731\n",
      "iteration 9800 / 10000: loss 2.239641\n",
      "iteration 9900 / 10000: loss 2.246873\n",
      "Validation accuracy:  0.245\n",
      "\n",
      "\n",
      "Starting iteration with rate 3.536896e-01, regstrength 7.196857e-03 and hidden layer size 11\n",
      "iteration 0 / 10000: loss 2.302585\n",
      "iteration 100 / 10000: loss 1.943771\n",
      "iteration 200 / 10000: loss 1.636606\n",
      "iteration 300 / 10000: loss 1.751419\n",
      "iteration 400 / 10000: loss 1.642642\n",
      "iteration 500 / 10000: loss 1.545809\n",
      "iteration 600 / 10000: loss 1.658286\n",
      "iteration 700 / 10000: loss 1.596526\n",
      "iteration 800 / 10000: loss 1.526562\n",
      "iteration 900 / 10000: loss 1.545387\n",
      "iteration 1000 / 10000: loss 1.443594\n",
      "iteration 1100 / 10000: loss 1.518741\n",
      "iteration 1200 / 10000: loss 1.577896\n",
      "iteration 1300 / 10000: loss 1.722550\n",
      "iteration 1400 / 10000: loss 1.687094\n",
      "iteration 1500 / 10000: loss 1.692894\n",
      "iteration 1600 / 10000: loss 1.560897\n",
      "iteration 1700 / 10000: loss 1.714098\n",
      "iteration 1800 / 10000: loss 1.540862\n",
      "iteration 1900 / 10000: loss 1.566246\n",
      "iteration 2000 / 10000: loss 1.526309\n",
      "iteration 2100 / 10000: loss 1.666575\n",
      "iteration 2200 / 10000: loss 1.479301\n",
      "iteration 2300 / 10000: loss 1.585551\n",
      "iteration 2400 / 10000: loss 1.550250\n",
      "iteration 2500 / 10000: loss 1.624335\n",
      "iteration 2600 / 10000: loss 1.574422\n",
      "iteration 2700 / 10000: loss 1.564488\n",
      "iteration 2800 / 10000: loss 1.514380\n",
      "iteration 2900 / 10000: loss 1.559616\n",
      "iteration 3000 / 10000: loss 1.584137\n",
      "iteration 3100 / 10000: loss 1.498290\n",
      "iteration 3200 / 10000: loss 1.503865\n",
      "iteration 3300 / 10000: loss 1.483459\n",
      "iteration 3400 / 10000: loss 1.605129\n",
      "iteration 3500 / 10000: loss 1.598832\n",
      "iteration 3600 / 10000: loss 1.545877\n",
      "iteration 3700 / 10000: loss 1.535808\n",
      "iteration 3800 / 10000: loss 1.601020\n",
      "iteration 3900 / 10000: loss 1.534023\n",
      "iteration 4000 / 10000: loss 1.473713\n",
      "iteration 4100 / 10000: loss 1.559874\n",
      "iteration 4200 / 10000: loss 1.516080\n",
      "iteration 4300 / 10000: loss 1.521157\n",
      "iteration 4400 / 10000: loss 1.600432\n",
      "iteration 4500 / 10000: loss 1.727106\n",
      "iteration 4600 / 10000: loss 1.564414\n",
      "iteration 4700 / 10000: loss 1.655009\n",
      "iteration 4800 / 10000: loss 1.695000\n",
      "iteration 4900 / 10000: loss 1.589825\n",
      "iteration 5000 / 10000: loss 1.607259\n",
      "iteration 5100 / 10000: loss 1.584779\n",
      "iteration 5200 / 10000: loss 1.503451\n",
      "iteration 5300 / 10000: loss 1.653332\n",
      "iteration 5400 / 10000: loss 1.587623\n",
      "iteration 5500 / 10000: loss 1.548210\n",
      "iteration 5600 / 10000: loss 1.478607\n",
      "iteration 5700 / 10000: loss 1.523196\n",
      "iteration 5800 / 10000: loss 1.537295\n",
      "iteration 5900 / 10000: loss 1.536595\n",
      "iteration 6000 / 10000: loss 1.506713\n",
      "iteration 6100 / 10000: loss 1.478064\n",
      "iteration 6200 / 10000: loss 1.522691\n",
      "iteration 6300 / 10000: loss 1.438984\n",
      "iteration 6400 / 10000: loss 1.647723\n",
      "iteration 6500 / 10000: loss 1.542412\n",
      "iteration 6600 / 10000: loss 1.506187\n",
      "iteration 6700 / 10000: loss 1.495653\n",
      "iteration 6800 / 10000: loss 1.589822\n",
      "iteration 6900 / 10000: loss 1.533515\n",
      "iteration 7000 / 10000: loss 1.628523\n",
      "iteration 7100 / 10000: loss 1.511660\n",
      "iteration 7200 / 10000: loss 1.545960\n",
      "iteration 7300 / 10000: loss 1.555056\n",
      "iteration 7400 / 10000: loss 1.457811\n",
      "iteration 7500 / 10000: loss 1.725836\n",
      "iteration 7600 / 10000: loss 1.614699\n",
      "iteration 7700 / 10000: loss 1.628027\n",
      "iteration 7800 / 10000: loss 1.616350\n",
      "iteration 7900 / 10000: loss 1.667737\n",
      "iteration 8000 / 10000: loss 1.470662\n",
      "iteration 8100 / 10000: loss 1.608842\n",
      "iteration 8200 / 10000: loss 1.502921\n",
      "iteration 8300 / 10000: loss 1.466322\n",
      "iteration 8400 / 10000: loss 1.447754\n",
      "iteration 8500 / 10000: loss 1.549635\n",
      "iteration 8600 / 10000: loss 1.540157\n",
      "iteration 8700 / 10000: loss 1.501220\n",
      "iteration 8800 / 10000: loss 1.452191\n",
      "iteration 8900 / 10000: loss 1.457505\n",
      "iteration 9000 / 10000: loss 1.623354\n",
      "iteration 9100 / 10000: loss 1.594815\n",
      "iteration 9200 / 10000: loss 1.533686\n",
      "iteration 9300 / 10000: loss 1.492058\n",
      "iteration 9400 / 10000: loss 1.455036\n",
      "iteration 9500 / 10000: loss 1.593522\n",
      "iteration 9600 / 10000: loss 1.592794\n",
      "iteration 9700 / 10000: loss 1.665034\n",
      "iteration 9800 / 10000: loss 1.457709\n",
      "iteration 9900 / 10000: loss 1.559597\n",
      "Validation accuracy:  0.506\n",
      "\n",
      "\n",
      "Starting iteration with rate 2.305477e-01, regstrength 8.685114e-01 and hidden layer size 15\n",
      "iteration 0 / 10000: loss 2.302606\n",
      "iteration 100 / 10000: loss 2.303048\n",
      "iteration 200 / 10000: loss 2.302543\n",
      "iteration 300 / 10000: loss 2.302531\n",
      "iteration 400 / 10000: loss 2.302921\n",
      "iteration 500 / 10000: loss 2.302811\n",
      "iteration 600 / 10000: loss 2.302932\n",
      "iteration 700 / 10000: loss 2.302608\n",
      "iteration 800 / 10000: loss 2.303153\n",
      "iteration 900 / 10000: loss 2.302926\n",
      "iteration 1000 / 10000: loss 2.302562\n",
      "iteration 1100 / 10000: loss 2.302741\n",
      "iteration 1200 / 10000: loss 2.303290\n",
      "iteration 1300 / 10000: loss 2.302671\n",
      "iteration 1400 / 10000: loss 2.302506\n",
      "iteration 1500 / 10000: loss 2.302171\n",
      "iteration 1600 / 10000: loss 2.303308\n",
      "iteration 1700 / 10000: loss 2.302990\n",
      "iteration 1800 / 10000: loss 2.303395\n",
      "iteration 1900 / 10000: loss 2.303024\n",
      "iteration 2000 / 10000: loss 2.302436\n",
      "iteration 2100 / 10000: loss 2.302958\n",
      "iteration 2200 / 10000: loss 2.302781\n",
      "iteration 2300 / 10000: loss 2.303103\n",
      "iteration 2400 / 10000: loss 2.302676\n",
      "iteration 2500 / 10000: loss 2.303008\n",
      "iteration 2600 / 10000: loss 2.302583\n",
      "iteration 2700 / 10000: loss 2.302815\n",
      "iteration 2800 / 10000: loss 2.303187\n",
      "iteration 2900 / 10000: loss 2.302373\n",
      "iteration 3000 / 10000: loss 2.302804\n",
      "iteration 3100 / 10000: loss 2.303443\n",
      "iteration 3200 / 10000: loss 2.303375\n",
      "iteration 3300 / 10000: loss 2.302679\n",
      "iteration 3400 / 10000: loss 2.302968\n",
      "iteration 3500 / 10000: loss 2.302744\n",
      "iteration 3600 / 10000: loss 2.303501\n",
      "iteration 3700 / 10000: loss 2.302668\n",
      "iteration 3800 / 10000: loss 2.302797\n",
      "iteration 3900 / 10000: loss 2.302832\n",
      "iteration 4000 / 10000: loss 2.302722\n",
      "iteration 4100 / 10000: loss 2.302255\n",
      "iteration 4200 / 10000: loss 2.302477\n",
      "iteration 4300 / 10000: loss 2.302471\n",
      "iteration 4400 / 10000: loss 2.302429\n",
      "iteration 4500 / 10000: loss 2.303109\n",
      "iteration 4600 / 10000: loss 2.302370\n",
      "iteration 4700 / 10000: loss 2.302797\n",
      "iteration 4800 / 10000: loss 2.302724\n",
      "iteration 4900 / 10000: loss 2.302711\n",
      "iteration 5000 / 10000: loss 2.302581\n",
      "iteration 5100 / 10000: loss 2.302854\n",
      "iteration 5200 / 10000: loss 2.303015\n",
      "iteration 5300 / 10000: loss 2.302437\n",
      "iteration 5400 / 10000: loss 2.302583\n",
      "iteration 5500 / 10000: loss 2.302657\n",
      "iteration 5600 / 10000: loss 2.302753\n",
      "iteration 5700 / 10000: loss 2.302607\n",
      "iteration 5800 / 10000: loss 2.302628\n",
      "iteration 5900 / 10000: loss 2.302255\n",
      "iteration 6000 / 10000: loss 2.303428\n",
      "iteration 6100 / 10000: loss 2.302353\n",
      "iteration 6200 / 10000: loss 2.302995\n",
      "iteration 6300 / 10000: loss 2.302562\n",
      "iteration 6400 / 10000: loss 2.302635\n",
      "iteration 6500 / 10000: loss 2.302456\n",
      "iteration 6600 / 10000: loss 2.303149\n",
      "iteration 6700 / 10000: loss 2.302392\n",
      "iteration 6800 / 10000: loss 2.302722\n",
      "iteration 6900 / 10000: loss 2.302168\n",
      "iteration 7000 / 10000: loss 2.302774\n",
      "iteration 7100 / 10000: loss 2.302716\n",
      "iteration 7200 / 10000: loss 2.302914\n",
      "iteration 7300 / 10000: loss 2.303023\n",
      "iteration 7400 / 10000: loss 2.302882\n",
      "iteration 7500 / 10000: loss 2.303120\n",
      "iteration 7600 / 10000: loss 2.302282\n",
      "iteration 7700 / 10000: loss 2.302950\n",
      "iteration 7800 / 10000: loss 2.302847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7900 / 10000: loss 2.302794\n",
      "iteration 8000 / 10000: loss 2.302709\n",
      "iteration 8100 / 10000: loss 2.302487\n",
      "iteration 8200 / 10000: loss 2.303142\n",
      "iteration 8300 / 10000: loss 2.302756\n",
      "iteration 8400 / 10000: loss 2.302838\n",
      "iteration 8500 / 10000: loss 2.302561\n",
      "iteration 8600 / 10000: loss 2.302619\n",
      "iteration 8700 / 10000: loss 2.302563\n",
      "iteration 8800 / 10000: loss 2.302539\n",
      "iteration 8900 / 10000: loss 2.302948\n",
      "iteration 9000 / 10000: loss 2.302574\n",
      "iteration 9100 / 10000: loss 2.302725\n",
      "iteration 9200 / 10000: loss 2.302571\n",
      "iteration 9300 / 10000: loss 2.302562\n",
      "iteration 9400 / 10000: loss 2.302702\n",
      "iteration 9500 / 10000: loss 2.302808\n",
      "iteration 9600 / 10000: loss 2.302617\n",
      "iteration 9700 / 10000: loss 2.303052\n",
      "iteration 9800 / 10000: loss 2.302509\n",
      "iteration 9900 / 10000: loss 2.302855\n",
      "Validation accuracy:  0.078\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.698254e-01, regstrength 4.291934e-01 and hidden layer size 27\n",
      "iteration 0 / 10000: loss 2.302604\n",
      "iteration 100 / 10000: loss 2.302680\n",
      "iteration 200 / 10000: loss 2.303044\n",
      "iteration 300 / 10000: loss 2.302263\n",
      "iteration 400 / 10000: loss 2.302355\n",
      "iteration 500 / 10000: loss 2.302935\n",
      "iteration 600 / 10000: loss 2.303245\n",
      "iteration 700 / 10000: loss 2.302600\n",
      "iteration 800 / 10000: loss 2.302759\n",
      "iteration 900 / 10000: loss 2.303203\n",
      "iteration 1000 / 10000: loss 2.302545\n",
      "iteration 1100 / 10000: loss 2.303234\n",
      "iteration 1200 / 10000: loss 2.303311\n",
      "iteration 1300 / 10000: loss 2.302729\n",
      "iteration 1400 / 10000: loss 2.302056\n",
      "iteration 1500 / 10000: loss 2.302356\n",
      "iteration 1600 / 10000: loss 2.302585\n",
      "iteration 1700 / 10000: loss 2.302567\n",
      "iteration 1800 / 10000: loss 2.302752\n",
      "iteration 1900 / 10000: loss 2.303371\n",
      "iteration 2000 / 10000: loss 2.302705\n",
      "iteration 2100 / 10000: loss 2.303125\n",
      "iteration 2200 / 10000: loss 2.302999\n",
      "iteration 2300 / 10000: loss 2.302208\n",
      "iteration 2400 / 10000: loss 2.302521\n",
      "iteration 2500 / 10000: loss 2.302774\n",
      "iteration 2600 / 10000: loss 2.302570\n",
      "iteration 2700 / 10000: loss 2.302713\n",
      "iteration 2800 / 10000: loss 2.302750\n",
      "iteration 2900 / 10000: loss 2.302212\n",
      "iteration 3000 / 10000: loss 2.302650\n",
      "iteration 3100 / 10000: loss 2.302388\n",
      "iteration 3200 / 10000: loss 2.302232\n",
      "iteration 3300 / 10000: loss 2.303077\n",
      "iteration 3400 / 10000: loss 2.303162\n",
      "iteration 3500 / 10000: loss 2.302354\n",
      "iteration 3600 / 10000: loss 2.302885\n",
      "iteration 3700 / 10000: loss 2.302489\n",
      "iteration 3800 / 10000: loss 2.302288\n",
      "iteration 3900 / 10000: loss 2.302734\n",
      "iteration 4000 / 10000: loss 2.302513\n",
      "iteration 4100 / 10000: loss 2.303403\n",
      "iteration 4200 / 10000: loss 2.303324\n",
      "iteration 4300 / 10000: loss 2.303918\n",
      "iteration 4400 / 10000: loss 2.302618\n",
      "iteration 4500 / 10000: loss 2.302456\n",
      "iteration 4600 / 10000: loss 2.301725\n",
      "iteration 4700 / 10000: loss 2.302348\n",
      "iteration 4800 / 10000: loss 2.303044\n",
      "iteration 4900 / 10000: loss 2.303045\n",
      "iteration 5000 / 10000: loss 2.302449\n",
      "iteration 5100 / 10000: loss 2.303130\n",
      "iteration 5200 / 10000: loss 2.302409\n",
      "iteration 5300 / 10000: loss 2.302121\n",
      "iteration 5400 / 10000: loss 2.303228\n",
      "iteration 5500 / 10000: loss 2.302393\n",
      "iteration 5600 / 10000: loss 2.302680\n",
      "iteration 5700 / 10000: loss 2.302330\n",
      "iteration 5800 / 10000: loss 2.303467\n",
      "iteration 5900 / 10000: loss 2.302387\n",
      "iteration 6000 / 10000: loss 2.302394\n",
      "iteration 6100 / 10000: loss 2.302604\n",
      "iteration 6200 / 10000: loss 2.302691\n",
      "iteration 6300 / 10000: loss 2.302308\n",
      "iteration 6400 / 10000: loss 2.302602\n",
      "iteration 6500 / 10000: loss 2.302855\n",
      "iteration 6600 / 10000: loss 2.302664\n",
      "iteration 6700 / 10000: loss 2.302449\n",
      "iteration 6800 / 10000: loss 2.302775\n",
      "iteration 6900 / 10000: loss 2.302514\n",
      "iteration 7000 / 10000: loss 2.302125\n",
      "iteration 7100 / 10000: loss 2.302606\n",
      "iteration 7200 / 10000: loss 2.303456\n",
      "iteration 7300 / 10000: loss 2.302697\n",
      "iteration 7400 / 10000: loss 2.302637\n",
      "iteration 7500 / 10000: loss 2.301997\n",
      "iteration 7600 / 10000: loss 2.302896\n",
      "iteration 7700 / 10000: loss 2.302398\n",
      "iteration 7800 / 10000: loss 2.302198\n",
      "iteration 7900 / 10000: loss 2.302526\n",
      "iteration 8000 / 10000: loss 2.302541\n",
      "iteration 8100 / 10000: loss 2.302389\n",
      "iteration 8200 / 10000: loss 2.302684\n",
      "iteration 8300 / 10000: loss 2.302515\n",
      "iteration 8400 / 10000: loss 2.302324\n",
      "iteration 8500 / 10000: loss 2.302363\n",
      "iteration 8600 / 10000: loss 2.302621\n",
      "iteration 8700 / 10000: loss 2.302794\n",
      "iteration 8800 / 10000: loss 2.302506\n",
      "iteration 8900 / 10000: loss 2.302456\n",
      "iteration 9000 / 10000: loss 2.302879\n",
      "iteration 9100 / 10000: loss 2.303347\n",
      "iteration 9200 / 10000: loss 2.302833\n",
      "iteration 9300 / 10000: loss 2.302184\n",
      "iteration 9400 / 10000: loss 2.302692\n",
      "iteration 9500 / 10000: loss 2.302493\n",
      "iteration 9600 / 10000: loss 2.302357\n",
      "iteration 9700 / 10000: loss 2.302961\n",
      "iteration 9800 / 10000: loss 2.302799\n",
      "iteration 9900 / 10000: loss 2.302593\n",
      "Validation accuracy:  0.087\n",
      "\n",
      "\n",
      "Starting iteration with rate 2.040129e-01, regstrength 1.206793e-01 and hidden layer size 30\n",
      "iteration 0 / 10000: loss 2.302591\n",
      "iteration 100 / 10000: loss 2.303126\n",
      "iteration 200 / 10000: loss 2.302748\n",
      "iteration 300 / 10000: loss 2.277556\n",
      "iteration 400 / 10000: loss 2.259616\n",
      "iteration 500 / 10000: loss 2.275673\n",
      "iteration 600 / 10000: loss 2.281118\n",
      "iteration 700 / 10000: loss 2.269337\n",
      "iteration 800 / 10000: loss 2.277464\n",
      "iteration 900 / 10000: loss 2.257115\n",
      "iteration 1000 / 10000: loss 2.316254\n",
      "iteration 1100 / 10000: loss 2.306302\n",
      "iteration 1200 / 10000: loss 2.253035\n",
      "iteration 1300 / 10000: loss 2.295647\n",
      "iteration 1400 / 10000: loss 2.296056\n",
      "iteration 1500 / 10000: loss 2.291983\n",
      "iteration 1600 / 10000: loss 2.278883\n",
      "iteration 1700 / 10000: loss 2.267408\n",
      "iteration 1800 / 10000: loss 2.268576\n",
      "iteration 1900 / 10000: loss 2.278115\n",
      "iteration 2000 / 10000: loss 2.267039\n",
      "iteration 2100 / 10000: loss 2.290214\n",
      "iteration 2200 / 10000: loss 2.275782\n",
      "iteration 2300 / 10000: loss 2.287311\n",
      "iteration 2400 / 10000: loss 2.295406\n",
      "iteration 2500 / 10000: loss 2.301837\n",
      "iteration 2600 / 10000: loss 2.253867\n",
      "iteration 2700 / 10000: loss 2.247830\n",
      "iteration 2800 / 10000: loss 2.274599\n",
      "iteration 2900 / 10000: loss 2.299641\n",
      "iteration 3000 / 10000: loss 2.308303\n",
      "iteration 3100 / 10000: loss 2.245427\n",
      "iteration 3200 / 10000: loss 2.279250\n",
      "iteration 3300 / 10000: loss 2.282569\n",
      "iteration 3400 / 10000: loss 2.266289\n",
      "iteration 3500 / 10000: loss 2.303533\n",
      "iteration 3600 / 10000: loss 2.281454\n",
      "iteration 3700 / 10000: loss 2.262798\n",
      "iteration 3800 / 10000: loss 2.254692\n",
      "iteration 3900 / 10000: loss 2.280266\n",
      "iteration 4000 / 10000: loss 2.270979\n",
      "iteration 4100 / 10000: loss 2.284682\n",
      "iteration 4200 / 10000: loss 2.271669\n",
      "iteration 4300 / 10000: loss 2.269004\n",
      "iteration 4400 / 10000: loss 2.254886\n",
      "iteration 4500 / 10000: loss 2.254027\n",
      "iteration 4600 / 10000: loss 2.254210\n",
      "iteration 4700 / 10000: loss 2.255415\n",
      "iteration 4800 / 10000: loss 2.240597\n",
      "iteration 4900 / 10000: loss 2.296562\n",
      "iteration 5000 / 10000: loss 2.301165\n",
      "iteration 5100 / 10000: loss 2.291346\n",
      "iteration 5200 / 10000: loss 2.256273\n",
      "iteration 5300 / 10000: loss 2.264133\n",
      "iteration 5400 / 10000: loss 2.241167\n",
      "iteration 5500 / 10000: loss 2.265472\n",
      "iteration 5600 / 10000: loss 2.262785\n",
      "iteration 5700 / 10000: loss 2.279675\n",
      "iteration 5800 / 10000: loss 2.260115\n",
      "iteration 5900 / 10000: loss 2.260753\n",
      "iteration 6000 / 10000: loss 2.268929\n",
      "iteration 6100 / 10000: loss 2.272960\n",
      "iteration 6200 / 10000: loss 2.275704\n",
      "iteration 6300 / 10000: loss 2.247410\n",
      "iteration 6400 / 10000: loss 2.274258\n",
      "iteration 6500 / 10000: loss 2.244057\n",
      "iteration 6600 / 10000: loss 2.277361\n",
      "iteration 6700 / 10000: loss 2.277803\n",
      "iteration 6800 / 10000: loss 2.258725\n",
      "iteration 6900 / 10000: loss 2.301574\n",
      "iteration 7000 / 10000: loss 2.270075\n",
      "iteration 7100 / 10000: loss 2.249048\n",
      "iteration 7200 / 10000: loss 2.281369\n",
      "iteration 7300 / 10000: loss 2.282248\n",
      "iteration 7400 / 10000: loss 2.250335\n",
      "iteration 7500 / 10000: loss 2.261459\n",
      "iteration 7600 / 10000: loss 2.279776\n",
      "iteration 7700 / 10000: loss 2.257952\n",
      "iteration 7800 / 10000: loss 2.305673\n",
      "iteration 7900 / 10000: loss 2.273472\n",
      "iteration 8000 / 10000: loss 2.268924\n",
      "iteration 8100 / 10000: loss 2.262976\n",
      "iteration 8200 / 10000: loss 2.250567\n",
      "iteration 8300 / 10000: loss 2.294045\n",
      "iteration 8400 / 10000: loss 2.249799\n",
      "iteration 8500 / 10000: loss 2.212923\n",
      "iteration 8600 / 10000: loss 2.236095\n",
      "iteration 8700 / 10000: loss 2.267871\n",
      "iteration 8800 / 10000: loss 2.304391\n",
      "iteration 8900 / 10000: loss 2.263122\n",
      "iteration 9000 / 10000: loss 2.269472\n",
      "iteration 9100 / 10000: loss 2.245723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9200 / 10000: loss 2.234723\n",
      "iteration 9300 / 10000: loss 2.266675\n",
      "iteration 9400 / 10000: loss 2.287501\n",
      "iteration 9500 / 10000: loss 2.279804\n",
      "iteration 9600 / 10000: loss 2.277702\n",
      "iteration 9700 / 10000: loss 2.265068\n",
      "iteration 9800 / 10000: loss 2.287528\n",
      "iteration 9900 / 10000: loss 2.260986\n",
      "Validation accuracy:  0.246\n",
      "\n",
      "\n",
      "Starting iteration with rate 4.248906e-01, regstrength 7.543120e-01 and hidden layer size 14\n",
      "iteration 0 / 10000: loss 2.302602\n",
      "iteration 100 / 10000: loss 2.303464\n",
      "iteration 200 / 10000: loss 2.302951\n",
      "iteration 300 / 10000: loss 2.301923\n",
      "iteration 400 / 10000: loss 2.303877\n",
      "iteration 500 / 10000: loss 2.303219\n",
      "iteration 600 / 10000: loss 2.302953\n",
      "iteration 700 / 10000: loss 2.303298\n",
      "iteration 800 / 10000: loss 2.302540\n",
      "iteration 900 / 10000: loss 2.303283\n",
      "iteration 1000 / 10000: loss 2.303141\n",
      "iteration 1100 / 10000: loss 2.301819\n",
      "iteration 1200 / 10000: loss 2.303881\n",
      "iteration 1300 / 10000: loss 2.303780\n",
      "iteration 1400 / 10000: loss 2.302748\n",
      "iteration 1500 / 10000: loss 2.303655\n",
      "iteration 1600 / 10000: loss 2.302715\n",
      "iteration 1700 / 10000: loss 2.302666\n",
      "iteration 1800 / 10000: loss 2.302415\n",
      "iteration 1900 / 10000: loss 2.303507\n",
      "iteration 2000 / 10000: loss 2.303233\n",
      "iteration 2100 / 10000: loss 2.303165\n",
      "iteration 2200 / 10000: loss 2.304220\n",
      "iteration 2300 / 10000: loss 2.303118\n",
      "iteration 2400 / 10000: loss 2.304032\n",
      "iteration 2500 / 10000: loss 2.303250\n",
      "iteration 2600 / 10000: loss 2.302404\n",
      "iteration 2700 / 10000: loss 2.303058\n",
      "iteration 2800 / 10000: loss 2.303477\n",
      "iteration 2900 / 10000: loss 2.303147\n",
      "iteration 3000 / 10000: loss 2.304104\n",
      "iteration 3100 / 10000: loss 2.301945\n",
      "iteration 3200 / 10000: loss 2.302467\n",
      "iteration 3300 / 10000: loss 2.303337\n",
      "iteration 3400 / 10000: loss 2.301695\n",
      "iteration 3500 / 10000: loss 2.303140\n",
      "iteration 3600 / 10000: loss 2.302844\n",
      "iteration 3700 / 10000: loss 2.302402\n",
      "iteration 3800 / 10000: loss 2.304452\n",
      "iteration 3900 / 10000: loss 2.302749\n",
      "iteration 4000 / 10000: loss 2.303187\n",
      "iteration 4100 / 10000: loss 2.303712\n",
      "iteration 4200 / 10000: loss 2.301943\n",
      "iteration 4300 / 10000: loss 2.302673\n",
      "iteration 4400 / 10000: loss 2.302547\n",
      "iteration 4500 / 10000: loss 2.302688\n",
      "iteration 4600 / 10000: loss 2.302859\n",
      "iteration 4700 / 10000: loss 2.302359\n",
      "iteration 4800 / 10000: loss 2.302569\n",
      "iteration 4900 / 10000: loss 2.303159\n",
      "iteration 5000 / 10000: loss 2.303264\n",
      "iteration 5100 / 10000: loss 2.302515\n",
      "iteration 5200 / 10000: loss 2.301570\n",
      "iteration 5300 / 10000: loss 2.303045\n",
      "iteration 5400 / 10000: loss 2.303081\n",
      "iteration 5500 / 10000: loss 2.302717\n",
      "iteration 5600 / 10000: loss 2.302609\n",
      "iteration 5700 / 10000: loss 2.302559\n",
      "iteration 5800 / 10000: loss 2.303268\n",
      "iteration 5900 / 10000: loss 2.301644\n",
      "iteration 6000 / 10000: loss 2.303590\n",
      "iteration 6100 / 10000: loss 2.302163\n",
      "iteration 6200 / 10000: loss 2.303049\n",
      "iteration 6300 / 10000: loss 2.303722\n",
      "iteration 6400 / 10000: loss 2.303169\n",
      "iteration 6500 / 10000: loss 2.302714\n",
      "iteration 6600 / 10000: loss 2.303901\n",
      "iteration 6700 / 10000: loss 2.303045\n",
      "iteration 6800 / 10000: loss 2.302770\n",
      "iteration 6900 / 10000: loss 2.302724\n",
      "iteration 7000 / 10000: loss 2.302472\n",
      "iteration 7100 / 10000: loss 2.303324\n",
      "iteration 7200 / 10000: loss 2.302967\n",
      "iteration 7300 / 10000: loss 2.303419\n",
      "iteration 7400 / 10000: loss 2.302732\n",
      "iteration 7500 / 10000: loss 2.302098\n",
      "iteration 7600 / 10000: loss 2.303301\n",
      "iteration 7700 / 10000: loss 2.302299\n",
      "iteration 7800 / 10000: loss 2.302877\n",
      "iteration 7900 / 10000: loss 2.303106\n",
      "iteration 8000 / 10000: loss 2.302328\n",
      "iteration 8100 / 10000: loss 2.302968\n",
      "iteration 8200 / 10000: loss 2.302679\n",
      "iteration 8300 / 10000: loss 2.303060\n",
      "iteration 8400 / 10000: loss 2.303397\n",
      "iteration 8500 / 10000: loss 2.302403\n",
      "iteration 8600 / 10000: loss 2.302993\n",
      "iteration 8700 / 10000: loss 2.302791\n",
      "iteration 8800 / 10000: loss 2.302470\n",
      "iteration 8900 / 10000: loss 2.302856\n",
      "iteration 9000 / 10000: loss 2.302897\n",
      "iteration 9100 / 10000: loss 2.302969\n",
      "iteration 9200 / 10000: loss 2.302348\n",
      "iteration 9300 / 10000: loss 2.303039\n",
      "iteration 9400 / 10000: loss 2.302639\n",
      "iteration 9500 / 10000: loss 2.302298\n",
      "iteration 9600 / 10000: loss 2.303256\n",
      "iteration 9700 / 10000: loss 2.302639\n",
      "iteration 9800 / 10000: loss 2.302357\n",
      "iteration 9900 / 10000: loss 2.302498\n",
      "Validation accuracy:  0.087\n",
      "\n",
      "\n",
      "Starting iteration with rate 7.670637e-02, regstrength 1.264855e-02 and hidden layer size 45\n",
      "iteration 0 / 10000: loss 2.302586\n",
      "iteration 100 / 10000: loss 2.302313\n",
      "iteration 200 / 10000: loss 2.302491\n",
      "iteration 300 / 10000: loss 2.181841\n",
      "iteration 400 / 10000: loss 2.036595\n",
      "iteration 500 / 10000: loss 1.939088\n",
      "iteration 600 / 10000: loss 1.812578\n",
      "iteration 700 / 10000: loss 1.735953\n",
      "iteration 800 / 10000: loss 1.656778\n",
      "iteration 900 / 10000: loss 1.781786\n",
      "iteration 1000 / 10000: loss 1.656118\n",
      "iteration 1100 / 10000: loss 1.601806\n",
      "iteration 1200 / 10000: loss 1.678027\n",
      "iteration 1300 / 10000: loss 1.692427\n",
      "iteration 1400 / 10000: loss 1.721443\n",
      "iteration 1500 / 10000: loss 1.686292\n",
      "iteration 1600 / 10000: loss 1.751160\n",
      "iteration 1700 / 10000: loss 1.651992\n",
      "iteration 1800 / 10000: loss 1.704231\n",
      "iteration 1900 / 10000: loss 1.681299\n",
      "iteration 2000 / 10000: loss 1.586440\n",
      "iteration 2100 / 10000: loss 1.724417\n",
      "iteration 2200 / 10000: loss 1.659894\n",
      "iteration 2300 / 10000: loss 1.601161\n",
      "iteration 2400 / 10000: loss 1.518224\n",
      "iteration 2500 / 10000: loss 1.620612\n",
      "iteration 2600 / 10000: loss 1.650068\n",
      "iteration 2700 / 10000: loss 1.616100\n",
      "iteration 2800 / 10000: loss 1.670009\n",
      "iteration 2900 / 10000: loss 1.499632\n",
      "iteration 3000 / 10000: loss 1.772136\n",
      "iteration 3100 / 10000: loss 1.670696\n",
      "iteration 3200 / 10000: loss 1.655266\n",
      "iteration 3300 / 10000: loss 1.584823\n",
      "iteration 3400 / 10000: loss 1.563807\n",
      "iteration 3500 / 10000: loss 1.663260\n",
      "iteration 3600 / 10000: loss 1.641002\n",
      "iteration 3700 / 10000: loss 1.618514\n",
      "iteration 3800 / 10000: loss 1.703764\n",
      "iteration 3900 / 10000: loss 1.802410\n",
      "iteration 4000 / 10000: loss 1.664033\n",
      "iteration 4100 / 10000: loss 1.633728\n",
      "iteration 4200 / 10000: loss 1.528423\n",
      "iteration 4300 / 10000: loss 1.626110\n",
      "iteration 4400 / 10000: loss 1.617607\n",
      "iteration 4500 / 10000: loss 1.574944\n",
      "iteration 4600 / 10000: loss 1.546024\n",
      "iteration 4700 / 10000: loss 1.561021\n",
      "iteration 4800 / 10000: loss 1.630526\n",
      "iteration 4900 / 10000: loss 1.656125\n",
      "iteration 5000 / 10000: loss 1.624591\n",
      "iteration 5100 / 10000: loss 1.674110\n",
      "iteration 5200 / 10000: loss 1.511140\n",
      "iteration 5300 / 10000: loss 1.608779\n",
      "iteration 5400 / 10000: loss 1.554639\n",
      "iteration 5500 / 10000: loss 1.528176\n",
      "iteration 5600 / 10000: loss 1.565767\n",
      "iteration 5700 / 10000: loss 1.600026\n",
      "iteration 5800 / 10000: loss 1.644349\n",
      "iteration 5900 / 10000: loss 1.623643\n",
      "iteration 6000 / 10000: loss 1.667194\n",
      "iteration 6100 / 10000: loss 1.646007\n",
      "iteration 6200 / 10000: loss 1.589127\n",
      "iteration 6300 / 10000: loss 1.603462\n",
      "iteration 6400 / 10000: loss 1.701679\n",
      "iteration 6500 / 10000: loss 1.645332\n",
      "iteration 6600 / 10000: loss 1.673945\n",
      "iteration 6700 / 10000: loss 1.613285\n",
      "iteration 6800 / 10000: loss 1.503395\n",
      "iteration 6900 / 10000: loss 1.561138\n",
      "iteration 7000 / 10000: loss 1.620353\n",
      "iteration 7100 / 10000: loss 1.828485\n",
      "iteration 7200 / 10000: loss 1.544271\n",
      "iteration 7300 / 10000: loss 1.593575\n",
      "iteration 7400 / 10000: loss 1.641485\n",
      "iteration 7500 / 10000: loss 1.729536\n",
      "iteration 7600 / 10000: loss 1.720783\n",
      "iteration 7700 / 10000: loss 1.566925\n",
      "iteration 7800 / 10000: loss 1.693870\n",
      "iteration 7900 / 10000: loss 1.636518\n",
      "iteration 8000 / 10000: loss 1.618994\n",
      "iteration 8100 / 10000: loss 1.481129\n",
      "iteration 8200 / 10000: loss 1.662137\n",
      "iteration 8300 / 10000: loss 1.556481\n",
      "iteration 8400 / 10000: loss 1.616734\n",
      "iteration 8500 / 10000: loss 1.515911\n",
      "iteration 8600 / 10000: loss 1.595706\n",
      "iteration 8700 / 10000: loss 1.721758\n",
      "iteration 8800 / 10000: loss 1.645636\n",
      "iteration 8900 / 10000: loss 1.661568\n",
      "iteration 9000 / 10000: loss 1.646572\n",
      "iteration 9100 / 10000: loss 1.533333\n",
      "iteration 9200 / 10000: loss 1.681282\n",
      "iteration 9300 / 10000: loss 1.589635\n",
      "iteration 9400 / 10000: loss 1.692676\n",
      "iteration 9500 / 10000: loss 1.477052\n",
      "iteration 9600 / 10000: loss 1.610286\n",
      "iteration 9700 / 10000: loss 1.470979\n",
      "iteration 9800 / 10000: loss 1.706396\n",
      "iteration 9900 / 10000: loss 1.692903\n",
      "Validation accuracy:  0.497\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.329830e-01, regstrength 2.442053e-01 and hidden layer size 37\n",
      "iteration 0 / 10000: loss 2.302600\n",
      "iteration 100 / 10000: loss 2.302849\n",
      "iteration 200 / 10000: loss 2.302484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 300 / 10000: loss 2.302755\n",
      "iteration 400 / 10000: loss 2.303489\n",
      "iteration 500 / 10000: loss 2.303445\n",
      "iteration 600 / 10000: loss 2.302902\n",
      "iteration 700 / 10000: loss 2.301902\n",
      "iteration 800 / 10000: loss 2.303059\n",
      "iteration 900 / 10000: loss 2.302652\n",
      "iteration 1000 / 10000: loss 2.302576\n",
      "iteration 1100 / 10000: loss 2.302276\n",
      "iteration 1200 / 10000: loss 2.303108\n",
      "iteration 1300 / 10000: loss 2.302664\n",
      "iteration 1400 / 10000: loss 2.304152\n",
      "iteration 1500 / 10000: loss 2.303301\n",
      "iteration 1600 / 10000: loss 2.301904\n",
      "iteration 1700 / 10000: loss 2.302692\n",
      "iteration 1800 / 10000: loss 2.303559\n",
      "iteration 1900 / 10000: loss 2.302628\n",
      "iteration 2000 / 10000: loss 2.302154\n",
      "iteration 2100 / 10000: loss 2.303096\n",
      "iteration 2200 / 10000: loss 2.303375\n",
      "iteration 2300 / 10000: loss 2.303048\n",
      "iteration 2400 / 10000: loss 2.302473\n",
      "iteration 2500 / 10000: loss 2.301267\n",
      "iteration 2600 / 10000: loss 2.302836\n",
      "iteration 2700 / 10000: loss 2.303354\n",
      "iteration 2800 / 10000: loss 2.303053\n",
      "iteration 2900 / 10000: loss 2.302553\n",
      "iteration 3000 / 10000: loss 2.302179\n",
      "iteration 3100 / 10000: loss 2.303115\n",
      "iteration 3200 / 10000: loss 2.303039\n",
      "iteration 3300 / 10000: loss 2.302261\n",
      "iteration 3400 / 10000: loss 2.302633\n",
      "iteration 3500 / 10000: loss 2.302914\n",
      "iteration 3600 / 10000: loss 2.302682\n",
      "iteration 3700 / 10000: loss 2.303109\n",
      "iteration 3800 / 10000: loss 2.302272\n",
      "iteration 3900 / 10000: loss 2.302326\n",
      "iteration 4000 / 10000: loss 2.303330\n",
      "iteration 4100 / 10000: loss 2.302638\n",
      "iteration 4200 / 10000: loss 2.302776\n",
      "iteration 4300 / 10000: loss 2.302973\n",
      "iteration 4400 / 10000: loss 2.302286\n",
      "iteration 4500 / 10000: loss 2.303378\n",
      "iteration 4600 / 10000: loss 2.302848\n",
      "iteration 4700 / 10000: loss 2.303042\n",
      "iteration 4800 / 10000: loss 2.302074\n",
      "iteration 4900 / 10000: loss 2.303334\n",
      "iteration 5000 / 10000: loss 2.302877\n",
      "iteration 5100 / 10000: loss 2.302777\n",
      "iteration 5200 / 10000: loss 2.302889\n",
      "iteration 5300 / 10000: loss 2.302724\n",
      "iteration 5400 / 10000: loss 2.302593\n",
      "iteration 5500 / 10000: loss 2.302928\n",
      "iteration 5600 / 10000: loss 2.303286\n",
      "iteration 5700 / 10000: loss 2.302770\n",
      "iteration 5800 / 10000: loss 2.302419\n",
      "iteration 5900 / 10000: loss 2.302651\n",
      "iteration 6000 / 10000: loss 2.302918\n",
      "iteration 6100 / 10000: loss 2.302299\n",
      "iteration 6200 / 10000: loss 2.302162\n",
      "iteration 6300 / 10000: loss 2.302670\n",
      "iteration 6400 / 10000: loss 2.302566\n",
      "iteration 6500 / 10000: loss 2.302231\n",
      "iteration 6600 / 10000: loss 2.303110\n",
      "iteration 6700 / 10000: loss 2.303372\n",
      "iteration 6800 / 10000: loss 2.302140\n",
      "iteration 6900 / 10000: loss 2.302985\n",
      "iteration 7000 / 10000: loss 2.303171\n",
      "iteration 7100 / 10000: loss 2.303078\n",
      "iteration 7200 / 10000: loss 2.302588\n",
      "iteration 7300 / 10000: loss 2.302504\n",
      "iteration 7400 / 10000: loss 2.302541\n",
      "iteration 7500 / 10000: loss 2.302533\n",
      "iteration 7600 / 10000: loss 2.302525\n",
      "iteration 7700 / 10000: loss 2.302252\n",
      "iteration 7800 / 10000: loss 2.302760\n",
      "iteration 7900 / 10000: loss 2.302753\n",
      "iteration 8000 / 10000: loss 2.302947\n",
      "iteration 8100 / 10000: loss 2.302268\n",
      "iteration 8200 / 10000: loss 2.303101\n",
      "iteration 8300 / 10000: loss 2.302689\n",
      "iteration 8400 / 10000: loss 2.302346\n",
      "iteration 8500 / 10000: loss 2.302576\n",
      "iteration 8600 / 10000: loss 2.302611\n",
      "iteration 8700 / 10000: loss 2.302439\n",
      "iteration 8800 / 10000: loss 2.303551\n",
      "iteration 8900 / 10000: loss 2.302336\n",
      "iteration 9000 / 10000: loss 2.302880\n",
      "iteration 9100 / 10000: loss 2.302863\n",
      "iteration 9200 / 10000: loss 2.302759\n",
      "iteration 9300 / 10000: loss 2.302738\n",
      "iteration 9400 / 10000: loss 2.302487\n",
      "iteration 9500 / 10000: loss 2.302484\n",
      "iteration 9600 / 10000: loss 2.302753\n",
      "iteration 9700 / 10000: loss 2.302500\n",
      "iteration 9800 / 10000: loss 2.302592\n",
      "iteration 9900 / 10000: loss 2.302407\n",
      "Validation accuracy:  0.098\n",
      "\n",
      "\n",
      "Starting iteration with rate 3.327136e-01, regstrength 5.689866e-01 and hidden layer size 20\n",
      "iteration 0 / 10000: loss 2.302604\n",
      "iteration 100 / 10000: loss 2.302502\n",
      "iteration 200 / 10000: loss 2.302634\n",
      "iteration 300 / 10000: loss 2.303197\n",
      "iteration 400 / 10000: loss 2.302806\n",
      "iteration 500 / 10000: loss 2.302842\n",
      "iteration 600 / 10000: loss 2.302973\n",
      "iteration 700 / 10000: loss 2.303035\n",
      "iteration 800 / 10000: loss 2.303306\n",
      "iteration 900 / 10000: loss 2.302336\n",
      "iteration 1000 / 10000: loss 2.302187\n",
      "iteration 1100 / 10000: loss 2.302706\n",
      "iteration 1200 / 10000: loss 2.302390\n",
      "iteration 1300 / 10000: loss 2.302665\n",
      "iteration 1400 / 10000: loss 2.302489\n",
      "iteration 1500 / 10000: loss 2.302671\n",
      "iteration 1600 / 10000: loss 2.302103\n",
      "iteration 1700 / 10000: loss 2.303099\n",
      "iteration 1800 / 10000: loss 2.302711\n",
      "iteration 1900 / 10000: loss 2.302500\n",
      "iteration 2000 / 10000: loss 2.303004\n",
      "iteration 2100 / 10000: loss 2.302479\n",
      "iteration 2200 / 10000: loss 2.303961\n",
      "iteration 2300 / 10000: loss 2.302821\n",
      "iteration 2400 / 10000: loss 2.303487\n",
      "iteration 2500 / 10000: loss 2.302531\n",
      "iteration 2600 / 10000: loss 2.303129\n",
      "iteration 2700 / 10000: loss 2.304478\n",
      "iteration 2800 / 10000: loss 2.302360\n",
      "iteration 2900 / 10000: loss 2.303485\n",
      "iteration 3000 / 10000: loss 2.302470\n",
      "iteration 3100 / 10000: loss 2.303580\n",
      "iteration 3200 / 10000: loss 2.302611\n",
      "iteration 3300 / 10000: loss 2.302960\n",
      "iteration 3400 / 10000: loss 2.303357\n",
      "iteration 3500 / 10000: loss 2.303060\n",
      "iteration 3600 / 10000: loss 2.302862\n",
      "iteration 3700 / 10000: loss 2.302536\n",
      "iteration 3800 / 10000: loss 2.302682\n",
      "iteration 3900 / 10000: loss 2.303191\n",
      "iteration 4000 / 10000: loss 2.302872\n",
      "iteration 4100 / 10000: loss 2.303298\n",
      "iteration 4200 / 10000: loss 2.303211\n",
      "iteration 4300 / 10000: loss 2.301994\n",
      "iteration 4400 / 10000: loss 2.303670\n",
      "iteration 4500 / 10000: loss 2.302940\n",
      "iteration 4600 / 10000: loss 2.303241\n",
      "iteration 4700 / 10000: loss 2.302927\n",
      "iteration 4800 / 10000: loss 2.303224\n",
      "iteration 4900 / 10000: loss 2.302992\n",
      "iteration 5000 / 10000: loss 2.302812\n",
      "iteration 5100 / 10000: loss 2.302604\n",
      "iteration 5200 / 10000: loss 2.303001\n",
      "iteration 5300 / 10000: loss 2.303434\n",
      "iteration 5400 / 10000: loss 2.303042\n",
      "iteration 5500 / 10000: loss 2.302948\n",
      "iteration 5600 / 10000: loss 2.303237\n",
      "iteration 5700 / 10000: loss 2.302397\n",
      "iteration 5800 / 10000: loss 2.303007\n",
      "iteration 5900 / 10000: loss 2.302913\n",
      "iteration 6000 / 10000: loss 2.303182\n",
      "iteration 6100 / 10000: loss 2.302342\n",
      "iteration 6200 / 10000: loss 2.302958\n",
      "iteration 6300 / 10000: loss 2.303302\n",
      "iteration 6400 / 10000: loss 2.302563\n",
      "iteration 6500 / 10000: loss 2.302390\n",
      "iteration 6600 / 10000: loss 2.302991\n",
      "iteration 6700 / 10000: loss 2.302921\n",
      "iteration 6800 / 10000: loss 2.302660\n",
      "iteration 6900 / 10000: loss 2.303327\n",
      "iteration 7000 / 10000: loss 2.302906\n",
      "iteration 7100 / 10000: loss 2.302718\n",
      "iteration 7200 / 10000: loss 2.302602\n",
      "iteration 7300 / 10000: loss 2.302225\n",
      "iteration 7400 / 10000: loss 2.301998\n",
      "iteration 7500 / 10000: loss 2.302603\n",
      "iteration 7600 / 10000: loss 2.303030\n",
      "iteration 7700 / 10000: loss 2.302978\n",
      "iteration 7800 / 10000: loss 2.302507\n",
      "iteration 7900 / 10000: loss 2.302031\n",
      "iteration 8000 / 10000: loss 2.302766\n",
      "iteration 8100 / 10000: loss 2.303673\n",
      "iteration 8200 / 10000: loss 2.302985\n",
      "iteration 8300 / 10000: loss 2.302927\n",
      "iteration 8400 / 10000: loss 2.302696\n",
      "iteration 8500 / 10000: loss 2.303961\n",
      "iteration 8600 / 10000: loss 2.302079\n",
      "iteration 8700 / 10000: loss 2.302414\n",
      "iteration 8800 / 10000: loss 2.302277\n",
      "iteration 8900 / 10000: loss 2.302823\n",
      "iteration 9000 / 10000: loss 2.302965\n",
      "iteration 9100 / 10000: loss 2.302787\n",
      "iteration 9200 / 10000: loss 2.302547\n",
      "iteration 9300 / 10000: loss 2.302367\n",
      "iteration 9400 / 10000: loss 2.303051\n",
      "iteration 9500 / 10000: loss 2.303215\n",
      "iteration 9600 / 10000: loss 2.302359\n",
      "iteration 9700 / 10000: loss 2.302401\n",
      "iteration 9800 / 10000: loss 2.302390\n",
      "iteration 9900 / 10000: loss 2.302772\n",
      "Validation accuracy:  0.087\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.000000e+00, regstrength 8.286428e-03 and hidden layer size 19\n",
      "iteration 0 / 10000: loss 2.302585\n",
      "iteration 100 / 10000: loss 1.660913\n",
      "iteration 200 / 10000: loss 1.684894\n",
      "iteration 300 / 10000: loss 1.849240\n",
      "iteration 400 / 10000: loss 1.768728\n",
      "iteration 500 / 10000: loss 1.680227\n",
      "iteration 600 / 10000: loss 1.753053\n",
      "iteration 700 / 10000: loss 1.697105\n",
      "iteration 800 / 10000: loss 1.724485\n",
      "iteration 900 / 10000: loss 1.509143\n",
      "iteration 1000 / 10000: loss 1.702224\n",
      "iteration 1100 / 10000: loss 1.787174\n",
      "iteration 1200 / 10000: loss 1.451526\n",
      "iteration 1300 / 10000: loss 1.801075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1400 / 10000: loss 1.667297\n",
      "iteration 1500 / 10000: loss 1.709878\n",
      "iteration 1600 / 10000: loss 1.559384\n",
      "iteration 1700 / 10000: loss 1.635898\n",
      "iteration 1800 / 10000: loss 1.754268\n",
      "iteration 1900 / 10000: loss 1.486444\n",
      "iteration 2000 / 10000: loss 1.513016\n",
      "iteration 2100 / 10000: loss 1.628677\n",
      "iteration 2200 / 10000: loss 1.720247\n",
      "iteration 2300 / 10000: loss 1.687594\n",
      "iteration 2400 / 10000: loss 1.667012\n",
      "iteration 2500 / 10000: loss 1.591356\n",
      "iteration 2600 / 10000: loss 1.538070\n",
      "iteration 2700 / 10000: loss 1.601928\n",
      "iteration 2800 / 10000: loss 1.658194\n",
      "iteration 2900 / 10000: loss 1.475274\n",
      "iteration 3000 / 10000: loss 1.642611\n",
      "iteration 3100 / 10000: loss 1.589763\n",
      "iteration 3200 / 10000: loss 1.691730\n",
      "iteration 3300 / 10000: loss 1.626959\n",
      "iteration 3400 / 10000: loss 1.518866\n",
      "iteration 3500 / 10000: loss 1.585676\n",
      "iteration 3600 / 10000: loss 1.587334\n",
      "iteration 3700 / 10000: loss 1.656732\n",
      "iteration 3800 / 10000: loss 1.693548\n",
      "iteration 3900 / 10000: loss 1.728381\n",
      "iteration 4000 / 10000: loss 1.590781\n",
      "iteration 4100 / 10000: loss 1.641880\n",
      "iteration 4200 / 10000: loss 1.659463\n",
      "iteration 4300 / 10000: loss 1.652124\n",
      "iteration 4400 / 10000: loss 1.613169\n",
      "iteration 4500 / 10000: loss 1.520231\n",
      "iteration 4600 / 10000: loss 1.705337\n",
      "iteration 4700 / 10000: loss 1.720297\n",
      "iteration 4800 / 10000: loss 1.447677\n",
      "iteration 4900 / 10000: loss 1.453705\n",
      "iteration 5000 / 10000: loss 1.492314\n",
      "iteration 5100 / 10000: loss 1.585771\n",
      "iteration 5200 / 10000: loss 1.502990\n",
      "iteration 5300 / 10000: loss 1.617943\n",
      "iteration 5400 / 10000: loss 1.521513\n",
      "iteration 5500 / 10000: loss 1.629013\n",
      "iteration 5600 / 10000: loss 1.651408\n",
      "iteration 5700 / 10000: loss 1.588866\n",
      "iteration 5800 / 10000: loss 1.641586\n",
      "iteration 5900 / 10000: loss 1.720977\n",
      "iteration 6000 / 10000: loss 1.752540\n",
      "iteration 6100 / 10000: loss 1.506074\n",
      "iteration 6200 / 10000: loss 1.638828\n",
      "iteration 6300 / 10000: loss 1.634670\n",
      "iteration 6400 / 10000: loss 1.516972\n",
      "iteration 6500 / 10000: loss 1.518307\n",
      "iteration 6600 / 10000: loss 1.574847\n",
      "iteration 6700 / 10000: loss 1.546885\n",
      "iteration 6800 / 10000: loss 1.561140\n",
      "iteration 6900 / 10000: loss 1.594168\n",
      "iteration 7000 / 10000: loss 1.559574\n",
      "iteration 7100 / 10000: loss 1.620313\n",
      "iteration 7200 / 10000: loss 1.550590\n",
      "iteration 7300 / 10000: loss 1.557091\n",
      "iteration 7400 / 10000: loss 1.677164\n",
      "iteration 7500 / 10000: loss 1.496875\n",
      "iteration 7600 / 10000: loss 1.732312\n",
      "iteration 7700 / 10000: loss 1.560638\n",
      "iteration 7800 / 10000: loss 1.547090\n",
      "iteration 7900 / 10000: loss 1.538701\n",
      "iteration 8000 / 10000: loss 1.517905\n",
      "iteration 8100 / 10000: loss 1.576749\n",
      "iteration 8200 / 10000: loss 1.498028\n",
      "iteration 8300 / 10000: loss 1.580067\n",
      "iteration 8400 / 10000: loss 1.564567\n",
      "iteration 8500 / 10000: loss 1.453786\n",
      "iteration 8600 / 10000: loss 1.528182\n",
      "iteration 8700 / 10000: loss 1.620089\n",
      "iteration 8800 / 10000: loss 1.528730\n",
      "iteration 8900 / 10000: loss 1.515957\n",
      "iteration 9000 / 10000: loss 1.495431\n",
      "iteration 9100 / 10000: loss 1.676277\n",
      "iteration 9200 / 10000: loss 1.556794\n",
      "iteration 9300 / 10000: loss 1.633817\n",
      "iteration 9400 / 10000: loss 1.620711\n",
      "iteration 9500 / 10000: loss 1.567884\n",
      "iteration 9600 / 10000: loss 1.551958\n",
      "iteration 9700 / 10000: loss 1.572299\n",
      "iteration 9800 / 10000: loss 1.474583\n",
      "iteration 9900 / 10000: loss 1.553683\n",
      "Validation accuracy:  0.509\n",
      "\n",
      "\n",
      "Starting iteration with rate 2.605338e-01, regstrength 2.947052e-02 and hidden layer size 18\n",
      "iteration 0 / 10000: loss 2.302586\n",
      "iteration 100 / 10000: loss 2.262359\n",
      "iteration 200 / 10000: loss 1.920105\n",
      "iteration 300 / 10000: loss 1.911055\n",
      "iteration 400 / 10000: loss 1.924196\n",
      "iteration 500 / 10000: loss 1.891699\n",
      "iteration 600 / 10000: loss 1.820882\n",
      "iteration 700 / 10000: loss 1.909496\n",
      "iteration 800 / 10000: loss 1.812291\n",
      "iteration 900 / 10000: loss 1.840128\n",
      "iteration 1000 / 10000: loss 1.846660\n",
      "iteration 1100 / 10000: loss 1.866746\n",
      "iteration 1200 / 10000: loss 2.004618\n",
      "iteration 1300 / 10000: loss 1.888121\n",
      "iteration 1400 / 10000: loss 1.843360\n",
      "iteration 1500 / 10000: loss 1.890868\n",
      "iteration 1600 / 10000: loss 1.805882\n",
      "iteration 1700 / 10000: loss 1.883674\n",
      "iteration 1800 / 10000: loss 1.833274\n",
      "iteration 1900 / 10000: loss 1.942686\n",
      "iteration 2000 / 10000: loss 1.853021\n",
      "iteration 2100 / 10000: loss 1.860009\n",
      "iteration 2200 / 10000: loss 1.984668\n",
      "iteration 2300 / 10000: loss 1.907860\n",
      "iteration 2400 / 10000: loss 1.927956\n",
      "iteration 2500 / 10000: loss 1.888771\n",
      "iteration 2600 / 10000: loss 1.792373\n",
      "iteration 2700 / 10000: loss 1.923832\n",
      "iteration 2800 / 10000: loss 1.855607\n",
      "iteration 2900 / 10000: loss 1.872953\n",
      "iteration 3000 / 10000: loss 1.886768\n",
      "iteration 3100 / 10000: loss 1.866845\n",
      "iteration 3200 / 10000: loss 1.867724\n",
      "iteration 3300 / 10000: loss 1.912525\n",
      "iteration 3400 / 10000: loss 1.793268\n",
      "iteration 3500 / 10000: loss 1.848623\n",
      "iteration 3600 / 10000: loss 1.776101\n",
      "iteration 3700 / 10000: loss 1.833139\n",
      "iteration 3800 / 10000: loss 1.914793\n",
      "iteration 3900 / 10000: loss 1.898151\n",
      "iteration 4000 / 10000: loss 1.861187\n",
      "iteration 4100 / 10000: loss 1.831569\n",
      "iteration 4200 / 10000: loss 1.885022\n",
      "iteration 4300 / 10000: loss 1.854167\n",
      "iteration 4400 / 10000: loss 1.903108\n",
      "iteration 4500 / 10000: loss 1.895742\n",
      "iteration 4600 / 10000: loss 2.056981\n",
      "iteration 4700 / 10000: loss 1.820648\n",
      "iteration 4800 / 10000: loss 1.825558\n",
      "iteration 4900 / 10000: loss 1.908182\n",
      "iteration 5000 / 10000: loss 1.914425\n",
      "iteration 5100 / 10000: loss 1.936513\n",
      "iteration 5200 / 10000: loss 1.814997\n",
      "iteration 5300 / 10000: loss 1.941179\n",
      "iteration 5400 / 10000: loss 1.803418\n",
      "iteration 5500 / 10000: loss 1.930477\n",
      "iteration 5600 / 10000: loss 1.863531\n",
      "iteration 5700 / 10000: loss 1.885273\n",
      "iteration 5800 / 10000: loss 1.817848\n",
      "iteration 5900 / 10000: loss 1.833481\n",
      "iteration 6000 / 10000: loss 1.866982\n",
      "iteration 6100 / 10000: loss 1.880601\n",
      "iteration 6200 / 10000: loss 1.896495\n",
      "iteration 6300 / 10000: loss 1.865777\n",
      "iteration 6400 / 10000: loss 1.865203\n",
      "iteration 6500 / 10000: loss 1.866338\n",
      "iteration 6600 / 10000: loss 1.861812\n",
      "iteration 6700 / 10000: loss 1.877520\n",
      "iteration 6800 / 10000: loss 1.841968\n",
      "iteration 6900 / 10000: loss 1.807005\n",
      "iteration 7000 / 10000: loss 1.925960\n",
      "iteration 7100 / 10000: loss 1.989781\n",
      "iteration 7200 / 10000: loss 1.879619\n",
      "iteration 7300 / 10000: loss 1.849889\n",
      "iteration 7400 / 10000: loss 1.855438\n",
      "iteration 7500 / 10000: loss 1.827143\n",
      "iteration 7600 / 10000: loss 1.832042\n",
      "iteration 7700 / 10000: loss 1.799879\n",
      "iteration 7800 / 10000: loss 1.880403\n",
      "iteration 7900 / 10000: loss 1.772089\n",
      "iteration 8000 / 10000: loss 1.931881\n",
      "iteration 8100 / 10000: loss 1.848108\n",
      "iteration 8200 / 10000: loss 1.930414\n",
      "iteration 8300 / 10000: loss 1.782022\n",
      "iteration 8400 / 10000: loss 1.859412\n",
      "iteration 8500 / 10000: loss 1.944116\n",
      "iteration 8600 / 10000: loss 1.804723\n",
      "iteration 8700 / 10000: loss 1.867006\n",
      "iteration 8800 / 10000: loss 1.861125\n",
      "iteration 8900 / 10000: loss 1.882671\n",
      "iteration 9000 / 10000: loss 1.855590\n",
      "iteration 9100 / 10000: loss 1.887413\n",
      "iteration 9200 / 10000: loss 1.912097\n",
      "iteration 9300 / 10000: loss 1.939088\n",
      "iteration 9400 / 10000: loss 1.752122\n",
      "iteration 9500 / 10000: loss 1.896682\n",
      "iteration 9600 / 10000: loss 1.816355\n",
      "iteration 9700 / 10000: loss 1.980435\n",
      "iteration 9800 / 10000: loss 1.824461\n",
      "iteration 9900 / 10000: loss 1.911593\n",
      "Validation accuracy:  0.487\n",
      "\n",
      "\n",
      "Starting iteration with rate 5.650323e-02, regstrength 2.023590e-03 and hidden layer size 12\n",
      "iteration 0 / 10000: loss 2.302585\n",
      "iteration 100 / 10000: loss 2.302132\n",
      "iteration 200 / 10000: loss 2.301712\n",
      "iteration 300 / 10000: loss 2.300529\n",
      "iteration 400 / 10000: loss 2.237054\n",
      "iteration 500 / 10000: loss 2.054737\n",
      "iteration 600 / 10000: loss 1.897346\n",
      "iteration 700 / 10000: loss 1.705962\n",
      "iteration 800 / 10000: loss 1.692865\n",
      "iteration 900 / 10000: loss 1.751905\n",
      "iteration 1000 / 10000: loss 1.592345\n",
      "iteration 1100 / 10000: loss 1.585916\n",
      "iteration 1200 / 10000: loss 1.536379\n",
      "iteration 1300 / 10000: loss 1.498363\n",
      "iteration 1400 / 10000: loss 1.550482\n",
      "iteration 1500 / 10000: loss 1.471048\n",
      "iteration 1600 / 10000: loss 1.419899\n",
      "iteration 1700 / 10000: loss 1.435080\n",
      "iteration 1800 / 10000: loss 1.469659\n",
      "iteration 1900 / 10000: loss 1.352206\n",
      "iteration 2000 / 10000: loss 1.439692\n",
      "iteration 2100 / 10000: loss 1.443510\n",
      "iteration 2200 / 10000: loss 1.312300\n",
      "iteration 2300 / 10000: loss 1.478640\n",
      "iteration 2400 / 10000: loss 1.321304\n",
      "iteration 2500 / 10000: loss 1.449138\n",
      "iteration 2600 / 10000: loss 1.399978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2700 / 10000: loss 1.407606\n",
      "iteration 2800 / 10000: loss 1.570463\n",
      "iteration 2900 / 10000: loss 1.435671\n",
      "iteration 3000 / 10000: loss 1.312213\n",
      "iteration 3100 / 10000: loss 1.470047\n",
      "iteration 3200 / 10000: loss 1.444248\n",
      "iteration 3300 / 10000: loss 1.512257\n",
      "iteration 3400 / 10000: loss 1.319917\n",
      "iteration 3500 / 10000: loss 1.527247\n",
      "iteration 3600 / 10000: loss 1.512363\n",
      "iteration 3700 / 10000: loss 1.484125\n",
      "iteration 3800 / 10000: loss 1.478022\n",
      "iteration 3900 / 10000: loss 1.439000\n",
      "iteration 4000 / 10000: loss 1.447258\n",
      "iteration 4100 / 10000: loss 1.409302\n",
      "iteration 4200 / 10000: loss 1.338082\n",
      "iteration 4300 / 10000: loss 1.558621\n",
      "iteration 4400 / 10000: loss 1.384228\n",
      "iteration 4500 / 10000: loss 1.466829\n",
      "iteration 4600 / 10000: loss 1.563885\n",
      "iteration 4700 / 10000: loss 1.377424\n",
      "iteration 4800 / 10000: loss 1.446931\n",
      "iteration 4900 / 10000: loss 1.305346\n",
      "iteration 5000 / 10000: loss 1.430273\n",
      "iteration 5100 / 10000: loss 1.403147\n",
      "iteration 5200 / 10000: loss 1.397489\n",
      "iteration 5300 / 10000: loss 1.389466\n",
      "iteration 5400 / 10000: loss 1.333229\n",
      "iteration 5500 / 10000: loss 1.500212\n",
      "iteration 5600 / 10000: loss 1.384567\n",
      "iteration 5700 / 10000: loss 1.329230\n",
      "iteration 5800 / 10000: loss 1.447583\n",
      "iteration 5900 / 10000: loss 1.346613\n",
      "iteration 6000 / 10000: loss 1.365618\n",
      "iteration 6100 / 10000: loss 1.347196\n",
      "iteration 6200 / 10000: loss 1.295484\n",
      "iteration 6300 / 10000: loss 1.382565\n",
      "iteration 6400 / 10000: loss 1.428370\n",
      "iteration 6500 / 10000: loss 1.259932\n",
      "iteration 6600 / 10000: loss 1.299781\n",
      "iteration 6700 / 10000: loss 1.340103\n",
      "iteration 6800 / 10000: loss 1.448849\n",
      "iteration 6900 / 10000: loss 1.504114\n",
      "iteration 7000 / 10000: loss 1.276924\n",
      "iteration 7100 / 10000: loss 1.316871\n",
      "iteration 7200 / 10000: loss 1.424034\n",
      "iteration 7300 / 10000: loss 1.372171\n",
      "iteration 7400 / 10000: loss 1.485337\n",
      "iteration 7500 / 10000: loss 1.421351\n",
      "iteration 7600 / 10000: loss 1.368704\n",
      "iteration 7700 / 10000: loss 1.393391\n",
      "iteration 7800 / 10000: loss 1.249795\n",
      "iteration 7900 / 10000: loss 1.451912\n",
      "iteration 8000 / 10000: loss 1.433528\n",
      "iteration 8100 / 10000: loss 1.294635\n",
      "iteration 8200 / 10000: loss 1.398950\n",
      "iteration 8300 / 10000: loss 1.398511\n",
      "iteration 8400 / 10000: loss 1.195632\n",
      "iteration 8500 / 10000: loss 1.448287\n",
      "iteration 8600 / 10000: loss 1.385734\n",
      "iteration 8700 / 10000: loss 1.422767\n",
      "iteration 8800 / 10000: loss 1.402621\n",
      "iteration 8900 / 10000: loss 1.445368\n",
      "iteration 9000 / 10000: loss 1.283384\n",
      "iteration 9100 / 10000: loss 1.479013\n",
      "iteration 9200 / 10000: loss 1.410737\n",
      "iteration 9300 / 10000: loss 1.432351\n",
      "iteration 9400 / 10000: loss 1.419951\n",
      "iteration 9500 / 10000: loss 1.368302\n",
      "iteration 9600 / 10000: loss 1.343605\n",
      "iteration 9700 / 10000: loss 1.386412\n",
      "iteration 9800 / 10000: loss 1.437437\n",
      "iteration 9900 / 10000: loss 1.396098\n",
      "Validation accuracy:  0.525\n",
      "\n",
      "\n",
      "Starting iteration with rate 3.129817e-01, regstrength 3.088844e-03 and hidden layer size 48\n",
      "iteration 0 / 10000: loss 2.302585\n",
      "iteration 100 / 10000: loss 1.898115\n",
      "iteration 200 / 10000: loss 1.467800\n",
      "iteration 300 / 10000: loss 1.363401\n",
      "iteration 400 / 10000: loss 1.378492\n",
      "iteration 500 / 10000: loss 1.589381\n",
      "iteration 600 / 10000: loss 1.431693\n",
      "iteration 700 / 10000: loss 1.476943\n",
      "iteration 800 / 10000: loss 1.425063\n",
      "iteration 900 / 10000: loss 1.357560\n",
      "iteration 1000 / 10000: loss 1.350581\n",
      "iteration 1100 / 10000: loss 1.431340\n",
      "iteration 1200 / 10000: loss 1.408784\n",
      "iteration 1300 / 10000: loss 1.439377\n",
      "iteration 1400 / 10000: loss 1.341475\n",
      "iteration 1500 / 10000: loss 1.459578\n",
      "iteration 1600 / 10000: loss 1.362882\n",
      "iteration 1700 / 10000: loss 1.330181\n",
      "iteration 1800 / 10000: loss 1.189128\n",
      "iteration 1900 / 10000: loss 1.429461\n",
      "iteration 2000 / 10000: loss 1.441876\n",
      "iteration 2100 / 10000: loss 1.368372\n",
      "iteration 2200 / 10000: loss 1.270388\n",
      "iteration 2300 / 10000: loss 1.373203\n",
      "iteration 2400 / 10000: loss 1.383437\n",
      "iteration 2500 / 10000: loss 1.315167\n",
      "iteration 2600 / 10000: loss 1.414690\n",
      "iteration 2700 / 10000: loss 1.558198\n",
      "iteration 2800 / 10000: loss 1.251661\n",
      "iteration 2900 / 10000: loss 1.436374\n",
      "iteration 3000 / 10000: loss 1.396690\n",
      "iteration 3100 / 10000: loss 1.370225\n",
      "iteration 3200 / 10000: loss 1.357446\n",
      "iteration 3300 / 10000: loss 1.230163\n",
      "iteration 3400 / 10000: loss 1.316966\n",
      "iteration 3500 / 10000: loss 1.335876\n",
      "iteration 3600 / 10000: loss 1.299887\n",
      "iteration 3700 / 10000: loss 1.351042\n",
      "iteration 3800 / 10000: loss 1.343278\n",
      "iteration 3900 / 10000: loss 1.302926\n",
      "iteration 4000 / 10000: loss 1.370426\n",
      "iteration 4100 / 10000: loss 1.326378\n",
      "iteration 4200 / 10000: loss 1.225437\n",
      "iteration 4300 / 10000: loss 1.333141\n",
      "iteration 4400 / 10000: loss 1.394468\n",
      "iteration 4500 / 10000: loss 1.359771\n",
      "iteration 4600 / 10000: loss 1.477797\n",
      "iteration 4700 / 10000: loss 1.323969\n",
      "iteration 4800 / 10000: loss 1.352278\n",
      "iteration 4900 / 10000: loss 1.355499\n",
      "iteration 5000 / 10000: loss 1.243131\n",
      "iteration 5100 / 10000: loss 1.348206\n",
      "iteration 5200 / 10000: loss 1.301636\n",
      "iteration 5300 / 10000: loss 1.278710\n",
      "iteration 5400 / 10000: loss 1.380078\n",
      "iteration 5500 / 10000: loss 1.316538\n",
      "iteration 5600 / 10000: loss 1.298015\n",
      "iteration 5700 / 10000: loss 1.267011\n",
      "iteration 5800 / 10000: loss 1.341494\n",
      "iteration 5900 / 10000: loss 1.321483\n",
      "iteration 6000 / 10000: loss 1.244563\n",
      "iteration 6100 / 10000: loss 1.300221\n",
      "iteration 6200 / 10000: loss 1.320515\n",
      "iteration 6300 / 10000: loss 1.356030\n",
      "iteration 6400 / 10000: loss 1.373227\n",
      "iteration 6500 / 10000: loss 1.362788\n",
      "iteration 6600 / 10000: loss 1.251579\n",
      "iteration 6700 / 10000: loss 1.390391\n",
      "iteration 6800 / 10000: loss 1.334029\n",
      "iteration 6900 / 10000: loss 1.289085\n",
      "iteration 7000 / 10000: loss 1.263163\n",
      "iteration 7100 / 10000: loss 1.343087\n",
      "iteration 7200 / 10000: loss 1.258263\n",
      "iteration 7300 / 10000: loss 1.306344\n",
      "iteration 7400 / 10000: loss 1.361613\n",
      "iteration 7500 / 10000: loss 1.396475\n",
      "iteration 7600 / 10000: loss 1.267524\n",
      "iteration 7700 / 10000: loss 1.354102\n",
      "iteration 7800 / 10000: loss 1.303777\n",
      "iteration 7900 / 10000: loss 1.362318\n",
      "iteration 8000 / 10000: loss 1.432691\n",
      "iteration 8100 / 10000: loss 1.327217\n",
      "iteration 8200 / 10000: loss 1.317505\n",
      "iteration 8300 / 10000: loss 1.354548\n",
      "iteration 8400 / 10000: loss 1.342355\n",
      "iteration 8500 / 10000: loss 1.259760\n",
      "iteration 8600 / 10000: loss 1.399254\n",
      "iteration 8700 / 10000: loss 1.267031\n",
      "iteration 8800 / 10000: loss 1.306984\n",
      "iteration 8900 / 10000: loss 1.266151\n",
      "iteration 9000 / 10000: loss 1.480628\n",
      "iteration 9100 / 10000: loss 1.367235\n",
      "iteration 9200 / 10000: loss 1.300409\n",
      "iteration 9300 / 10000: loss 1.280588\n",
      "iteration 9400 / 10000: loss 1.328179\n",
      "iteration 9500 / 10000: loss 1.308195\n",
      "iteration 9600 / 10000: loss 1.307658\n",
      "iteration 9700 / 10000: loss 1.372818\n",
      "iteration 9800 / 10000: loss 1.375269\n",
      "iteration 9900 / 10000: loss 1.329463\n",
      "Validation accuracy:  0.57\n",
      "\n",
      "\n",
      "Starting iteration with rate 5.768134e-01, regstrength 1.526418e-03 and hidden layer size 49\n",
      "iteration 0 / 10000: loss 2.302585\n",
      "iteration 100 / 10000: loss 1.516573\n",
      "iteration 200 / 10000: loss 1.383287\n",
      "iteration 300 / 10000: loss 1.369439\n",
      "iteration 400 / 10000: loss 1.414379\n",
      "iteration 500 / 10000: loss 1.356604\n",
      "iteration 600 / 10000: loss 1.369713\n",
      "iteration 700 / 10000: loss 1.299165\n",
      "iteration 800 / 10000: loss 1.337753\n",
      "iteration 900 / 10000: loss 1.345957\n",
      "iteration 1000 / 10000: loss 1.300507\n",
      "iteration 1100 / 10000: loss 1.430139\n",
      "iteration 1200 / 10000: loss 1.447963\n",
      "iteration 1300 / 10000: loss 1.162525\n",
      "iteration 1400 / 10000: loss 1.300362\n",
      "iteration 1500 / 10000: loss 1.374213\n",
      "iteration 1600 / 10000: loss 1.338139\n",
      "iteration 1700 / 10000: loss 1.378315\n",
      "iteration 1800 / 10000: loss 1.275305\n",
      "iteration 1900 / 10000: loss 1.416940\n",
      "iteration 2000 / 10000: loss 1.192673\n",
      "iteration 2100 / 10000: loss 1.413282\n",
      "iteration 2200 / 10000: loss 1.201770\n",
      "iteration 2300 / 10000: loss 1.272126\n",
      "iteration 2400 / 10000: loss 1.283629\n",
      "iteration 2500 / 10000: loss 1.261631\n",
      "iteration 2600 / 10000: loss 1.194454\n",
      "iteration 2700 / 10000: loss 1.330245\n",
      "iteration 2800 / 10000: loss 1.280742\n",
      "iteration 2900 / 10000: loss 1.300384\n",
      "iteration 3000 / 10000: loss 1.202474\n",
      "iteration 3100 / 10000: loss 1.241630\n",
      "iteration 3200 / 10000: loss 1.222590\n",
      "iteration 3300 / 10000: loss 1.407508\n",
      "iteration 3400 / 10000: loss 1.232234\n",
      "iteration 3500 / 10000: loss 1.297494\n",
      "iteration 3600 / 10000: loss 1.296081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3700 / 10000: loss 1.420108\n",
      "iteration 3800 / 10000: loss 1.208221\n",
      "iteration 3900 / 10000: loss 1.205861\n",
      "iteration 4000 / 10000: loss 1.262558\n",
      "iteration 4100 / 10000: loss 1.361935\n",
      "iteration 4200 / 10000: loss 1.293236\n",
      "iteration 4300 / 10000: loss 1.195955\n",
      "iteration 4400 / 10000: loss 1.265698\n",
      "iteration 4500 / 10000: loss 1.222636\n",
      "iteration 4600 / 10000: loss 1.347304\n",
      "iteration 4700 / 10000: loss 1.274673\n",
      "iteration 4800 / 10000: loss 1.338785\n",
      "iteration 4900 / 10000: loss 1.260759\n",
      "iteration 5000 / 10000: loss 1.267301\n",
      "iteration 5100 / 10000: loss 1.434883\n",
      "iteration 5200 / 10000: loss 1.224795\n",
      "iteration 5300 / 10000: loss 1.368246\n",
      "iteration 5400 / 10000: loss 1.243142\n",
      "iteration 5500 / 10000: loss 1.396041\n",
      "iteration 5600 / 10000: loss 1.222209\n",
      "iteration 5700 / 10000: loss 1.321766\n",
      "iteration 5800 / 10000: loss 1.238372\n",
      "iteration 5900 / 10000: loss 1.154574\n",
      "iteration 6000 / 10000: loss 1.288770\n",
      "iteration 6100 / 10000: loss 1.285680\n",
      "iteration 6200 / 10000: loss 1.322902\n",
      "iteration 6300 / 10000: loss 1.365595\n",
      "iteration 6400 / 10000: loss 1.274738\n",
      "iteration 6500 / 10000: loss 1.156152\n",
      "iteration 6600 / 10000: loss 1.250160\n",
      "iteration 6700 / 10000: loss 1.356759\n",
      "iteration 6800 / 10000: loss 1.282817\n",
      "iteration 6900 / 10000: loss 1.340679\n",
      "iteration 7000 / 10000: loss 1.378143\n",
      "iteration 7100 / 10000: loss 1.265441\n",
      "iteration 7200 / 10000: loss 1.306840\n",
      "iteration 7300 / 10000: loss 1.199545\n",
      "iteration 7400 / 10000: loss 1.362356\n",
      "iteration 7500 / 10000: loss 1.255999\n",
      "iteration 7600 / 10000: loss 1.252171\n",
      "iteration 7700 / 10000: loss 1.260356\n",
      "iteration 7800 / 10000: loss 1.265377\n",
      "iteration 7900 / 10000: loss 1.208909\n",
      "iteration 8000 / 10000: loss 1.268786\n",
      "iteration 8100 / 10000: loss 1.240635\n",
      "iteration 8200 / 10000: loss 1.175370\n",
      "iteration 8300 / 10000: loss 1.162691\n",
      "iteration 8400 / 10000: loss 1.232536\n",
      "iteration 8500 / 10000: loss 1.258845\n",
      "iteration 8600 / 10000: loss 1.287791\n",
      "iteration 8700 / 10000: loss 1.262652\n",
      "iteration 8800 / 10000: loss 1.215689\n",
      "iteration 8900 / 10000: loss 1.277045\n",
      "iteration 9000 / 10000: loss 1.176799\n",
      "iteration 9100 / 10000: loss 1.127423\n",
      "iteration 9200 / 10000: loss 1.346014\n",
      "iteration 9300 / 10000: loss 1.384889\n",
      "iteration 9400 / 10000: loss 1.251847\n",
      "iteration 9500 / 10000: loss 1.207088\n",
      "iteration 9600 / 10000: loss 1.117748\n",
      "iteration 9700 / 10000: loss 1.165056\n",
      "iteration 9800 / 10000: loss 1.261476\n",
      "iteration 9900 / 10000: loss 1.253484\n",
      "Validation accuracy:  0.572\n",
      "\n",
      "\n",
      "Starting iteration with rate 8.668315e-02, regstrength 2.811769e-01 and hidden layer size 36\n",
      "iteration 0 / 10000: loss 2.302603\n",
      "iteration 100 / 10000: loss 2.302999\n",
      "iteration 200 / 10000: loss 2.303000\n",
      "iteration 300 / 10000: loss 2.303409\n",
      "iteration 400 / 10000: loss 2.302598\n",
      "iteration 500 / 10000: loss 2.302878\n",
      "iteration 600 / 10000: loss 2.302304\n",
      "iteration 700 / 10000: loss 2.302966\n",
      "iteration 800 / 10000: loss 2.303414\n",
      "iteration 900 / 10000: loss 2.301790\n",
      "iteration 1000 / 10000: loss 2.303088\n",
      "iteration 1100 / 10000: loss 2.303013\n",
      "iteration 1200 / 10000: loss 2.303043\n",
      "iteration 1300 / 10000: loss 2.302998\n",
      "iteration 1400 / 10000: loss 2.303142\n",
      "iteration 1500 / 10000: loss 2.302777\n",
      "iteration 1600 / 10000: loss 2.302722\n",
      "iteration 1700 / 10000: loss 2.302975\n",
      "iteration 1800 / 10000: loss 2.302660\n",
      "iteration 1900 / 10000: loss 2.302552\n",
      "iteration 2000 / 10000: loss 2.302436\n",
      "iteration 2100 / 10000: loss 2.302398\n",
      "iteration 2200 / 10000: loss 2.302657\n",
      "iteration 2300 / 10000: loss 2.302753\n",
      "iteration 2400 / 10000: loss 2.302631\n",
      "iteration 2500 / 10000: loss 2.302537\n",
      "iteration 2600 / 10000: loss 2.302317\n",
      "iteration 2700 / 10000: loss 2.303150\n",
      "iteration 2800 / 10000: loss 2.303083\n",
      "iteration 2900 / 10000: loss 2.302571\n",
      "iteration 3000 / 10000: loss 2.302523\n",
      "iteration 3100 / 10000: loss 2.302800\n",
      "iteration 3200 / 10000: loss 2.302834\n",
      "iteration 3300 / 10000: loss 2.303077\n",
      "iteration 3400 / 10000: loss 2.302883\n",
      "iteration 3500 / 10000: loss 2.302856\n",
      "iteration 3600 / 10000: loss 2.302556\n",
      "iteration 3700 / 10000: loss 2.302517\n",
      "iteration 3800 / 10000: loss 2.303192\n",
      "iteration 3900 / 10000: loss 2.302693\n",
      "iteration 4000 / 10000: loss 2.302643\n",
      "iteration 4100 / 10000: loss 2.302374\n",
      "iteration 4200 / 10000: loss 2.302655\n",
      "iteration 4300 / 10000: loss 2.302021\n",
      "iteration 4400 / 10000: loss 2.302833\n",
      "iteration 4500 / 10000: loss 2.302298\n",
      "iteration 4600 / 10000: loss 2.302221\n",
      "iteration 4700 / 10000: loss 2.303217\n",
      "iteration 4800 / 10000: loss 2.302978\n",
      "iteration 4900 / 10000: loss 2.302638\n",
      "iteration 5000 / 10000: loss 2.302527\n",
      "iteration 5100 / 10000: loss 2.302888\n",
      "iteration 5200 / 10000: loss 2.302503\n",
      "iteration 5300 / 10000: loss 2.302903\n",
      "iteration 5400 / 10000: loss 2.303391\n",
      "iteration 5500 / 10000: loss 2.302057\n",
      "iteration 5600 / 10000: loss 2.302670\n",
      "iteration 5700 / 10000: loss 2.302604\n",
      "iteration 5800 / 10000: loss 2.302690\n",
      "iteration 5900 / 10000: loss 2.302165\n",
      "iteration 6000 / 10000: loss 2.302752\n",
      "iteration 6100 / 10000: loss 2.302778\n",
      "iteration 6200 / 10000: loss 2.302801\n",
      "iteration 6300 / 10000: loss 2.302716\n",
      "iteration 6400 / 10000: loss 2.302863\n",
      "iteration 6500 / 10000: loss 2.302295\n",
      "iteration 6600 / 10000: loss 2.303114\n",
      "iteration 6700 / 10000: loss 2.302873\n",
      "iteration 6800 / 10000: loss 2.302570\n",
      "iteration 6900 / 10000: loss 2.302551\n",
      "iteration 7000 / 10000: loss 2.303049\n",
      "iteration 7100 / 10000: loss 2.302689\n",
      "iteration 7200 / 10000: loss 2.302619\n",
      "iteration 7300 / 10000: loss 2.302711\n",
      "iteration 7400 / 10000: loss 2.302903\n",
      "iteration 7500 / 10000: loss 2.302302\n",
      "iteration 7600 / 10000: loss 2.302865\n",
      "iteration 7700 / 10000: loss 2.302472\n",
      "iteration 7800 / 10000: loss 2.302848\n",
      "iteration 7900 / 10000: loss 2.302305\n",
      "iteration 8000 / 10000: loss 2.302822\n",
      "iteration 8100 / 10000: loss 2.302473\n",
      "iteration 8200 / 10000: loss 2.302937\n",
      "iteration 8300 / 10000: loss 2.302580\n",
      "iteration 8400 / 10000: loss 2.302486\n",
      "iteration 8500 / 10000: loss 2.302204\n",
      "iteration 8600 / 10000: loss 2.302925\n",
      "iteration 8700 / 10000: loss 2.302268\n",
      "iteration 8800 / 10000: loss 2.302542\n",
      "iteration 8900 / 10000: loss 2.302462\n",
      "iteration 9000 / 10000: loss 2.303265\n",
      "iteration 9100 / 10000: loss 2.302722\n",
      "iteration 9200 / 10000: loss 2.302603\n",
      "iteration 9300 / 10000: loss 2.302530\n",
      "iteration 9400 / 10000: loss 2.302887\n",
      "iteration 9500 / 10000: loss 2.302475\n",
      "iteration 9600 / 10000: loss 2.302480\n",
      "iteration 9700 / 10000: loss 2.302463\n",
      "iteration 9800 / 10000: loss 2.302398\n",
      "iteration 9900 / 10000: loss 2.302815\n",
      "Validation accuracy:  0.112\n",
      "\n",
      "\n",
      "Starting iteration with rate 6.131785e-01, regstrength 6.551286e-01 and hidden layer size 12\n",
      "iteration 0 / 10000: loss 2.302598\n",
      "iteration 100 / 10000: loss 2.303306\n",
      "iteration 200 / 10000: loss 2.303202\n",
      "iteration 300 / 10000: loss 2.304378\n",
      "iteration 400 / 10000: loss 2.303550\n",
      "iteration 500 / 10000: loss 2.304593\n",
      "iteration 600 / 10000: loss 2.303407\n",
      "iteration 700 / 10000: loss 2.307293\n",
      "iteration 800 / 10000: loss 2.302593\n",
      "iteration 900 / 10000: loss 2.303626\n",
      "iteration 1000 / 10000: loss 2.302838\n",
      "iteration 1100 / 10000: loss 2.303234\n",
      "iteration 1200 / 10000: loss 2.304158\n",
      "iteration 1300 / 10000: loss 2.304278\n",
      "iteration 1400 / 10000: loss 2.304478\n",
      "iteration 1500 / 10000: loss 2.303348\n",
      "iteration 1600 / 10000: loss 2.304761\n",
      "iteration 1700 / 10000: loss 2.305370\n",
      "iteration 1800 / 10000: loss 2.303762\n",
      "iteration 1900 / 10000: loss 2.302719\n",
      "iteration 2000 / 10000: loss 2.302223\n",
      "iteration 2100 / 10000: loss 2.302664\n",
      "iteration 2200 / 10000: loss 2.303626\n",
      "iteration 2300 / 10000: loss 2.302334\n",
      "iteration 2400 / 10000: loss 2.303759\n",
      "iteration 2500 / 10000: loss 2.303676\n",
      "iteration 2600 / 10000: loss 2.303686\n",
      "iteration 2700 / 10000: loss 2.305557\n",
      "iteration 2800 / 10000: loss 2.302465\n",
      "iteration 2900 / 10000: loss 2.303212\n",
      "iteration 3000 / 10000: loss 2.302188\n",
      "iteration 3100 / 10000: loss 2.301862\n",
      "iteration 3200 / 10000: loss 2.302408\n",
      "iteration 3300 / 10000: loss 2.303020\n",
      "iteration 3400 / 10000: loss 2.305044\n",
      "iteration 3500 / 10000: loss 2.303853\n",
      "iteration 3600 / 10000: loss 2.302644\n",
      "iteration 3700 / 10000: loss 2.302727\n",
      "iteration 3800 / 10000: loss 2.303149\n",
      "iteration 3900 / 10000: loss 2.303339\n",
      "iteration 4000 / 10000: loss 2.302932\n",
      "iteration 4100 / 10000: loss 2.302207\n",
      "iteration 4200 / 10000: loss 2.302813\n",
      "iteration 4300 / 10000: loss 2.304157\n",
      "iteration 4400 / 10000: loss 2.303201\n",
      "iteration 4500 / 10000: loss 2.303058\n",
      "iteration 4600 / 10000: loss 2.303740\n",
      "iteration 4700 / 10000: loss 2.303955\n",
      "iteration 4800 / 10000: loss 2.303770\n",
      "iteration 4900 / 10000: loss 2.303558\n",
      "iteration 5000 / 10000: loss 2.302857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5100 / 10000: loss 2.302931\n",
      "iteration 5200 / 10000: loss 2.303543\n",
      "iteration 5300 / 10000: loss 2.304093\n",
      "iteration 5400 / 10000: loss 2.303183\n",
      "iteration 5500 / 10000: loss 2.304175\n",
      "iteration 5600 / 10000: loss 2.302774\n",
      "iteration 5700 / 10000: loss 2.302668\n",
      "iteration 5800 / 10000: loss 2.302257\n",
      "iteration 5900 / 10000: loss 2.302330\n",
      "iteration 6000 / 10000: loss 2.302739\n",
      "iteration 6100 / 10000: loss 2.302846\n",
      "iteration 6200 / 10000: loss 2.303630\n",
      "iteration 6300 / 10000: loss 2.303649\n",
      "iteration 6400 / 10000: loss 2.302473\n",
      "iteration 6500 / 10000: loss 2.302418\n",
      "iteration 6600 / 10000: loss 2.302976\n",
      "iteration 6700 / 10000: loss 2.305166\n",
      "iteration 6800 / 10000: loss 2.303062\n",
      "iteration 6900 / 10000: loss 2.303507\n",
      "iteration 7000 / 10000: loss 2.302131\n",
      "iteration 7100 / 10000: loss 2.303508\n",
      "iteration 7200 / 10000: loss 2.301559\n",
      "iteration 7300 / 10000: loss 2.303020\n",
      "iteration 7400 / 10000: loss 2.304438\n",
      "iteration 7500 / 10000: loss 2.303241\n",
      "iteration 7600 / 10000: loss 2.302587\n",
      "iteration 7700 / 10000: loss 2.303031\n",
      "iteration 7800 / 10000: loss 2.302351\n",
      "iteration 7900 / 10000: loss 2.302051\n",
      "iteration 8000 / 10000: loss 2.302953\n",
      "iteration 8100 / 10000: loss 2.304232\n",
      "iteration 8200 / 10000: loss 2.302977\n",
      "iteration 8300 / 10000: loss 2.303962\n",
      "iteration 8400 / 10000: loss 2.302746\n",
      "iteration 8500 / 10000: loss 2.302632\n",
      "iteration 8600 / 10000: loss 2.303024\n",
      "iteration 8700 / 10000: loss 2.302407\n",
      "iteration 8800 / 10000: loss 2.303126\n",
      "iteration 8900 / 10000: loss 2.302938\n",
      "iteration 9000 / 10000: loss 2.302919\n",
      "iteration 9100 / 10000: loss 2.303499\n",
      "iteration 9200 / 10000: loss 2.302233\n",
      "iteration 9300 / 10000: loss 2.303311\n",
      "iteration 9400 / 10000: loss 2.302930\n",
      "iteration 9500 / 10000: loss 2.301931\n",
      "iteration 9600 / 10000: loss 2.303154\n",
      "iteration 9700 / 10000: loss 2.302531\n",
      "iteration 9800 / 10000: loss 2.303402\n",
      "iteration 9900 / 10000: loss 2.302366\n",
      "Validation accuracy:  0.112\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.805320e-01, regstrength 1.000000e+00 and hidden layer size 10\n",
      "iteration 0 / 10000: loss 2.302602\n",
      "iteration 100 / 10000: loss 2.303132\n",
      "iteration 200 / 10000: loss 2.303321\n",
      "iteration 300 / 10000: loss 2.302067\n",
      "iteration 400 / 10000: loss 2.302745\n",
      "iteration 500 / 10000: loss 2.302586\n",
      "iteration 600 / 10000: loss 2.303159\n",
      "iteration 700 / 10000: loss 2.302506\n",
      "iteration 800 / 10000: loss 2.302902\n",
      "iteration 900 / 10000: loss 2.302955\n",
      "iteration 1000 / 10000: loss 2.302381\n",
      "iteration 1100 / 10000: loss 2.302683\n",
      "iteration 1200 / 10000: loss 2.302533\n",
      "iteration 1300 / 10000: loss 2.302497\n",
      "iteration 1400 / 10000: loss 2.303364\n",
      "iteration 1500 / 10000: loss 2.302824\n",
      "iteration 1600 / 10000: loss 2.302569\n",
      "iteration 1700 / 10000: loss 2.302829\n",
      "iteration 1800 / 10000: loss 2.302568\n",
      "iteration 1900 / 10000: loss 2.303038\n",
      "iteration 2000 / 10000: loss 2.302762\n",
      "iteration 2100 / 10000: loss 2.302563\n",
      "iteration 2200 / 10000: loss 2.302745\n",
      "iteration 2300 / 10000: loss 2.303903\n",
      "iteration 2400 / 10000: loss 2.302955\n",
      "iteration 2500 / 10000: loss 2.302508\n",
      "iteration 2600 / 10000: loss 2.302610\n",
      "iteration 2700 / 10000: loss 2.302362\n",
      "iteration 2800 / 10000: loss 2.302584\n",
      "iteration 2900 / 10000: loss 2.303242\n",
      "iteration 3000 / 10000: loss 2.302784\n",
      "iteration 3100 / 10000: loss 2.302841\n",
      "iteration 3200 / 10000: loss 2.302689\n",
      "iteration 3300 / 10000: loss 2.302961\n",
      "iteration 3400 / 10000: loss 2.302886\n",
      "iteration 3500 / 10000: loss 2.303154\n",
      "iteration 3600 / 10000: loss 2.302884\n",
      "iteration 3700 / 10000: loss 2.302768\n",
      "iteration 3800 / 10000: loss 2.302635\n",
      "iteration 3900 / 10000: loss 2.302502\n",
      "iteration 4000 / 10000: loss 2.302772\n",
      "iteration 4100 / 10000: loss 2.302509\n",
      "iteration 4200 / 10000: loss 2.302686\n",
      "iteration 4300 / 10000: loss 2.302648\n",
      "iteration 4400 / 10000: loss 2.303062\n",
      "iteration 4500 / 10000: loss 2.302036\n",
      "iteration 4600 / 10000: loss 2.303000\n",
      "iteration 4700 / 10000: loss 2.302768\n",
      "iteration 4800 / 10000: loss 2.302666\n",
      "iteration 4900 / 10000: loss 2.302685\n",
      "iteration 5000 / 10000: loss 2.303046\n",
      "iteration 5100 / 10000: loss 2.302952\n",
      "iteration 5200 / 10000: loss 2.302093\n",
      "iteration 5300 / 10000: loss 2.302983\n",
      "iteration 5400 / 10000: loss 2.302237\n",
      "iteration 5500 / 10000: loss 2.302676\n",
      "iteration 5600 / 10000: loss 2.302430\n",
      "iteration 5700 / 10000: loss 2.302933\n",
      "iteration 5800 / 10000: loss 2.303085\n",
      "iteration 5900 / 10000: loss 2.303248\n",
      "iteration 6000 / 10000: loss 2.302599\n",
      "iteration 6100 / 10000: loss 2.302673\n",
      "iteration 6200 / 10000: loss 2.302837\n",
      "iteration 6300 / 10000: loss 2.303061\n",
      "iteration 6400 / 10000: loss 2.302562\n",
      "iteration 6500 / 10000: loss 2.302524\n",
      "iteration 6600 / 10000: loss 2.302740\n",
      "iteration 6700 / 10000: loss 2.302816\n",
      "iteration 6800 / 10000: loss 2.302739\n",
      "iteration 6900 / 10000: loss 2.302740\n",
      "iteration 7000 / 10000: loss 2.303007\n",
      "iteration 7100 / 10000: loss 2.302279\n",
      "iteration 7200 / 10000: loss 2.302819\n",
      "iteration 7300 / 10000: loss 2.303162\n",
      "iteration 7400 / 10000: loss 2.302339\n",
      "iteration 7500 / 10000: loss 2.302467\n",
      "iteration 7600 / 10000: loss 2.302806\n",
      "iteration 7700 / 10000: loss 2.302356\n",
      "iteration 7800 / 10000: loss 2.302661\n",
      "iteration 7900 / 10000: loss 2.303319\n",
      "iteration 8000 / 10000: loss 2.302937\n",
      "iteration 8100 / 10000: loss 2.302592\n",
      "iteration 8200 / 10000: loss 2.302562\n",
      "iteration 8300 / 10000: loss 2.302701\n",
      "iteration 8400 / 10000: loss 2.302848\n",
      "iteration 8500 / 10000: loss 2.302452\n",
      "iteration 8600 / 10000: loss 2.302950\n",
      "iteration 8700 / 10000: loss 2.302612\n",
      "iteration 8800 / 10000: loss 2.302496\n",
      "iteration 8900 / 10000: loss 2.302640\n",
      "iteration 9000 / 10000: loss 2.302561\n",
      "iteration 9100 / 10000: loss 2.302852\n",
      "iteration 9200 / 10000: loss 2.302960\n",
      "iteration 9300 / 10000: loss 2.302330\n",
      "iteration 9400 / 10000: loss 2.302611\n",
      "iteration 9500 / 10000: loss 2.302847\n",
      "iteration 9600 / 10000: loss 2.302727\n",
      "iteration 9700 / 10000: loss 2.302451\n",
      "iteration 9800 / 10000: loss 2.302527\n",
      "iteration 9900 / 10000: loss 2.302673\n",
      "Validation accuracy:  0.098\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.919137e-01, regstrength 3.556480e-03 and hidden layer size 13\n",
      "iteration 0 / 10000: loss 2.302585\n",
      "iteration 100 / 10000: loss 2.300525\n",
      "iteration 200 / 10000: loss 1.795644\n",
      "iteration 300 / 10000: loss 1.557329\n",
      "iteration 400 / 10000: loss 1.527226\n",
      "iteration 500 / 10000: loss 1.416925\n",
      "iteration 600 / 10000: loss 1.425774\n",
      "iteration 700 / 10000: loss 1.452661\n",
      "iteration 800 / 10000: loss 1.506054\n",
      "iteration 900 / 10000: loss 1.440674\n",
      "iteration 1000 / 10000: loss 1.369353\n",
      "iteration 1100 / 10000: loss 1.455468\n",
      "iteration 1200 / 10000: loss 1.490133\n",
      "iteration 1300 / 10000: loss 1.466096\n",
      "iteration 1400 / 10000: loss 1.478792\n",
      "iteration 1500 / 10000: loss 1.409109\n",
      "iteration 1600 / 10000: loss 1.340392\n",
      "iteration 1700 / 10000: loss 1.573392\n",
      "iteration 1800 / 10000: loss 1.464320\n",
      "iteration 1900 / 10000: loss 1.534411\n",
      "iteration 2000 / 10000: loss 1.377174\n",
      "iteration 2100 / 10000: loss 1.426949\n",
      "iteration 2200 / 10000: loss 1.463143\n",
      "iteration 2300 / 10000: loss 1.557184\n",
      "iteration 2400 / 10000: loss 1.516654\n",
      "iteration 2500 / 10000: loss 1.481723\n",
      "iteration 2600 / 10000: loss 1.521278\n",
      "iteration 2700 / 10000: loss 1.382182\n",
      "iteration 2800 / 10000: loss 1.426091\n",
      "iteration 2900 / 10000: loss 1.507422\n",
      "iteration 3000 / 10000: loss 1.422134\n",
      "iteration 3100 / 10000: loss 1.391880\n",
      "iteration 3200 / 10000: loss 1.418184\n",
      "iteration 3300 / 10000: loss 1.456944\n",
      "iteration 3400 / 10000: loss 1.351781\n",
      "iteration 3500 / 10000: loss 1.381346\n",
      "iteration 3600 / 10000: loss 1.443423\n",
      "iteration 3700 / 10000: loss 1.380730\n",
      "iteration 3800 / 10000: loss 1.416297\n",
      "iteration 3900 / 10000: loss 1.560810\n",
      "iteration 4000 / 10000: loss 1.481771\n",
      "iteration 4100 / 10000: loss 1.465897\n",
      "iteration 4200 / 10000: loss 1.398994\n",
      "iteration 4300 / 10000: loss 1.529927\n",
      "iteration 4400 / 10000: loss 1.484407\n",
      "iteration 4500 / 10000: loss 1.505549\n",
      "iteration 4600 / 10000: loss 1.480368\n",
      "iteration 4700 / 10000: loss 1.591431\n",
      "iteration 4800 / 10000: loss 1.452149\n",
      "iteration 4900 / 10000: loss 1.356709\n",
      "iteration 5000 / 10000: loss 1.336811\n",
      "iteration 5100 / 10000: loss 1.453355\n",
      "iteration 5200 / 10000: loss 1.471762\n",
      "iteration 5300 / 10000: loss 1.449524\n",
      "iteration 5400 / 10000: loss 1.316485\n",
      "iteration 5500 / 10000: loss 1.368759\n",
      "iteration 5600 / 10000: loss 1.436056\n",
      "iteration 5700 / 10000: loss 1.353365\n",
      "iteration 5800 / 10000: loss 1.495895\n",
      "iteration 5900 / 10000: loss 1.406470\n",
      "iteration 6000 / 10000: loss 1.368326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6100 / 10000: loss 1.424526\n",
      "iteration 6200 / 10000: loss 1.315896\n",
      "iteration 6300 / 10000: loss 1.500410\n",
      "iteration 6400 / 10000: loss 1.459916\n",
      "iteration 6500 / 10000: loss 1.450712\n",
      "iteration 6600 / 10000: loss 1.388978\n",
      "iteration 6700 / 10000: loss 1.467159\n",
      "iteration 6800 / 10000: loss 1.477071\n",
      "iteration 6900 / 10000: loss 1.370885\n",
      "iteration 7000 / 10000: loss 1.437062\n",
      "iteration 7100 / 10000: loss 1.465850\n",
      "iteration 7200 / 10000: loss 1.447033\n",
      "iteration 7300 / 10000: loss 1.468152\n",
      "iteration 7400 / 10000: loss 1.376891\n",
      "iteration 7500 / 10000: loss 1.488475\n",
      "iteration 7600 / 10000: loss 1.454415\n",
      "iteration 7700 / 10000: loss 1.468680\n",
      "iteration 7800 / 10000: loss 1.505212\n",
      "iteration 7900 / 10000: loss 1.619820\n",
      "iteration 8000 / 10000: loss 1.501995\n",
      "iteration 8100 / 10000: loss 1.511108\n",
      "iteration 8200 / 10000: loss 1.502269\n",
      "iteration 8300 / 10000: loss 1.440324\n",
      "iteration 8400 / 10000: loss 1.449861\n",
      "iteration 8500 / 10000: loss 1.417692\n",
      "iteration 8600 / 10000: loss 1.473459\n",
      "iteration 8700 / 10000: loss 1.461322\n",
      "iteration 8800 / 10000: loss 1.483050\n",
      "iteration 8900 / 10000: loss 1.406623\n",
      "iteration 9000 / 10000: loss 1.382010\n",
      "iteration 9100 / 10000: loss 1.525023\n",
      "iteration 9200 / 10000: loss 1.460336\n",
      "iteration 9300 / 10000: loss 1.416505\n",
      "iteration 9400 / 10000: loss 1.507519\n",
      "iteration 9500 / 10000: loss 1.424145\n",
      "iteration 9600 / 10000: loss 1.516403\n",
      "iteration 9700 / 10000: loss 1.573085\n",
      "iteration 9800 / 10000: loss 1.516749\n",
      "iteration 9900 / 10000: loss 1.461247\n",
      "Validation accuracy:  0.507\n",
      "\n",
      "\n",
      "Starting iteration with rate 2.168748e-01, regstrength 7.906043e-02 and hidden layer size 23\n",
      "iteration 0 / 10000: loss 2.302588\n",
      "iteration 100 / 10000: loss 2.302603\n",
      "iteration 200 / 10000: loss 2.249849\n",
      "iteration 300 / 10000: loss 2.215117\n",
      "iteration 400 / 10000: loss 2.173578\n",
      "iteration 500 / 10000: loss 2.132502\n",
      "iteration 600 / 10000: loss 2.194742\n",
      "iteration 700 / 10000: loss 2.230946\n",
      "iteration 800 / 10000: loss 2.177516\n",
      "iteration 900 / 10000: loss 2.205578\n",
      "iteration 1000 / 10000: loss 2.213334\n",
      "iteration 1100 / 10000: loss 2.135618\n",
      "iteration 1200 / 10000: loss 2.138875\n",
      "iteration 1300 / 10000: loss 2.148408\n",
      "iteration 1400 / 10000: loss 2.196222\n",
      "iteration 1500 / 10000: loss 2.196461\n",
      "iteration 1600 / 10000: loss 2.141554\n",
      "iteration 1700 / 10000: loss 2.189531\n",
      "iteration 1800 / 10000: loss 2.178594\n",
      "iteration 1900 / 10000: loss 2.171380\n",
      "iteration 2000 / 10000: loss 2.211109\n",
      "iteration 2100 / 10000: loss 2.191403\n",
      "iteration 2200 / 10000: loss 2.214233\n",
      "iteration 2300 / 10000: loss 2.174213\n",
      "iteration 2400 / 10000: loss 2.187631\n",
      "iteration 2500 / 10000: loss 2.180584\n",
      "iteration 2600 / 10000: loss 2.183022\n",
      "iteration 2700 / 10000: loss 2.217173\n",
      "iteration 2800 / 10000: loss 2.137160\n",
      "iteration 2900 / 10000: loss 2.154031\n",
      "iteration 3000 / 10000: loss 2.183967\n",
      "iteration 3100 / 10000: loss 2.186898\n",
      "iteration 3200 / 10000: loss 2.192218\n",
      "iteration 3300 / 10000: loss 2.218589\n",
      "iteration 3400 / 10000: loss 2.198369\n",
      "iteration 3500 / 10000: loss 2.183076\n",
      "iteration 3600 / 10000: loss 2.188236\n",
      "iteration 3700 / 10000: loss 2.165151\n",
      "iteration 3800 / 10000: loss 2.198007\n",
      "iteration 3900 / 10000: loss 2.151103\n",
      "iteration 4000 / 10000: loss 2.176719\n",
      "iteration 4100 / 10000: loss 2.138224\n",
      "iteration 4200 / 10000: loss 2.195255\n",
      "iteration 4300 / 10000: loss 2.165462\n",
      "iteration 4400 / 10000: loss 2.212321\n",
      "iteration 4500 / 10000: loss 2.187190\n",
      "iteration 4600 / 10000: loss 2.233412\n",
      "iteration 4700 / 10000: loss 2.181696\n",
      "iteration 4800 / 10000: loss 2.210818\n",
      "iteration 4900 / 10000: loss 2.216292\n",
      "iteration 5000 / 10000: loss 2.229597\n",
      "iteration 5100 / 10000: loss 2.161803\n",
      "iteration 5200 / 10000: loss 2.162580\n",
      "iteration 5300 / 10000: loss 2.202654\n",
      "iteration 5400 / 10000: loss 2.188459\n",
      "iteration 5500 / 10000: loss 2.161284\n",
      "iteration 5600 / 10000: loss 2.157881\n",
      "iteration 5700 / 10000: loss 2.189166\n",
      "iteration 5800 / 10000: loss 2.163330\n",
      "iteration 5900 / 10000: loss 2.106026\n",
      "iteration 6000 / 10000: loss 2.199666\n",
      "iteration 6100 / 10000: loss 2.184716\n",
      "iteration 6200 / 10000: loss 2.204218\n",
      "iteration 6300 / 10000: loss 2.191601\n",
      "iteration 6400 / 10000: loss 2.210093\n",
      "iteration 6500 / 10000: loss 2.175881\n",
      "iteration 6600 / 10000: loss 2.176076\n",
      "iteration 6700 / 10000: loss 2.215821\n",
      "iteration 6800 / 10000: loss 2.137196\n",
      "iteration 6900 / 10000: loss 2.183690\n",
      "iteration 7000 / 10000: loss 2.170509\n",
      "iteration 7100 / 10000: loss 2.194204\n",
      "iteration 7200 / 10000: loss 2.233741\n",
      "iteration 7300 / 10000: loss 2.212207\n",
      "iteration 7400 / 10000: loss 2.220860\n",
      "iteration 7500 / 10000: loss 2.194968\n",
      "iteration 7600 / 10000: loss 2.205646\n",
      "iteration 7700 / 10000: loss 2.180247\n",
      "iteration 7800 / 10000: loss 2.179694\n",
      "iteration 7900 / 10000: loss 2.209643\n",
      "iteration 8000 / 10000: loss 2.177236\n",
      "iteration 8100 / 10000: loss 2.163163\n",
      "iteration 8200 / 10000: loss 2.154959\n",
      "iteration 8300 / 10000: loss 2.155061\n",
      "iteration 8400 / 10000: loss 2.222922\n",
      "iteration 8500 / 10000: loss 2.135847\n",
      "iteration 8600 / 10000: loss 2.154744\n",
      "iteration 8700 / 10000: loss 2.195106\n",
      "iteration 8800 / 10000: loss 2.151445\n",
      "iteration 8900 / 10000: loss 2.231956\n",
      "iteration 9000 / 10000: loss 2.155476\n",
      "iteration 9100 / 10000: loss 2.201069\n",
      "iteration 9200 / 10000: loss 2.136227\n",
      "iteration 9300 / 10000: loss 2.212918\n",
      "iteration 9400 / 10000: loss 2.194085\n",
      "iteration 9500 / 10000: loss 2.131996\n",
      "iteration 9600 / 10000: loss 2.203918\n",
      "iteration 9700 / 10000: loss 2.169930\n",
      "iteration 9800 / 10000: loss 2.212790\n",
      "iteration 9900 / 10000: loss 2.220516\n",
      "Validation accuracy:  0.358\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.413669e-01, regstrength 5.963623e-02 and hidden layer size 16\n",
      "iteration 0 / 10000: loss 2.302587\n",
      "iteration 100 / 10000: loss 2.302212\n",
      "iteration 200 / 10000: loss 2.293725\n",
      "iteration 300 / 10000: loss 2.212139\n",
      "iteration 400 / 10000: loss 2.163707\n",
      "iteration 500 / 10000: loss 2.146601\n",
      "iteration 600 / 10000: loss 2.166047\n",
      "iteration 700 / 10000: loss 2.059084\n",
      "iteration 800 / 10000: loss 2.036791\n",
      "iteration 900 / 10000: loss 2.070616\n",
      "iteration 1000 / 10000: loss 2.104087\n",
      "iteration 1100 / 10000: loss 2.064484\n",
      "iteration 1200 / 10000: loss 2.070819\n",
      "iteration 1300 / 10000: loss 2.077487\n",
      "iteration 1400 / 10000: loss 2.120816\n",
      "iteration 1500 / 10000: loss 2.112618\n",
      "iteration 1600 / 10000: loss 2.109227\n",
      "iteration 1700 / 10000: loss 2.106104\n",
      "iteration 1800 / 10000: loss 2.155675\n",
      "iteration 1900 / 10000: loss 2.112210\n",
      "iteration 2000 / 10000: loss 2.124638\n",
      "iteration 2100 / 10000: loss 2.112520\n",
      "iteration 2200 / 10000: loss 2.142379\n",
      "iteration 2300 / 10000: loss 2.108913\n",
      "iteration 2400 / 10000: loss 2.121102\n",
      "iteration 2500 / 10000: loss 2.141561\n",
      "iteration 2600 / 10000: loss 2.011250\n",
      "iteration 2700 / 10000: loss 2.089960\n",
      "iteration 2800 / 10000: loss 2.142290\n",
      "iteration 2900 / 10000: loss 2.187464\n",
      "iteration 3000 / 10000: loss 2.089724\n",
      "iteration 3100 / 10000: loss 2.048836\n",
      "iteration 3200 / 10000: loss 2.082351\n",
      "iteration 3300 / 10000: loss 2.047794\n",
      "iteration 3400 / 10000: loss 2.062386\n",
      "iteration 3500 / 10000: loss 2.044657\n",
      "iteration 3600 / 10000: loss 2.083750\n",
      "iteration 3700 / 10000: loss 2.129229\n",
      "iteration 3800 / 10000: loss 2.110398\n",
      "iteration 3900 / 10000: loss 2.083354\n",
      "iteration 4000 / 10000: loss 2.145554\n",
      "iteration 4100 / 10000: loss 2.056493\n",
      "iteration 4200 / 10000: loss 1.986050\n",
      "iteration 4300 / 10000: loss 2.060653\n",
      "iteration 4400 / 10000: loss 2.073005\n",
      "iteration 4500 / 10000: loss 2.137856\n",
      "iteration 4600 / 10000: loss 2.138276\n",
      "iteration 4700 / 10000: loss 2.054791\n",
      "iteration 4800 / 10000: loss 2.108880\n",
      "iteration 4900 / 10000: loss 2.082744\n",
      "iteration 5000 / 10000: loss 2.105551\n",
      "iteration 5100 / 10000: loss 2.069851\n",
      "iteration 5200 / 10000: loss 2.033925\n",
      "iteration 5300 / 10000: loss 2.120317\n",
      "iteration 5400 / 10000: loss 2.146852\n",
      "iteration 5500 / 10000: loss 2.098020\n",
      "iteration 5600 / 10000: loss 2.024733\n",
      "iteration 5700 / 10000: loss 2.012796\n",
      "iteration 5800 / 10000: loss 2.139847\n",
      "iteration 5900 / 10000: loss 2.065264\n",
      "iteration 6000 / 10000: loss 2.044867\n",
      "iteration 6100 / 10000: loss 2.093357\n",
      "iteration 6200 / 10000: loss 2.096016\n",
      "iteration 6300 / 10000: loss 2.091181\n",
      "iteration 6400 / 10000: loss 2.070065\n",
      "iteration 6500 / 10000: loss 2.095500\n",
      "iteration 6600 / 10000: loss 2.132667\n",
      "iteration 6700 / 10000: loss 2.087020\n",
      "iteration 6800 / 10000: loss 2.111382\n",
      "iteration 6900 / 10000: loss 2.107136\n",
      "iteration 7000 / 10000: loss 2.055974\n",
      "iteration 7100 / 10000: loss 2.124786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7200 / 10000: loss 2.111165\n",
      "iteration 7300 / 10000: loss 2.042822\n",
      "iteration 7400 / 10000: loss 2.088653\n",
      "iteration 7500 / 10000: loss 2.167026\n",
      "iteration 7600 / 10000: loss 2.081321\n",
      "iteration 7700 / 10000: loss 2.160885\n",
      "iteration 7800 / 10000: loss 2.064628\n",
      "iteration 7900 / 10000: loss 2.107052\n",
      "iteration 8000 / 10000: loss 2.099915\n",
      "iteration 8100 / 10000: loss 2.045243\n",
      "iteration 8200 / 10000: loss 2.107991\n",
      "iteration 8300 / 10000: loss 2.099091\n",
      "iteration 8400 / 10000: loss 2.075445\n",
      "iteration 8500 / 10000: loss 2.095346\n",
      "iteration 8600 / 10000: loss 2.053267\n",
      "iteration 8700 / 10000: loss 2.074744\n",
      "iteration 8800 / 10000: loss 2.132036\n",
      "iteration 8900 / 10000: loss 2.024275\n",
      "iteration 9000 / 10000: loss 2.103189\n",
      "iteration 9100 / 10000: loss 2.097199\n",
      "iteration 9200 / 10000: loss 2.122937\n",
      "iteration 9300 / 10000: loss 2.054799\n",
      "iteration 9400 / 10000: loss 2.028215\n",
      "iteration 9500 / 10000: loss 2.139684\n",
      "iteration 9600 / 10000: loss 2.070047\n",
      "iteration 9700 / 10000: loss 2.069353\n",
      "iteration 9800 / 10000: loss 2.117062\n",
      "iteration 9900 / 10000: loss 2.115691\n",
      "Validation accuracy:  0.42\n",
      "\n",
      "\n",
      "Starting iteration with rate 2.450826e-01, regstrength 5.428675e-03 and hidden layer size 28\n",
      "iteration 0 / 10000: loss 2.302585\n",
      "iteration 100 / 10000: loss 2.202120\n",
      "iteration 200 / 10000: loss 1.685463\n",
      "iteration 300 / 10000: loss 1.588314\n",
      "iteration 400 / 10000: loss 1.451457\n",
      "iteration 500 / 10000: loss 1.487316\n",
      "iteration 600 / 10000: loss 1.575743\n",
      "iteration 700 / 10000: loss 1.431867\n",
      "iteration 800 / 10000: loss 1.595878\n",
      "iteration 900 / 10000: loss 1.422360\n",
      "iteration 1000 / 10000: loss 1.482260\n",
      "iteration 1100 / 10000: loss 1.538402\n",
      "iteration 1200 / 10000: loss 1.544880\n",
      "iteration 1300 / 10000: loss 1.410419\n",
      "iteration 1400 / 10000: loss 1.431497\n",
      "iteration 1500 / 10000: loss 1.542401\n",
      "iteration 1600 / 10000: loss 1.493577\n",
      "iteration 1700 / 10000: loss 1.594684\n",
      "iteration 1800 / 10000: loss 1.536629\n",
      "iteration 1900 / 10000: loss 1.453900\n",
      "iteration 2000 / 10000: loss 1.644337\n",
      "iteration 2100 / 10000: loss 1.485691\n",
      "iteration 2200 / 10000: loss 1.545633\n",
      "iteration 2300 / 10000: loss 1.493054\n",
      "iteration 2400 / 10000: loss 1.586620\n",
      "iteration 2500 / 10000: loss 1.457364\n",
      "iteration 2600 / 10000: loss 1.480071\n",
      "iteration 2700 / 10000: loss 1.418689\n",
      "iteration 2800 / 10000: loss 1.407754\n",
      "iteration 2900 / 10000: loss 1.573692\n",
      "iteration 3000 / 10000: loss 1.517573\n",
      "iteration 3100 / 10000: loss 1.518538\n",
      "iteration 3200 / 10000: loss 1.425195\n",
      "iteration 3300 / 10000: loss 1.478844\n",
      "iteration 3400 / 10000: loss 1.481482\n",
      "iteration 3500 / 10000: loss 1.491244\n",
      "iteration 3600 / 10000: loss 1.492464\n",
      "iteration 3700 / 10000: loss 1.476273\n",
      "iteration 3800 / 10000: loss 1.333704\n",
      "iteration 3900 / 10000: loss 1.366748\n",
      "iteration 4000 / 10000: loss 1.486524\n",
      "iteration 4100 / 10000: loss 1.337369\n",
      "iteration 4200 / 10000: loss 1.388524\n",
      "iteration 4300 / 10000: loss 1.438224\n",
      "iteration 4400 / 10000: loss 1.497502\n",
      "iteration 4500 / 10000: loss 1.524346\n",
      "iteration 4600 / 10000: loss 1.431242\n",
      "iteration 4700 / 10000: loss 1.472569\n",
      "iteration 4800 / 10000: loss 1.418824\n",
      "iteration 4900 / 10000: loss 1.367709\n",
      "iteration 5000 / 10000: loss 1.501579\n",
      "iteration 5100 / 10000: loss 1.419365\n",
      "iteration 5200 / 10000: loss 1.418342\n",
      "iteration 5300 / 10000: loss 1.431491\n",
      "iteration 5400 / 10000: loss 1.376638\n",
      "iteration 5500 / 10000: loss 1.468744\n",
      "iteration 5600 / 10000: loss 1.493057\n",
      "iteration 5700 / 10000: loss 1.484084\n",
      "iteration 5800 / 10000: loss 1.451848\n",
      "iteration 5900 / 10000: loss 1.454096\n",
      "iteration 6000 / 10000: loss 1.402637\n",
      "iteration 6100 / 10000: loss 1.542792\n",
      "iteration 6200 / 10000: loss 1.413180\n",
      "iteration 6300 / 10000: loss 1.539631\n",
      "iteration 6400 / 10000: loss 1.538638\n",
      "iteration 6500 / 10000: loss 1.355599\n",
      "iteration 6600 / 10000: loss 1.526447\n",
      "iteration 6700 / 10000: loss 1.437903\n",
      "iteration 6800 / 10000: loss 1.372432\n",
      "iteration 6900 / 10000: loss 1.422532\n",
      "iteration 7000 / 10000: loss 1.463705\n",
      "iteration 7100 / 10000: loss 1.442324\n",
      "iteration 7200 / 10000: loss 1.451542\n",
      "iteration 7300 / 10000: loss 1.417860\n",
      "iteration 7400 / 10000: loss 1.390298\n",
      "iteration 7500 / 10000: loss 1.539101\n",
      "iteration 7600 / 10000: loss 1.560282\n",
      "iteration 7700 / 10000: loss 1.490188\n",
      "iteration 7800 / 10000: loss 1.519096\n",
      "iteration 7900 / 10000: loss 1.457357\n",
      "iteration 8000 / 10000: loss 1.380985\n",
      "iteration 8100 / 10000: loss 1.572976\n",
      "iteration 8200 / 10000: loss 1.389154\n",
      "iteration 8300 / 10000: loss 1.357610\n",
      "iteration 8400 / 10000: loss 1.497131\n",
      "iteration 8500 / 10000: loss 1.358343\n",
      "iteration 8600 / 10000: loss 1.572853\n",
      "iteration 8700 / 10000: loss 1.508802\n",
      "iteration 8800 / 10000: loss 1.581484\n",
      "iteration 8900 / 10000: loss 1.436273\n",
      "iteration 9000 / 10000: loss 1.487729\n",
      "iteration 9100 / 10000: loss 1.395687\n",
      "iteration 9200 / 10000: loss 1.515074\n",
      "iteration 9300 / 10000: loss 1.415844\n",
      "iteration 9400 / 10000: loss 1.416329\n",
      "iteration 9500 / 10000: loss 1.461220\n",
      "iteration 9600 / 10000: loss 1.419001\n",
      "iteration 9700 / 10000: loss 1.444922\n",
      "iteration 9800 / 10000: loss 1.330419\n",
      "iteration 9900 / 10000: loss 1.552973\n",
      "Validation accuracy:  0.547\n",
      "\n",
      "\n",
      "Starting iteration with rate 8.154232e-02, regstrength 1.676833e-02 and hidden layer size 13\n",
      "iteration 0 / 10000: loss 2.302585\n",
      "iteration 100 / 10000: loss 2.302171\n",
      "iteration 200 / 10000: loss 2.303498\n",
      "iteration 300 / 10000: loss 2.227300\n",
      "iteration 400 / 10000: loss 2.033279\n",
      "iteration 500 / 10000: loss 1.909132\n",
      "iteration 600 / 10000: loss 1.853089\n",
      "iteration 700 / 10000: loss 1.809332\n",
      "iteration 800 / 10000: loss 1.788608\n",
      "iteration 900 / 10000: loss 1.731377\n",
      "iteration 1000 / 10000: loss 1.724941\n",
      "iteration 1100 / 10000: loss 1.689495\n",
      "iteration 1200 / 10000: loss 1.741297\n",
      "iteration 1300 / 10000: loss 1.629706\n",
      "iteration 1400 / 10000: loss 1.656746\n",
      "iteration 1500 / 10000: loss 1.596884\n",
      "iteration 1600 / 10000: loss 1.726209\n",
      "iteration 1700 / 10000: loss 1.709559\n",
      "iteration 1800 / 10000: loss 1.722384\n",
      "iteration 1900 / 10000: loss 1.781962\n",
      "iteration 2000 / 10000: loss 1.698783\n",
      "iteration 2100 / 10000: loss 1.727548\n",
      "iteration 2200 / 10000: loss 1.672820\n",
      "iteration 2300 / 10000: loss 1.728618\n",
      "iteration 2400 / 10000: loss 1.732187\n",
      "iteration 2500 / 10000: loss 1.774389\n",
      "iteration 2600 / 10000: loss 1.597452\n",
      "iteration 2700 / 10000: loss 1.696479\n",
      "iteration 2800 / 10000: loss 1.641754\n",
      "iteration 2900 / 10000: loss 1.717700\n",
      "iteration 3000 / 10000: loss 1.748808\n",
      "iteration 3100 / 10000: loss 1.655869\n",
      "iteration 3200 / 10000: loss 1.574919\n",
      "iteration 3300 / 10000: loss 1.699572\n",
      "iteration 3400 / 10000: loss 1.709371\n",
      "iteration 3500 / 10000: loss 1.770531\n",
      "iteration 3600 / 10000: loss 1.836285\n",
      "iteration 3700 / 10000: loss 1.719787\n",
      "iteration 3800 / 10000: loss 1.599351\n",
      "iteration 3900 / 10000: loss 1.763584\n",
      "iteration 4000 / 10000: loss 1.675838\n",
      "iteration 4100 / 10000: loss 1.629622\n",
      "iteration 4200 / 10000: loss 1.734175\n",
      "iteration 4300 / 10000: loss 1.621123\n",
      "iteration 4400 / 10000: loss 1.599140\n",
      "iteration 4500 / 10000: loss 1.695133\n",
      "iteration 4600 / 10000: loss 1.670946\n",
      "iteration 4700 / 10000: loss 1.673733\n",
      "iteration 4800 / 10000: loss 1.743124\n",
      "iteration 4900 / 10000: loss 1.862426\n",
      "iteration 5000 / 10000: loss 1.608819\n",
      "iteration 5100 / 10000: loss 1.841864\n",
      "iteration 5200 / 10000: loss 1.666183\n",
      "iteration 5300 / 10000: loss 1.783472\n",
      "iteration 5400 / 10000: loss 1.820442\n",
      "iteration 5500 / 10000: loss 1.803853\n",
      "iteration 5600 / 10000: loss 1.799169\n",
      "iteration 5700 / 10000: loss 1.705816\n",
      "iteration 5800 / 10000: loss 1.798803\n",
      "iteration 5900 / 10000: loss 1.713391\n",
      "iteration 6000 / 10000: loss 1.719091\n",
      "iteration 6100 / 10000: loss 1.655232\n",
      "iteration 6200 / 10000: loss 1.629539\n",
      "iteration 6300 / 10000: loss 1.762703\n",
      "iteration 6400 / 10000: loss 1.776104\n",
      "iteration 6500 / 10000: loss 1.679438\n",
      "iteration 6600 / 10000: loss 1.715391\n",
      "iteration 6700 / 10000: loss 1.662360\n",
      "iteration 6800 / 10000: loss 1.710851\n",
      "iteration 6900 / 10000: loss 1.750131\n",
      "iteration 7000 / 10000: loss 1.743483\n",
      "iteration 7100 / 10000: loss 1.682071\n",
      "iteration 7200 / 10000: loss 1.751507\n",
      "iteration 7300 / 10000: loss 1.692626\n",
      "iteration 7400 / 10000: loss 1.731429\n",
      "iteration 7500 / 10000: loss 1.652021\n",
      "iteration 7600 / 10000: loss 1.667739\n",
      "iteration 7700 / 10000: loss 1.692172\n",
      "iteration 7800 / 10000: loss 1.737854\n",
      "iteration 7900 / 10000: loss 1.743718\n",
      "iteration 8000 / 10000: loss 1.642078\n",
      "iteration 8100 / 10000: loss 1.713182\n",
      "iteration 8200 / 10000: loss 1.697508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8300 / 10000: loss 1.764881\n",
      "iteration 8400 / 10000: loss 1.710535\n",
      "iteration 8500 / 10000: loss 1.758889\n",
      "iteration 8600 / 10000: loss 1.742544\n",
      "iteration 8700 / 10000: loss 1.583349\n",
      "iteration 8800 / 10000: loss 1.698970\n",
      "iteration 8900 / 10000: loss 1.686594\n",
      "iteration 9000 / 10000: loss 1.655781\n",
      "iteration 9100 / 10000: loss 1.739992\n",
      "iteration 9200 / 10000: loss 1.732181\n",
      "iteration 9300 / 10000: loss 1.660652\n",
      "iteration 9400 / 10000: loss 1.679238\n",
      "iteration 9500 / 10000: loss 1.707372\n",
      "iteration 9600 / 10000: loss 1.707106\n",
      "iteration 9700 / 10000: loss 1.638606\n",
      "iteration 9800 / 10000: loss 1.718009\n",
      "iteration 9900 / 10000: loss 1.690683\n",
      "Validation accuracy:  0.505\n",
      "\n",
      "\n",
      "Starting iteration with rate 5.104251e-01, regstrength 1.151395e-03 and hidden layer size 22\n",
      "iteration 0 / 10000: loss 2.302585\n",
      "iteration 100 / 10000: loss 1.661267\n",
      "iteration 200 / 10000: loss 1.550444\n",
      "iteration 300 / 10000: loss 1.388713\n",
      "iteration 400 / 10000: loss 1.359431\n",
      "iteration 500 / 10000: loss 1.434241\n",
      "iteration 600 / 10000: loss 1.308684\n",
      "iteration 700 / 10000: loss 1.463103\n",
      "iteration 800 / 10000: loss 1.393498\n",
      "iteration 900 / 10000: loss 1.379584\n",
      "iteration 1000 / 10000: loss 1.326299\n",
      "iteration 1100 / 10000: loss 1.393602\n",
      "iteration 1200 / 10000: loss 1.334363\n",
      "iteration 1300 / 10000: loss 1.364264\n",
      "iteration 1400 / 10000: loss 1.371167\n",
      "iteration 1500 / 10000: loss 1.451904\n",
      "iteration 1600 / 10000: loss 1.286701\n",
      "iteration 1700 / 10000: loss 1.237774\n",
      "iteration 1800 / 10000: loss 1.257736\n",
      "iteration 1900 / 10000: loss 1.436692\n",
      "iteration 2000 / 10000: loss 1.380895\n",
      "iteration 2100 / 10000: loss 1.371802\n",
      "iteration 2200 / 10000: loss 1.371572\n",
      "iteration 2300 / 10000: loss 1.342119\n",
      "iteration 2400 / 10000: loss 1.208715\n",
      "iteration 2500 / 10000: loss 1.328574\n",
      "iteration 2600 / 10000: loss 1.230780\n",
      "iteration 2700 / 10000: loss 1.391208\n",
      "iteration 2800 / 10000: loss 1.275941\n",
      "iteration 2900 / 10000: loss 1.320468\n",
      "iteration 3000 / 10000: loss 1.381626\n",
      "iteration 3100 / 10000: loss 1.228174\n",
      "iteration 3200 / 10000: loss 1.300839\n",
      "iteration 3300 / 10000: loss 1.267898\n",
      "iteration 3400 / 10000: loss 1.464165\n",
      "iteration 3500 / 10000: loss 1.327445\n",
      "iteration 3600 / 10000: loss 1.327999\n",
      "iteration 3700 / 10000: loss 1.254003\n",
      "iteration 3800 / 10000: loss 1.318406\n",
      "iteration 3900 / 10000: loss 1.258979\n",
      "iteration 4000 / 10000: loss 1.282458\n",
      "iteration 4100 / 10000: loss 1.310706\n",
      "iteration 4200 / 10000: loss 1.299096\n",
      "iteration 4300 / 10000: loss 1.487489\n",
      "iteration 4400 / 10000: loss 1.293627\n",
      "iteration 4500 / 10000: loss 1.218572\n",
      "iteration 4600 / 10000: loss 1.204256\n",
      "iteration 4700 / 10000: loss 1.208315\n",
      "iteration 4800 / 10000: loss 1.274494\n",
      "iteration 4900 / 10000: loss 1.450826\n",
      "iteration 5000 / 10000: loss 1.291529\n",
      "iteration 5100 / 10000: loss 1.182619\n",
      "iteration 5200 / 10000: loss 1.239062\n",
      "iteration 5300 / 10000: loss 1.448713\n",
      "iteration 5400 / 10000: loss 1.409427\n",
      "iteration 5500 / 10000: loss 1.354898\n",
      "iteration 5600 / 10000: loss 1.156781\n",
      "iteration 5700 / 10000: loss 1.367965\n",
      "iteration 5800 / 10000: loss 1.319040\n",
      "iteration 5900 / 10000: loss 1.293848\n",
      "iteration 6000 / 10000: loss 1.160286\n",
      "iteration 6100 / 10000: loss 1.433683\n",
      "iteration 6200 / 10000: loss 1.374762\n",
      "iteration 6300 / 10000: loss 1.253188\n",
      "iteration 6400 / 10000: loss 1.318027\n",
      "iteration 6500 / 10000: loss 1.371761\n",
      "iteration 6600 / 10000: loss 1.396286\n",
      "iteration 6700 / 10000: loss 1.191623\n",
      "iteration 6800 / 10000: loss 1.279180\n",
      "iteration 6900 / 10000: loss 1.371702\n",
      "iteration 7000 / 10000: loss 1.405858\n",
      "iteration 7100 / 10000: loss 1.200047\n",
      "iteration 7200 / 10000: loss 1.324275\n",
      "iteration 7300 / 10000: loss 1.298987\n",
      "iteration 7400 / 10000: loss 1.365924\n",
      "iteration 7500 / 10000: loss 1.280436\n",
      "iteration 7600 / 10000: loss 1.215433\n",
      "iteration 7700 / 10000: loss 1.193320\n",
      "iteration 7800 / 10000: loss 1.291594\n",
      "iteration 7900 / 10000: loss 1.368709\n",
      "iteration 8000 / 10000: loss 1.194257\n",
      "iteration 8100 / 10000: loss 1.317440\n",
      "iteration 8200 / 10000: loss 1.151534\n",
      "iteration 8300 / 10000: loss 1.315027\n",
      "iteration 8400 / 10000: loss 1.444374\n",
      "iteration 8500 / 10000: loss 1.317541\n",
      "iteration 8600 / 10000: loss 1.242586\n",
      "iteration 8700 / 10000: loss 1.271428\n",
      "iteration 8800 / 10000: loss 1.172663\n",
      "iteration 8900 / 10000: loss 1.349491\n",
      "iteration 9000 / 10000: loss 1.331040\n",
      "iteration 9100 / 10000: loss 1.220508\n",
      "iteration 9200 / 10000: loss 1.159394\n",
      "iteration 9300 / 10000: loss 1.396744\n",
      "iteration 9400 / 10000: loss 1.313736\n",
      "iteration 9500 / 10000: loss 1.235466\n",
      "iteration 9600 / 10000: loss 1.243720\n",
      "iteration 9700 / 10000: loss 1.302790\n",
      "iteration 9800 / 10000: loss 1.115700\n",
      "iteration 9900 / 10000: loss 1.339465\n",
      "Validation accuracy:  0.538\n",
      "\n",
      "\n",
      "Starting iteration with rate 4.516778e-01, regstrength 2.559548e-02 and hidden layer size 17\n",
      "iteration 0 / 10000: loss 2.302586\n",
      "iteration 100 / 10000: loss 1.973791\n",
      "iteration 200 / 10000: loss 1.917549\n",
      "iteration 300 / 10000: loss 1.837068\n",
      "iteration 400 / 10000: loss 1.864862\n",
      "iteration 500 / 10000: loss 1.859476\n",
      "iteration 600 / 10000: loss 1.946202\n",
      "iteration 700 / 10000: loss 1.907694\n",
      "iteration 800 / 10000: loss 1.855262\n",
      "iteration 900 / 10000: loss 1.770381\n",
      "iteration 1000 / 10000: loss 1.826434\n",
      "iteration 1100 / 10000: loss 1.872587\n",
      "iteration 1200 / 10000: loss 1.884391\n",
      "iteration 1300 / 10000: loss 1.886452\n",
      "iteration 1400 / 10000: loss 1.845624\n",
      "iteration 1500 / 10000: loss 1.753107\n",
      "iteration 1600 / 10000: loss 1.879382\n",
      "iteration 1700 / 10000: loss 1.944021\n",
      "iteration 1800 / 10000: loss 1.854326\n",
      "iteration 1900 / 10000: loss 1.830380\n",
      "iteration 2000 / 10000: loss 1.827087\n",
      "iteration 2100 / 10000: loss 1.790735\n",
      "iteration 2200 / 10000: loss 1.825394\n",
      "iteration 2300 / 10000: loss 1.950103\n",
      "iteration 2400 / 10000: loss 1.793727\n",
      "iteration 2500 / 10000: loss 1.913079\n",
      "iteration 2600 / 10000: loss 1.874693\n",
      "iteration 2700 / 10000: loss 1.930401\n",
      "iteration 2800 / 10000: loss 1.856032\n",
      "iteration 2900 / 10000: loss 1.755899\n",
      "iteration 3000 / 10000: loss 1.942233\n",
      "iteration 3100 / 10000: loss 1.811297\n",
      "iteration 3200 / 10000: loss 1.731412\n",
      "iteration 3300 / 10000: loss 1.928301\n",
      "iteration 3400 / 10000: loss 1.844179\n",
      "iteration 3500 / 10000: loss 1.859151\n",
      "iteration 3600 / 10000: loss 1.909651\n",
      "iteration 3700 / 10000: loss 1.882456\n",
      "iteration 3800 / 10000: loss 1.851777\n",
      "iteration 3900 / 10000: loss 1.893668\n",
      "iteration 4000 / 10000: loss 1.785559\n",
      "iteration 4100 / 10000: loss 1.945336\n",
      "iteration 4200 / 10000: loss 1.846151\n",
      "iteration 4300 / 10000: loss 1.894667\n",
      "iteration 4400 / 10000: loss 1.856111\n",
      "iteration 4500 / 10000: loss 1.814938\n",
      "iteration 4600 / 10000: loss 1.987225\n",
      "iteration 4700 / 10000: loss 1.909577\n",
      "iteration 4800 / 10000: loss 1.924388\n",
      "iteration 4900 / 10000: loss 1.849429\n",
      "iteration 5000 / 10000: loss 1.793136\n",
      "iteration 5100 / 10000: loss 1.710837\n",
      "iteration 5200 / 10000: loss 1.918441\n",
      "iteration 5300 / 10000: loss 1.792215\n",
      "iteration 5400 / 10000: loss 1.828848\n",
      "iteration 5500 / 10000: loss 1.781354\n",
      "iteration 5600 / 10000: loss 1.762690\n",
      "iteration 5700 / 10000: loss 1.822073\n",
      "iteration 5800 / 10000: loss 1.814481\n",
      "iteration 5900 / 10000: loss 1.861298\n",
      "iteration 6000 / 10000: loss 1.834169\n",
      "iteration 6100 / 10000: loss 1.964883\n",
      "iteration 6200 / 10000: loss 1.864059\n",
      "iteration 6300 / 10000: loss 1.732162\n",
      "iteration 6400 / 10000: loss 1.808745\n",
      "iteration 6500 / 10000: loss 1.839540\n",
      "iteration 6600 / 10000: loss 1.870161\n",
      "iteration 6700 / 10000: loss 1.814756\n",
      "iteration 6800 / 10000: loss 1.915063\n",
      "iteration 6900 / 10000: loss 1.907372\n",
      "iteration 7000 / 10000: loss 1.785521\n",
      "iteration 7100 / 10000: loss 1.905342\n",
      "iteration 7200 / 10000: loss 1.760401\n",
      "iteration 7300 / 10000: loss 1.811114\n",
      "iteration 7400 / 10000: loss 1.791132\n",
      "iteration 7500 / 10000: loss 1.892742\n",
      "iteration 7600 / 10000: loss 1.834532\n",
      "iteration 7700 / 10000: loss 1.830583\n",
      "iteration 7800 / 10000: loss 1.721724\n",
      "iteration 7900 / 10000: loss 1.825269\n",
      "iteration 8000 / 10000: loss 1.815422\n",
      "iteration 8100 / 10000: loss 1.869708\n",
      "iteration 8200 / 10000: loss 1.767366\n",
      "iteration 8300 / 10000: loss 1.863717\n",
      "iteration 8400 / 10000: loss 1.815845\n",
      "iteration 8500 / 10000: loss 1.780669\n",
      "iteration 8600 / 10000: loss 1.836505\n",
      "iteration 8700 / 10000: loss 1.892816\n",
      "iteration 8800 / 10000: loss 1.796728\n",
      "iteration 8900 / 10000: loss 1.770752\n",
      "iteration 9000 / 10000: loss 1.777059\n",
      "iteration 9100 / 10000: loss 1.831114\n",
      "iteration 9200 / 10000: loss 1.849413\n",
      "iteration 9300 / 10000: loss 1.750366\n",
      "iteration 9400 / 10000: loss 1.885590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9500 / 10000: loss 1.763235\n",
      "iteration 9600 / 10000: loss 1.834600\n",
      "iteration 9700 / 10000: loss 1.776848\n",
      "iteration 9800 / 10000: loss 1.733148\n",
      "iteration 9900 / 10000: loss 1.787514\n",
      "Validation accuracy:  0.481\n",
      "\n",
      "\n",
      "Starting iteration with rate 9.795756e-02, regstrength 2.329952e-03 and hidden layer size 39\n",
      "iteration 0 / 10000: loss 2.302585\n",
      "iteration 100 / 10000: loss 2.302289\n",
      "iteration 200 / 10000: loss 2.275506\n",
      "iteration 300 / 10000: loss 1.936253\n",
      "iteration 400 / 10000: loss 1.845921\n",
      "iteration 500 / 10000: loss 1.593510\n",
      "iteration 600 / 10000: loss 1.495938\n",
      "iteration 700 / 10000: loss 1.490173\n",
      "iteration 800 / 10000: loss 1.446810\n",
      "iteration 900 / 10000: loss 1.469488\n",
      "iteration 1000 / 10000: loss 1.433285\n",
      "iteration 1100 / 10000: loss 1.495090\n",
      "iteration 1200 / 10000: loss 1.493555\n",
      "iteration 1300 / 10000: loss 1.396490\n",
      "iteration 1400 / 10000: loss 1.386925\n",
      "iteration 1500 / 10000: loss 1.356871\n",
      "iteration 1600 / 10000: loss 1.576909\n",
      "iteration 1700 / 10000: loss 1.505768\n",
      "iteration 1800 / 10000: loss 1.461508\n",
      "iteration 1900 / 10000: loss 1.370789\n",
      "iteration 2000 / 10000: loss 1.427642\n",
      "iteration 2100 / 10000: loss 1.296773\n",
      "iteration 2200 / 10000: loss 1.284643\n",
      "iteration 2300 / 10000: loss 1.482906\n",
      "iteration 2400 / 10000: loss 1.455835\n",
      "iteration 2500 / 10000: loss 1.333754\n",
      "iteration 2600 / 10000: loss 1.379659\n",
      "iteration 2700 / 10000: loss 1.371290\n",
      "iteration 2800 / 10000: loss 1.335644\n",
      "iteration 2900 / 10000: loss 1.248652\n",
      "iteration 3000 / 10000: loss 1.391382\n",
      "iteration 3100 / 10000: loss 1.360958\n",
      "iteration 3200 / 10000: loss 1.348578\n",
      "iteration 3300 / 10000: loss 1.356775\n",
      "iteration 3400 / 10000: loss 1.332621\n",
      "iteration 3500 / 10000: loss 1.410871\n",
      "iteration 3600 / 10000: loss 1.338828\n",
      "iteration 3700 / 10000: loss 1.350508\n",
      "iteration 3800 / 10000: loss 1.407296\n",
      "iteration 3900 / 10000: loss 1.324558\n",
      "iteration 4000 / 10000: loss 1.203786\n",
      "iteration 4100 / 10000: loss 1.356723\n",
      "iteration 4200 / 10000: loss 1.302019\n",
      "iteration 4300 / 10000: loss 1.302994\n",
      "iteration 4400 / 10000: loss 1.317013\n",
      "iteration 4500 / 10000: loss 1.314661\n",
      "iteration 4600 / 10000: loss 1.217594\n",
      "iteration 4700 / 10000: loss 1.380193\n",
      "iteration 4800 / 10000: loss 1.220721\n",
      "iteration 4900 / 10000: loss 1.292358\n",
      "iteration 5000 / 10000: loss 1.329270\n",
      "iteration 5100 / 10000: loss 1.338650\n",
      "iteration 5200 / 10000: loss 1.264104\n",
      "iteration 5300 / 10000: loss 1.275492\n",
      "iteration 5400 / 10000: loss 1.339667\n",
      "iteration 5500 / 10000: loss 1.312027\n",
      "iteration 5600 / 10000: loss 1.288958\n",
      "iteration 5700 / 10000: loss 1.266390\n",
      "iteration 5800 / 10000: loss 1.220987\n",
      "iteration 5900 / 10000: loss 1.301853\n",
      "iteration 6000 / 10000: loss 1.267790\n",
      "iteration 6100 / 10000: loss 1.265796\n",
      "iteration 6200 / 10000: loss 1.282499\n",
      "iteration 6300 / 10000: loss 1.354810\n",
      "iteration 6400 / 10000: loss 1.188512\n",
      "iteration 6500 / 10000: loss 1.301109\n",
      "iteration 6600 / 10000: loss 1.290231\n",
      "iteration 6700 / 10000: loss 1.165195\n",
      "iteration 6800 / 10000: loss 1.387445\n",
      "iteration 6900 / 10000: loss 1.344717\n",
      "iteration 7000 / 10000: loss 1.388738\n",
      "iteration 7100 / 10000: loss 1.224203\n",
      "iteration 7200 / 10000: loss 1.266895\n",
      "iteration 7300 / 10000: loss 1.424889\n",
      "iteration 7400 / 10000: loss 1.343007\n",
      "iteration 7500 / 10000: loss 1.254779\n",
      "iteration 7600 / 10000: loss 1.318320\n",
      "iteration 7700 / 10000: loss 1.348913\n",
      "iteration 7800 / 10000: loss 1.282538\n",
      "iteration 7900 / 10000: loss 1.303159\n",
      "iteration 8000 / 10000: loss 1.299591\n",
      "iteration 8100 / 10000: loss 1.516250\n",
      "iteration 8200 / 10000: loss 1.394717\n",
      "iteration 8300 / 10000: loss 1.259731\n",
      "iteration 8400 / 10000: loss 1.398551\n",
      "iteration 8500 / 10000: loss 1.222023\n",
      "iteration 8600 / 10000: loss 1.334330\n",
      "iteration 8700 / 10000: loss 1.288542\n",
      "iteration 8800 / 10000: loss 1.394841\n",
      "iteration 8900 / 10000: loss 1.322110\n",
      "iteration 9000 / 10000: loss 1.326742\n",
      "iteration 9100 / 10000: loss 1.167780\n",
      "iteration 9200 / 10000: loss 1.452517\n",
      "iteration 9300 / 10000: loss 1.362071\n",
      "iteration 9400 / 10000: loss 1.274546\n",
      "iteration 9500 / 10000: loss 1.295518\n",
      "iteration 9600 / 10000: loss 1.225448\n",
      "iteration 9700 / 10000: loss 1.320848\n",
      "iteration 9800 / 10000: loss 1.328072\n",
      "iteration 9900 / 10000: loss 1.355349\n",
      "Validation accuracy:  0.562\n",
      "\n",
      "\n",
      "Starting iteration with rate 5.000000e-02, regstrength 1.456348e-02 and hidden layer size 33\n",
      "iteration 0 / 10000: loss 2.302586\n",
      "iteration 100 / 10000: loss 2.302639\n",
      "iteration 200 / 10000: loss 2.303591\n",
      "iteration 300 / 10000: loss 2.302414\n",
      "iteration 400 / 10000: loss 2.292102\n",
      "iteration 500 / 10000: loss 2.202978\n",
      "iteration 600 / 10000: loss 2.033244\n",
      "iteration 700 / 10000: loss 2.030406\n",
      "iteration 800 / 10000: loss 1.803603\n",
      "iteration 900 / 10000: loss 1.857771\n",
      "iteration 1000 / 10000: loss 1.841381\n",
      "iteration 1100 / 10000: loss 1.733825\n",
      "iteration 1200 / 10000: loss 1.745164\n",
      "iteration 1300 / 10000: loss 1.698092\n",
      "iteration 1400 / 10000: loss 1.730632\n",
      "iteration 1500 / 10000: loss 1.694755\n",
      "iteration 1600 / 10000: loss 1.668125\n",
      "iteration 1700 / 10000: loss 1.619327\n",
      "iteration 1800 / 10000: loss 1.619488\n",
      "iteration 1900 / 10000: loss 1.640838\n",
      "iteration 2000 / 10000: loss 1.729731\n",
      "iteration 2100 / 10000: loss 1.662454\n",
      "iteration 2200 / 10000: loss 1.755685\n",
      "iteration 2300 / 10000: loss 1.692215\n",
      "iteration 2400 / 10000: loss 1.609029\n",
      "iteration 2500 / 10000: loss 1.679278\n",
      "iteration 2600 / 10000: loss 1.626796\n",
      "iteration 2700 / 10000: loss 1.690847\n",
      "iteration 2800 / 10000: loss 1.614913\n",
      "iteration 2900 / 10000: loss 1.708684\n",
      "iteration 3000 / 10000: loss 1.673815\n",
      "iteration 3100 / 10000: loss 1.708397\n",
      "iteration 3200 / 10000: loss 1.599799\n",
      "iteration 3300 / 10000: loss 1.602906\n",
      "iteration 3400 / 10000: loss 1.645652\n",
      "iteration 3500 / 10000: loss 1.600038\n",
      "iteration 3600 / 10000: loss 1.603875\n",
      "iteration 3700 / 10000: loss 1.678465\n",
      "iteration 3800 / 10000: loss 1.675121\n",
      "iteration 3900 / 10000: loss 1.758501\n",
      "iteration 4000 / 10000: loss 1.605706\n",
      "iteration 4100 / 10000: loss 1.618877\n",
      "iteration 4200 / 10000: loss 1.665684\n",
      "iteration 4300 / 10000: loss 1.652756\n",
      "iteration 4400 / 10000: loss 1.635781\n",
      "iteration 4500 / 10000: loss 1.727544\n",
      "iteration 4600 / 10000: loss 1.661837\n",
      "iteration 4700 / 10000: loss 1.700980\n",
      "iteration 4800 / 10000: loss 1.666678\n",
      "iteration 4900 / 10000: loss 1.576090\n",
      "iteration 5000 / 10000: loss 1.615515\n",
      "iteration 5100 / 10000: loss 1.564160\n",
      "iteration 5200 / 10000: loss 1.715355\n",
      "iteration 5300 / 10000: loss 1.675377\n",
      "iteration 5400 / 10000: loss 1.732360\n",
      "iteration 5500 / 10000: loss 1.757843\n",
      "iteration 5600 / 10000: loss 1.698717\n",
      "iteration 5700 / 10000: loss 1.551863\n",
      "iteration 5800 / 10000: loss 1.566360\n",
      "iteration 5900 / 10000: loss 1.629915\n",
      "iteration 6000 / 10000: loss 1.711105\n",
      "iteration 6100 / 10000: loss 1.711927\n",
      "iteration 6200 / 10000: loss 1.649163\n",
      "iteration 6300 / 10000: loss 1.761103\n",
      "iteration 6400 / 10000: loss 1.649460\n",
      "iteration 6500 / 10000: loss 1.588776\n",
      "iteration 6600 / 10000: loss 1.667517\n",
      "iteration 6700 / 10000: loss 1.757106\n",
      "iteration 6800 / 10000: loss 1.681730\n",
      "iteration 6900 / 10000: loss 1.553301\n",
      "iteration 7000 / 10000: loss 1.614223\n",
      "iteration 7100 / 10000: loss 1.667149\n",
      "iteration 7200 / 10000: loss 1.653931\n",
      "iteration 7300 / 10000: loss 1.691455\n",
      "iteration 7400 / 10000: loss 1.666863\n",
      "iteration 7500 / 10000: loss 1.648502\n",
      "iteration 7600 / 10000: loss 1.568402\n",
      "iteration 7700 / 10000: loss 1.585686\n",
      "iteration 7800 / 10000: loss 1.560193\n",
      "iteration 7900 / 10000: loss 1.636322\n",
      "iteration 8000 / 10000: loss 1.697122\n",
      "iteration 8100 / 10000: loss 1.634967\n",
      "iteration 8200 / 10000: loss 1.621941\n",
      "iteration 8300 / 10000: loss 1.640143\n",
      "iteration 8400 / 10000: loss 1.607425\n",
      "iteration 8500 / 10000: loss 1.718905\n",
      "iteration 8600 / 10000: loss 1.564006\n",
      "iteration 8700 / 10000: loss 1.736800\n",
      "iteration 8800 / 10000: loss 1.708294\n",
      "iteration 8900 / 10000: loss 1.606034\n",
      "iteration 9000 / 10000: loss 1.602415\n",
      "iteration 9100 / 10000: loss 1.659731\n",
      "iteration 9200 / 10000: loss 1.593767\n",
      "iteration 9300 / 10000: loss 1.646436\n",
      "iteration 9400 / 10000: loss 1.622231\n",
      "iteration 9500 / 10000: loss 1.626136\n",
      "iteration 9600 / 10000: loss 1.620038\n",
      "iteration 9700 / 10000: loss 1.643873\n",
      "iteration 9800 / 10000: loss 1.602394\n",
      "iteration 9900 / 10000: loss 1.671613\n",
      "Validation accuracy:  0.509\n",
      "\n",
      "\n",
      "Starting iteration with rate 9.406940e-01, regstrength 2.222996e-02 and hidden layer size 24\n",
      "iteration 0 / 10000: loss 2.302586\n",
      "iteration 100 / 10000: loss 1.835856\n",
      "iteration 200 / 10000: loss 1.868984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 300 / 10000: loss 1.808169\n",
      "iteration 400 / 10000: loss 1.979468\n",
      "iteration 500 / 10000: loss 1.864808\n",
      "iteration 600 / 10000: loss 1.844334\n",
      "iteration 700 / 10000: loss 1.835288\n",
      "iteration 800 / 10000: loss 1.827542\n",
      "iteration 900 / 10000: loss 1.951279\n",
      "iteration 1000 / 10000: loss 1.861685\n",
      "iteration 1100 / 10000: loss 1.835694\n",
      "iteration 1200 / 10000: loss 1.791740\n",
      "iteration 1300 / 10000: loss 1.855561\n",
      "iteration 1400 / 10000: loss 1.856436\n",
      "iteration 1500 / 10000: loss 1.877985\n",
      "iteration 1600 / 10000: loss 1.847234\n",
      "iteration 1700 / 10000: loss 1.775610\n",
      "iteration 1800 / 10000: loss 1.753681\n",
      "iteration 1900 / 10000: loss 1.843674\n",
      "iteration 2000 / 10000: loss 1.787736\n",
      "iteration 2100 / 10000: loss 1.955475\n",
      "iteration 2200 / 10000: loss 1.812964\n",
      "iteration 2300 / 10000: loss 1.811662\n",
      "iteration 2400 / 10000: loss 1.825410\n",
      "iteration 2500 / 10000: loss 1.814749\n",
      "iteration 2600 / 10000: loss 1.889441\n",
      "iteration 2700 / 10000: loss 1.837066\n",
      "iteration 2800 / 10000: loss 1.871447\n",
      "iteration 2900 / 10000: loss 1.852853\n",
      "iteration 3000 / 10000: loss 1.868381\n",
      "iteration 3100 / 10000: loss 1.844508\n",
      "iteration 3200 / 10000: loss 1.750087\n",
      "iteration 3300 / 10000: loss 1.777958\n",
      "iteration 3400 / 10000: loss 1.862116\n",
      "iteration 3500 / 10000: loss 1.921546\n",
      "iteration 3600 / 10000: loss 1.871042\n",
      "iteration 3700 / 10000: loss 1.759282\n",
      "iteration 3800 / 10000: loss 1.858498\n",
      "iteration 3900 / 10000: loss 1.894670\n",
      "iteration 4000 / 10000: loss 1.882239\n",
      "iteration 4100 / 10000: loss 1.889543\n",
      "iteration 4200 / 10000: loss 1.888706\n",
      "iteration 4300 / 10000: loss 1.749224\n",
      "iteration 4400 / 10000: loss 1.785900\n",
      "iteration 4500 / 10000: loss 1.813864\n",
      "iteration 4600 / 10000: loss 1.831789\n",
      "iteration 4700 / 10000: loss 1.786272\n",
      "iteration 4800 / 10000: loss 1.776427\n",
      "iteration 4900 / 10000: loss 1.980500\n",
      "iteration 5000 / 10000: loss 1.697711\n",
      "iteration 5100 / 10000: loss 1.861202\n",
      "iteration 5200 / 10000: loss 1.804919\n",
      "iteration 5300 / 10000: loss 1.729175\n",
      "iteration 5400 / 10000: loss 1.826125\n",
      "iteration 5500 / 10000: loss 1.891104\n",
      "iteration 5600 / 10000: loss 1.804890\n",
      "iteration 5700 / 10000: loss 1.773413\n",
      "iteration 5800 / 10000: loss 1.856086\n",
      "iteration 5900 / 10000: loss 1.803882\n",
      "iteration 6000 / 10000: loss 1.924214\n",
      "iteration 6100 / 10000: loss 1.766657\n",
      "iteration 6200 / 10000: loss 1.842640\n",
      "iteration 6300 / 10000: loss 1.850680\n",
      "iteration 6400 / 10000: loss 1.834840\n",
      "iteration 6500 / 10000: loss 1.799746\n",
      "iteration 6600 / 10000: loss 1.763174\n",
      "iteration 6700 / 10000: loss 1.862134\n",
      "iteration 6800 / 10000: loss 1.829335\n",
      "iteration 6900 / 10000: loss 1.840846\n",
      "iteration 7000 / 10000: loss 1.808518\n",
      "iteration 7100 / 10000: loss 1.771541\n",
      "iteration 7200 / 10000: loss 1.860756\n",
      "iteration 7300 / 10000: loss 1.732591\n",
      "iteration 7400 / 10000: loss 1.877725\n",
      "iteration 7500 / 10000: loss 1.767824\n",
      "iteration 7600 / 10000: loss 1.759601\n",
      "iteration 7700 / 10000: loss 1.770205\n",
      "iteration 7800 / 10000: loss 1.863310\n",
      "iteration 7900 / 10000: loss 1.797607\n",
      "iteration 8000 / 10000: loss 1.778373\n",
      "iteration 8100 / 10000: loss 1.784966\n",
      "iteration 8200 / 10000: loss 1.822327\n",
      "iteration 8300 / 10000: loss 1.776536\n",
      "iteration 8400 / 10000: loss 1.811573\n",
      "iteration 8500 / 10000: loss 1.770017\n",
      "iteration 8600 / 10000: loss 1.780656\n",
      "iteration 8700 / 10000: loss 1.810849\n",
      "iteration 8800 / 10000: loss 1.852262\n",
      "iteration 8900 / 10000: loss 1.831898\n",
      "iteration 9000 / 10000: loss 1.788231\n",
      "iteration 9100 / 10000: loss 1.819126\n",
      "iteration 9200 / 10000: loss 1.811950\n",
      "iteration 9300 / 10000: loss 1.844993\n",
      "iteration 9400 / 10000: loss 1.809525\n",
      "iteration 9500 / 10000: loss 1.811248\n",
      "iteration 9600 / 10000: loss 1.662935\n",
      "iteration 9700 / 10000: loss 1.787464\n",
      "iteration 9800 / 10000: loss 1.868525\n",
      "iteration 9900 / 10000: loss 1.816844\n",
      "Validation accuracy:  0.503\n",
      "\n",
      "\n",
      "Starting iteration with rate 8.324250e-01, regstrength 1.599859e-01 and hidden layer size 16\n",
      "iteration 0 / 10000: loss 2.302589\n",
      "iteration 100 / 10000: loss 2.303695\n",
      "iteration 200 / 10000: loss 2.303743\n",
      "iteration 300 / 10000: loss 2.314738\n",
      "iteration 400 / 10000: loss 2.307160\n",
      "iteration 500 / 10000: loss 2.318790\n",
      "iteration 600 / 10000: loss 2.288603\n",
      "iteration 700 / 10000: loss 2.302173\n",
      "iteration 800 / 10000: loss 2.315103\n",
      "iteration 900 / 10000: loss 2.294379\n",
      "iteration 1000 / 10000: loss 2.294170\n",
      "iteration 1100 / 10000: loss 2.295692\n",
      "iteration 1200 / 10000: loss 2.302033\n",
      "iteration 1300 / 10000: loss 2.308813\n",
      "iteration 1400 / 10000: loss 2.300858\n",
      "iteration 1500 / 10000: loss 2.299622\n",
      "iteration 1600 / 10000: loss 2.302826\n",
      "iteration 1700 / 10000: loss 2.308249\n",
      "iteration 1800 / 10000: loss 2.292729\n",
      "iteration 1900 / 10000: loss 2.301627\n",
      "iteration 2000 / 10000: loss 2.293983\n",
      "iteration 2100 / 10000: loss 2.299802\n",
      "iteration 2200 / 10000: loss 2.286620\n",
      "iteration 2300 / 10000: loss 2.308636\n",
      "iteration 2400 / 10000: loss 2.308381\n",
      "iteration 2500 / 10000: loss 2.294951\n",
      "iteration 2600 / 10000: loss 2.311553\n",
      "iteration 2700 / 10000: loss 2.280212\n",
      "iteration 2800 / 10000: loss 2.306168\n",
      "iteration 2900 / 10000: loss 2.298025\n",
      "iteration 3000 / 10000: loss 2.292402\n",
      "iteration 3100 / 10000: loss 2.295229\n",
      "iteration 3200 / 10000: loss 2.317073\n",
      "iteration 3300 / 10000: loss 2.293614\n",
      "iteration 3400 / 10000: loss 2.307038\n",
      "iteration 3500 / 10000: loss 2.290381\n",
      "iteration 3600 / 10000: loss 2.298939\n",
      "iteration 3700 / 10000: loss 2.287936\n",
      "iteration 3800 / 10000: loss 2.310516\n",
      "iteration 3900 / 10000: loss 2.291180\n",
      "iteration 4000 / 10000: loss 2.287669\n",
      "iteration 4100 / 10000: loss 2.304263\n",
      "iteration 4200 / 10000: loss 2.309514\n",
      "iteration 4300 / 10000: loss 2.294134\n",
      "iteration 4400 / 10000: loss 2.310386\n",
      "iteration 4500 / 10000: loss 2.298111\n",
      "iteration 4600 / 10000: loss 2.300843\n",
      "iteration 4700 / 10000: loss 2.303669\n",
      "iteration 4800 / 10000: loss 2.330528\n",
      "iteration 4900 / 10000: loss 2.295753\n",
      "iteration 5000 / 10000: loss 2.308898\n",
      "iteration 5100 / 10000: loss 2.297634\n",
      "iteration 5200 / 10000: loss 2.309306\n",
      "iteration 5300 / 10000: loss 2.299786\n",
      "iteration 5400 / 10000: loss 2.288075\n",
      "iteration 5500 / 10000: loss 2.307357\n",
      "iteration 5600 / 10000: loss 2.309323\n",
      "iteration 5700 / 10000: loss 2.313445\n",
      "iteration 5800 / 10000: loss 2.296350\n",
      "iteration 5900 / 10000: loss 2.297000\n",
      "iteration 6000 / 10000: loss 2.301256\n",
      "iteration 6100 / 10000: loss 2.294102\n",
      "iteration 6200 / 10000: loss 2.300676\n",
      "iteration 6300 / 10000: loss 2.289179\n",
      "iteration 6400 / 10000: loss 2.299383\n",
      "iteration 6500 / 10000: loss 2.297436\n",
      "iteration 6600 / 10000: loss 2.305212\n",
      "iteration 6700 / 10000: loss 2.298341\n",
      "iteration 6800 / 10000: loss 2.300364\n",
      "iteration 6900 / 10000: loss 2.305384\n",
      "iteration 7000 / 10000: loss 2.290341\n",
      "iteration 7100 / 10000: loss 2.309100\n",
      "iteration 7200 / 10000: loss 2.304722\n",
      "iteration 7300 / 10000: loss 2.303850\n",
      "iteration 7400 / 10000: loss 2.295338\n",
      "iteration 7500 / 10000: loss 2.297156\n",
      "iteration 7600 / 10000: loss 2.293901\n",
      "iteration 7700 / 10000: loss 2.289889\n",
      "iteration 7800 / 10000: loss 2.300082\n",
      "iteration 7900 / 10000: loss 2.292374\n",
      "iteration 8000 / 10000: loss 2.298995\n",
      "iteration 8100 / 10000: loss 2.294038\n",
      "iteration 8200 / 10000: loss 2.302742\n",
      "iteration 8300 / 10000: loss 2.301765\n",
      "iteration 8400 / 10000: loss 2.304185\n",
      "iteration 8500 / 10000: loss 2.290657\n",
      "iteration 8600 / 10000: loss 2.300020\n",
      "iteration 8700 / 10000: loss 2.298328\n",
      "iteration 8800 / 10000: loss 2.313439\n",
      "iteration 8900 / 10000: loss 2.289898\n",
      "iteration 9000 / 10000: loss 2.321606\n",
      "iteration 9100 / 10000: loss 2.301659\n",
      "iteration 9200 / 10000: loss 2.300791\n",
      "iteration 9300 / 10000: loss 2.282830\n",
      "iteration 9400 / 10000: loss 2.297200\n",
      "iteration 9500 / 10000: loss 2.320194\n",
      "iteration 9600 / 10000: loss 2.295667\n",
      "iteration 9700 / 10000: loss 2.307132\n",
      "iteration 9800 / 10000: loss 2.290786\n",
      "iteration 9900 / 10000: loss 2.308356\n",
      "Validation accuracy:  0.183\n",
      "\n",
      "\n",
      "Starting iteration with rate 6.006547e-02, regstrength 1.389495e-01 and hidden layer size 26\n",
      "iteration 0 / 10000: loss 2.302591\n",
      "iteration 100 / 10000: loss 2.302222\n",
      "iteration 200 / 10000: loss 2.303155\n",
      "iteration 300 / 10000: loss 2.302761\n",
      "iteration 400 / 10000: loss 2.302303\n",
      "iteration 500 / 10000: loss 2.303234\n",
      "iteration 600 / 10000: loss 2.302060\n",
      "iteration 700 / 10000: loss 2.303327\n",
      "iteration 800 / 10000: loss 2.302820\n",
      "iteration 900 / 10000: loss 2.302192\n",
      "iteration 1000 / 10000: loss 2.301958\n",
      "iteration 1100 / 10000: loss 2.300580\n",
      "iteration 1200 / 10000: loss 2.296019\n",
      "iteration 1300 / 10000: loss 2.294944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1400 / 10000: loss 2.277948\n",
      "iteration 1500 / 10000: loss 2.299423\n",
      "iteration 1600 / 10000: loss 2.286211\n",
      "iteration 1700 / 10000: loss 2.266375\n",
      "iteration 1800 / 10000: loss 2.271405\n",
      "iteration 1900 / 10000: loss 2.293244\n",
      "iteration 2000 / 10000: loss 2.285206\n",
      "iteration 2100 / 10000: loss 2.294688\n",
      "iteration 2200 / 10000: loss 2.297235\n",
      "iteration 2300 / 10000: loss 2.297223\n",
      "iteration 2400 / 10000: loss 2.296894\n",
      "iteration 2500 / 10000: loss 2.278483\n",
      "iteration 2600 / 10000: loss 2.311107\n",
      "iteration 2700 / 10000: loss 2.283915\n",
      "iteration 2800 / 10000: loss 2.298911\n",
      "iteration 2900 / 10000: loss 2.301065\n",
      "iteration 3000 / 10000: loss 2.279020\n",
      "iteration 3100 / 10000: loss 2.283301\n",
      "iteration 3200 / 10000: loss 2.297140\n",
      "iteration 3300 / 10000: loss 2.281192\n",
      "iteration 3400 / 10000: loss 2.285699\n",
      "iteration 3500 / 10000: loss 2.289405\n",
      "iteration 3600 / 10000: loss 2.282517\n",
      "iteration 3700 / 10000: loss 2.286056\n",
      "iteration 3800 / 10000: loss 2.298857\n",
      "iteration 3900 / 10000: loss 2.285877\n",
      "iteration 4000 / 10000: loss 2.251380\n",
      "iteration 4100 / 10000: loss 2.290115\n",
      "iteration 4200 / 10000: loss 2.296161\n",
      "iteration 4300 / 10000: loss 2.262260\n",
      "iteration 4400 / 10000: loss 2.293455\n",
      "iteration 4500 / 10000: loss 2.268259\n",
      "iteration 4600 / 10000: loss 2.285620\n",
      "iteration 4700 / 10000: loss 2.288021\n",
      "iteration 4800 / 10000: loss 2.289988\n",
      "iteration 4900 / 10000: loss 2.302050\n",
      "iteration 5000 / 10000: loss 2.294041\n",
      "iteration 5100 / 10000: loss 2.292225\n",
      "iteration 5200 / 10000: loss 2.278349\n",
      "iteration 5300 / 10000: loss 2.311915\n",
      "iteration 5400 / 10000: loss 2.300412\n",
      "iteration 5500 / 10000: loss 2.286046\n",
      "iteration 5600 / 10000: loss 2.281432\n",
      "iteration 5700 / 10000: loss 2.281012\n",
      "iteration 5800 / 10000: loss 2.271286\n",
      "iteration 5900 / 10000: loss 2.284832\n",
      "iteration 6000 / 10000: loss 2.261363\n",
      "iteration 6100 / 10000: loss 2.273141\n",
      "iteration 6200 / 10000: loss 2.276065\n",
      "iteration 6300 / 10000: loss 2.293354\n",
      "iteration 6400 / 10000: loss 2.280730\n",
      "iteration 6500 / 10000: loss 2.279078\n",
      "iteration 6600 / 10000: loss 2.290228\n",
      "iteration 6700 / 10000: loss 2.297721\n",
      "iteration 6800 / 10000: loss 2.291927\n",
      "iteration 6900 / 10000: loss 2.272909\n",
      "iteration 7000 / 10000: loss 2.285950\n",
      "iteration 7100 / 10000: loss 2.286992\n",
      "iteration 7200 / 10000: loss 2.289250\n",
      "iteration 7300 / 10000: loss 2.270146\n",
      "iteration 7400 / 10000: loss 2.286803\n",
      "iteration 7500 / 10000: loss 2.275445\n",
      "iteration 7600 / 10000: loss 2.296677\n",
      "iteration 7700 / 10000: loss 2.313219\n",
      "iteration 7800 / 10000: loss 2.274385\n",
      "iteration 7900 / 10000: loss 2.273490\n",
      "iteration 8000 / 10000: loss 2.295022\n",
      "iteration 8100 / 10000: loss 2.257813\n",
      "iteration 8200 / 10000: loss 2.264326\n",
      "iteration 8300 / 10000: loss 2.289467\n",
      "iteration 8400 / 10000: loss 2.262210\n",
      "iteration 8500 / 10000: loss 2.313396\n",
      "iteration 8600 / 10000: loss 2.286192\n",
      "iteration 8700 / 10000: loss 2.287708\n",
      "iteration 8800 / 10000: loss 2.278165\n",
      "iteration 8900 / 10000: loss 2.279711\n",
      "iteration 9000 / 10000: loss 2.290493\n",
      "iteration 9100 / 10000: loss 2.291097\n",
      "iteration 9200 / 10000: loss 2.275045\n",
      "iteration 9300 / 10000: loss 2.290888\n",
      "iteration 9400 / 10000: loss 2.274076\n",
      "iteration 9500 / 10000: loss 2.295323\n",
      "iteration 9600 / 10000: loss 2.303166\n",
      "iteration 9700 / 10000: loss 2.293451\n",
      "iteration 9800 / 10000: loss 2.276235\n",
      "iteration 9900 / 10000: loss 2.296606\n",
      "Validation accuracy:  0.23\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.597537e-01, regstrength 3.237458e-01 and hidden layer size 21\n",
      "iteration 0 / 10000: loss 2.302596\n",
      "iteration 100 / 10000: loss 2.302380\n",
      "iteration 200 / 10000: loss 2.303254\n",
      "iteration 300 / 10000: loss 2.302491\n",
      "iteration 400 / 10000: loss 2.302653\n",
      "iteration 500 / 10000: loss 2.303661\n",
      "iteration 600 / 10000: loss 2.303050\n",
      "iteration 700 / 10000: loss 2.302964\n",
      "iteration 800 / 10000: loss 2.302145\n",
      "iteration 900 / 10000: loss 2.302905\n",
      "iteration 1000 / 10000: loss 2.303533\n",
      "iteration 1100 / 10000: loss 2.303247\n",
      "iteration 1200 / 10000: loss 2.302655\n",
      "iteration 1300 / 10000: loss 2.301808\n",
      "iteration 1400 / 10000: loss 2.303009\n",
      "iteration 1500 / 10000: loss 2.302624\n",
      "iteration 1600 / 10000: loss 2.304140\n",
      "iteration 1700 / 10000: loss 2.302665\n",
      "iteration 1800 / 10000: loss 2.303014\n",
      "iteration 1900 / 10000: loss 2.302826\n",
      "iteration 2000 / 10000: loss 2.302049\n",
      "iteration 2100 / 10000: loss 2.303738\n",
      "iteration 2200 / 10000: loss 2.302542\n",
      "iteration 2300 / 10000: loss 2.301813\n",
      "iteration 2400 / 10000: loss 2.303050\n",
      "iteration 2500 / 10000: loss 2.303355\n",
      "iteration 2600 / 10000: loss 2.303124\n",
      "iteration 2700 / 10000: loss 2.302524\n",
      "iteration 2800 / 10000: loss 2.302805\n",
      "iteration 2900 / 10000: loss 2.302367\n",
      "iteration 3000 / 10000: loss 2.302405\n",
      "iteration 3100 / 10000: loss 2.302011\n",
      "iteration 3200 / 10000: loss 2.302562\n",
      "iteration 3300 / 10000: loss 2.302519\n",
      "iteration 3400 / 10000: loss 2.301428\n",
      "iteration 3500 / 10000: loss 2.302901\n",
      "iteration 3600 / 10000: loss 2.301973\n",
      "iteration 3700 / 10000: loss 2.302906\n",
      "iteration 3800 / 10000: loss 2.302096\n",
      "iteration 3900 / 10000: loss 2.302612\n",
      "iteration 4000 / 10000: loss 2.302567\n",
      "iteration 4100 / 10000: loss 2.302746\n",
      "iteration 4200 / 10000: loss 2.302202\n",
      "iteration 4300 / 10000: loss 2.303268\n",
      "iteration 4400 / 10000: loss 2.303368\n",
      "iteration 4500 / 10000: loss 2.303005\n",
      "iteration 4600 / 10000: loss 2.302621\n",
      "iteration 4700 / 10000: loss 2.302207\n",
      "iteration 4800 / 10000: loss 2.302090\n",
      "iteration 4900 / 10000: loss 2.303109\n",
      "iteration 5000 / 10000: loss 2.302883\n",
      "iteration 5100 / 10000: loss 2.302715\n",
      "iteration 5200 / 10000: loss 2.302066\n",
      "iteration 5300 / 10000: loss 2.302215\n",
      "iteration 5400 / 10000: loss 2.303048\n",
      "iteration 5500 / 10000: loss 2.302666\n",
      "iteration 5600 / 10000: loss 2.302521\n",
      "iteration 5700 / 10000: loss 2.302116\n",
      "iteration 5800 / 10000: loss 2.302881\n",
      "iteration 5900 / 10000: loss 2.303397\n",
      "iteration 6000 / 10000: loss 2.302345\n",
      "iteration 6100 / 10000: loss 2.302528\n",
      "iteration 6200 / 10000: loss 2.302692\n",
      "iteration 6300 / 10000: loss 2.303067\n",
      "iteration 6400 / 10000: loss 2.302792\n",
      "iteration 6500 / 10000: loss 2.302588\n",
      "iteration 6600 / 10000: loss 2.302644\n",
      "iteration 6700 / 10000: loss 2.302320\n",
      "iteration 6800 / 10000: loss 2.303395\n",
      "iteration 6900 / 10000: loss 2.302610\n",
      "iteration 7000 / 10000: loss 2.302445\n",
      "iteration 7100 / 10000: loss 2.302858\n",
      "iteration 7200 / 10000: loss 2.302485\n",
      "iteration 7300 / 10000: loss 2.302384\n",
      "iteration 7400 / 10000: loss 2.303323\n",
      "iteration 7500 / 10000: loss 2.302387\n",
      "iteration 7600 / 10000: loss 2.302502\n",
      "iteration 7700 / 10000: loss 2.302382\n",
      "iteration 7800 / 10000: loss 2.302841\n",
      "iteration 7900 / 10000: loss 2.302551\n",
      "iteration 8000 / 10000: loss 2.303055\n",
      "iteration 8100 / 10000: loss 2.303100\n",
      "iteration 8200 / 10000: loss 2.302744\n",
      "iteration 8300 / 10000: loss 2.302941\n",
      "iteration 8400 / 10000: loss 2.301791\n",
      "iteration 8500 / 10000: loss 2.302450\n",
      "iteration 8600 / 10000: loss 2.302756\n",
      "iteration 8700 / 10000: loss 2.302483\n",
      "iteration 8800 / 10000: loss 2.303355\n",
      "iteration 8900 / 10000: loss 2.302343\n",
      "iteration 9000 / 10000: loss 2.302682\n",
      "iteration 9100 / 10000: loss 2.302820\n",
      "iteration 9200 / 10000: loss 2.302679\n",
      "iteration 9300 / 10000: loss 2.302876\n",
      "iteration 9400 / 10000: loss 2.302761\n",
      "iteration 9500 / 10000: loss 2.302377\n",
      "iteration 9600 / 10000: loss 2.302618\n",
      "iteration 9700 / 10000: loss 2.302760\n",
      "iteration 9800 / 10000: loss 2.302595\n",
      "iteration 9900 / 10000: loss 2.302743\n",
      "Validation accuracy:  0.112\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.176773e-01, regstrength 6.866488e-02 and hidden layer size 29\n",
      "iteration 0 / 10000: loss 2.302588\n",
      "iteration 100 / 10000: loss 2.302862\n",
      "iteration 200 / 10000: loss 2.301661\n",
      "iteration 300 / 10000: loss 2.259475\n",
      "iteration 400 / 10000: loss 2.163421\n",
      "iteration 500 / 10000: loss 2.195681\n",
      "iteration 600 / 10000: loss 2.135787\n",
      "iteration 700 / 10000: loss 2.199219\n",
      "iteration 800 / 10000: loss 2.152258\n",
      "iteration 900 / 10000: loss 2.162081\n",
      "iteration 1000 / 10000: loss 2.165038\n",
      "iteration 1100 / 10000: loss 2.087351\n",
      "iteration 1200 / 10000: loss 2.078766\n",
      "iteration 1300 / 10000: loss 2.139455\n",
      "iteration 1400 / 10000: loss 2.158627\n",
      "iteration 1500 / 10000: loss 2.148469\n",
      "iteration 1600 / 10000: loss 2.150718\n",
      "iteration 1700 / 10000: loss 2.107208\n",
      "iteration 1800 / 10000: loss 2.156443\n",
      "iteration 1900 / 10000: loss 2.142081\n",
      "iteration 2000 / 10000: loss 2.175098\n",
      "iteration 2100 / 10000: loss 2.104597\n",
      "iteration 2200 / 10000: loss 2.167369\n",
      "iteration 2300 / 10000: loss 2.178850\n",
      "iteration 2400 / 10000: loss 2.159470\n",
      "iteration 2500 / 10000: loss 2.217032\n",
      "iteration 2600 / 10000: loss 2.111179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2700 / 10000: loss 2.167112\n",
      "iteration 2800 / 10000: loss 2.168536\n",
      "iteration 2900 / 10000: loss 2.126284\n",
      "iteration 3000 / 10000: loss 2.147012\n",
      "iteration 3100 / 10000: loss 2.180487\n",
      "iteration 3200 / 10000: loss 2.124941\n",
      "iteration 3300 / 10000: loss 2.116924\n",
      "iteration 3400 / 10000: loss 2.200227\n",
      "iteration 3500 / 10000: loss 2.123995\n",
      "iteration 3600 / 10000: loss 2.134775\n",
      "iteration 3700 / 10000: loss 2.143416\n",
      "iteration 3800 / 10000: loss 2.094386\n",
      "iteration 3900 / 10000: loss 2.111883\n",
      "iteration 4000 / 10000: loss 2.164228\n",
      "iteration 4100 / 10000: loss 2.131567\n",
      "iteration 4200 / 10000: loss 2.143508\n",
      "iteration 4300 / 10000: loss 2.162927\n",
      "iteration 4400 / 10000: loss 2.089026\n",
      "iteration 4500 / 10000: loss 2.156284\n",
      "iteration 4600 / 10000: loss 2.130044\n",
      "iteration 4700 / 10000: loss 2.088511\n",
      "iteration 4800 / 10000: loss 2.138844\n",
      "iteration 4900 / 10000: loss 2.088553\n",
      "iteration 5000 / 10000: loss 2.228031\n",
      "iteration 5100 / 10000: loss 2.184834\n",
      "iteration 5200 / 10000: loss 2.162524\n",
      "iteration 5300 / 10000: loss 2.183714\n",
      "iteration 5400 / 10000: loss 2.189255\n",
      "iteration 5500 / 10000: loss 2.180955\n",
      "iteration 5600 / 10000: loss 2.132886\n",
      "iteration 5700 / 10000: loss 2.138624\n",
      "iteration 5800 / 10000: loss 2.205806\n",
      "iteration 5900 / 10000: loss 2.170669\n",
      "iteration 6000 / 10000: loss 2.112488\n",
      "iteration 6100 / 10000: loss 2.142397\n",
      "iteration 6200 / 10000: loss 2.096820\n",
      "iteration 6300 / 10000: loss 2.125172\n",
      "iteration 6400 / 10000: loss 2.128706\n",
      "iteration 6500 / 10000: loss 2.181084\n",
      "iteration 6600 / 10000: loss 2.125743\n",
      "iteration 6700 / 10000: loss 2.168912\n",
      "iteration 6800 / 10000: loss 2.153834\n",
      "iteration 6900 / 10000: loss 2.143896\n",
      "iteration 7000 / 10000: loss 2.127529\n",
      "iteration 7100 / 10000: loss 2.137368\n",
      "iteration 7200 / 10000: loss 2.164448\n",
      "iteration 7300 / 10000: loss 2.143062\n",
      "iteration 7400 / 10000: loss 2.197914\n",
      "iteration 7500 / 10000: loss 2.136228\n",
      "iteration 7600 / 10000: loss 2.116577\n",
      "iteration 7700 / 10000: loss 2.174022\n",
      "iteration 7800 / 10000: loss 2.174033\n",
      "iteration 7900 / 10000: loss 2.162630\n",
      "iteration 8000 / 10000: loss 2.142927\n",
      "iteration 8100 / 10000: loss 2.117395\n",
      "iteration 8200 / 10000: loss 2.160341\n",
      "iteration 8300 / 10000: loss 2.130410\n",
      "iteration 8400 / 10000: loss 2.124141\n",
      "iteration 8500 / 10000: loss 2.201328\n",
      "iteration 8600 / 10000: loss 2.176201\n",
      "iteration 8700 / 10000: loss 2.177559\n",
      "iteration 8800 / 10000: loss 2.166459\n",
      "iteration 8900 / 10000: loss 2.133935\n",
      "iteration 9000 / 10000: loss 2.147330\n",
      "iteration 9100 / 10000: loss 2.167215\n",
      "iteration 9200 / 10000: loss 2.110901\n",
      "iteration 9300 / 10000: loss 2.110463\n",
      "iteration 9400 / 10000: loss 2.144000\n",
      "iteration 9500 / 10000: loss 2.105567\n",
      "iteration 9600 / 10000: loss 2.155839\n",
      "iteration 9700 / 10000: loss 2.145290\n",
      "iteration 9800 / 10000: loss 2.138995\n",
      "iteration 9900 / 10000: loss 2.126980\n",
      "Validation accuracy:  0.38\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.106984e-01, regstrength 1.757511e-03 and hidden layer size 25\n",
      "iteration 0 / 10000: loss 2.302585\n",
      "iteration 100 / 10000: loss 2.302727\n",
      "iteration 200 / 10000: loss 2.234767\n",
      "iteration 300 / 10000: loss 1.831971\n",
      "iteration 400 / 10000: loss 1.606062\n",
      "iteration 500 / 10000: loss 1.519179\n",
      "iteration 600 / 10000: loss 1.490972\n",
      "iteration 700 / 10000: loss 1.467873\n",
      "iteration 800 / 10000: loss 1.402205\n",
      "iteration 900 / 10000: loss 1.409868\n",
      "iteration 1000 / 10000: loss 1.508759\n",
      "iteration 1100 / 10000: loss 1.326667\n",
      "iteration 1200 / 10000: loss 1.357474\n",
      "iteration 1300 / 10000: loss 1.433672\n",
      "iteration 1400 / 10000: loss 1.379243\n",
      "iteration 1500 / 10000: loss 1.406486\n",
      "iteration 1600 / 10000: loss 1.353777\n",
      "iteration 1700 / 10000: loss 1.441132\n",
      "iteration 1800 / 10000: loss 1.463616\n",
      "iteration 1900 / 10000: loss 1.426561\n",
      "iteration 2000 / 10000: loss 1.404055\n",
      "iteration 2100 / 10000: loss 1.338186\n",
      "iteration 2200 / 10000: loss 1.269849\n",
      "iteration 2300 / 10000: loss 1.475369\n",
      "iteration 2400 / 10000: loss 1.441366\n",
      "iteration 2500 / 10000: loss 1.325269\n",
      "iteration 2600 / 10000: loss 1.253283\n",
      "iteration 2700 / 10000: loss 1.422482\n",
      "iteration 2800 / 10000: loss 1.349460\n",
      "iteration 2900 / 10000: loss 1.363306\n",
      "iteration 3000 / 10000: loss 1.384485\n",
      "iteration 3100 / 10000: loss 1.383969\n",
      "iteration 3200 / 10000: loss 1.528577\n",
      "iteration 3300 / 10000: loss 1.404759\n",
      "iteration 3400 / 10000: loss 1.321435\n",
      "iteration 3500 / 10000: loss 1.396274\n",
      "iteration 3600 / 10000: loss 1.469192\n",
      "iteration 3700 / 10000: loss 1.292944\n",
      "iteration 3800 / 10000: loss 1.313043\n",
      "iteration 3900 / 10000: loss 1.404017\n",
      "iteration 4000 / 10000: loss 1.312935\n",
      "iteration 4100 / 10000: loss 1.244687\n",
      "iteration 4200 / 10000: loss 1.371833\n",
      "iteration 4300 / 10000: loss 1.526260\n",
      "iteration 4400 / 10000: loss 1.335593\n",
      "iteration 4500 / 10000: loss 1.306357\n",
      "iteration 4600 / 10000: loss 1.365959\n",
      "iteration 4700 / 10000: loss 1.292775\n",
      "iteration 4800 / 10000: loss 1.374622\n",
      "iteration 4900 / 10000: loss 1.240948\n",
      "iteration 5000 / 10000: loss 1.410757\n",
      "iteration 5100 / 10000: loss 1.268256\n",
      "iteration 5200 / 10000: loss 1.281163\n",
      "iteration 5300 / 10000: loss 1.328018\n",
      "iteration 5400 / 10000: loss 1.464533\n",
      "iteration 5500 / 10000: loss 1.219244\n",
      "iteration 5600 / 10000: loss 1.305755\n",
      "iteration 5700 / 10000: loss 1.280849\n",
      "iteration 5800 / 10000: loss 1.359371\n",
      "iteration 5900 / 10000: loss 1.381414\n",
      "iteration 6000 / 10000: loss 1.204703\n",
      "iteration 6100 / 10000: loss 1.081295\n",
      "iteration 6200 / 10000: loss 1.278945\n",
      "iteration 6300 / 10000: loss 1.233111\n",
      "iteration 6400 / 10000: loss 1.431093\n",
      "iteration 6500 / 10000: loss 1.313203\n",
      "iteration 6600 / 10000: loss 1.419809\n",
      "iteration 6700 / 10000: loss 1.362384\n",
      "iteration 6800 / 10000: loss 1.273255\n",
      "iteration 6900 / 10000: loss 1.389352\n",
      "iteration 7000 / 10000: loss 1.209782\n",
      "iteration 7100 / 10000: loss 1.228117\n",
      "iteration 7200 / 10000: loss 1.265441\n",
      "iteration 7300 / 10000: loss 1.201668\n",
      "iteration 7400 / 10000: loss 1.142920\n",
      "iteration 7500 / 10000: loss 1.241496\n",
      "iteration 7600 / 10000: loss 1.226252\n",
      "iteration 7700 / 10000: loss 1.297420\n",
      "iteration 7800 / 10000: loss 1.313297\n",
      "iteration 7900 / 10000: loss 1.339136\n",
      "iteration 8000 / 10000: loss 1.299389\n",
      "iteration 8100 / 10000: loss 1.362717\n",
      "iteration 8200 / 10000: loss 1.294359\n",
      "iteration 8300 / 10000: loss 1.238418\n",
      "iteration 8400 / 10000: loss 1.312115\n",
      "iteration 8500 / 10000: loss 1.354434\n",
      "iteration 8600 / 10000: loss 1.287652\n",
      "iteration 8700 / 10000: loss 1.349807\n",
      "iteration 8800 / 10000: loss 1.146689\n",
      "iteration 8900 / 10000: loss 1.293491\n",
      "iteration 9000 / 10000: loss 1.239445\n",
      "iteration 9100 / 10000: loss 1.302694\n",
      "iteration 9200 / 10000: loss 1.405079\n",
      "iteration 9300 / 10000: loss 1.210500\n",
      "iteration 9400 / 10000: loss 1.273714\n",
      "iteration 9500 / 10000: loss 1.269683\n",
      "iteration 9600 / 10000: loss 1.268010\n",
      "iteration 9700 / 10000: loss 1.299034\n",
      "iteration 9800 / 10000: loss 1.276193\n",
      "iteration 9900 / 10000: loss 1.379181\n",
      "Validation accuracy:  0.541\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.502794e-01, regstrength 4.498433e-02 and hidden layer size 21\n",
      "iteration 0 / 10000: loss 2.302587\n",
      "iteration 100 / 10000: loss 2.303167\n",
      "iteration 200 / 10000: loss 2.228821\n",
      "iteration 300 / 10000: loss 2.078080\n",
      "iteration 400 / 10000: loss 2.069716\n",
      "iteration 500 / 10000: loss 2.012958\n",
      "iteration 600 / 10000: loss 2.049531\n",
      "iteration 700 / 10000: loss 2.007731\n",
      "iteration 800 / 10000: loss 1.964248\n",
      "iteration 900 / 10000: loss 1.977333\n",
      "iteration 1000 / 10000: loss 2.000908\n",
      "iteration 1100 / 10000: loss 1.978974\n",
      "iteration 1200 / 10000: loss 2.012393\n",
      "iteration 1300 / 10000: loss 1.974584\n",
      "iteration 1400 / 10000: loss 2.009985\n",
      "iteration 1500 / 10000: loss 2.017198\n",
      "iteration 1600 / 10000: loss 1.967385\n",
      "iteration 1700 / 10000: loss 2.011640\n",
      "iteration 1800 / 10000: loss 1.973818\n",
      "iteration 1900 / 10000: loss 1.976306\n",
      "iteration 2000 / 10000: loss 2.030858\n",
      "iteration 2100 / 10000: loss 2.047928\n",
      "iteration 2200 / 10000: loss 2.072565\n",
      "iteration 2300 / 10000: loss 2.003012\n",
      "iteration 2400 / 10000: loss 2.004192\n",
      "iteration 2500 / 10000: loss 2.023447\n",
      "iteration 2600 / 10000: loss 1.994822\n",
      "iteration 2700 / 10000: loss 1.976812\n",
      "iteration 2800 / 10000: loss 1.962638\n",
      "iteration 2900 / 10000: loss 1.972970\n",
      "iteration 3000 / 10000: loss 2.052669\n",
      "iteration 3100 / 10000: loss 1.998776\n",
      "iteration 3200 / 10000: loss 1.988137\n",
      "iteration 3300 / 10000: loss 2.017789\n",
      "iteration 3400 / 10000: loss 2.038990\n",
      "iteration 3500 / 10000: loss 2.074478\n",
      "iteration 3600 / 10000: loss 2.043875\n",
      "iteration 3700 / 10000: loss 2.023137\n",
      "iteration 3800 / 10000: loss 1.987955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3900 / 10000: loss 2.019555\n",
      "iteration 4000 / 10000: loss 2.073035\n",
      "iteration 4100 / 10000: loss 1.980300\n",
      "iteration 4200 / 10000: loss 1.996427\n",
      "iteration 4300 / 10000: loss 2.039069\n",
      "iteration 4400 / 10000: loss 2.029131\n",
      "iteration 4500 / 10000: loss 2.020031\n",
      "iteration 4600 / 10000: loss 2.046646\n",
      "iteration 4700 / 10000: loss 1.954485\n",
      "iteration 4800 / 10000: loss 1.996005\n",
      "iteration 4900 / 10000: loss 2.049847\n",
      "iteration 5000 / 10000: loss 2.053161\n",
      "iteration 5100 / 10000: loss 1.968366\n",
      "iteration 5200 / 10000: loss 1.954269\n",
      "iteration 5300 / 10000: loss 2.031413\n",
      "iteration 5400 / 10000: loss 1.977640\n",
      "iteration 5500 / 10000: loss 2.092070\n",
      "iteration 5600 / 10000: loss 2.067812\n",
      "iteration 5700 / 10000: loss 2.045894\n",
      "iteration 5800 / 10000: loss 2.006849\n",
      "iteration 5900 / 10000: loss 1.993435\n",
      "iteration 6000 / 10000: loss 2.042986\n",
      "iteration 6100 / 10000: loss 1.986215\n",
      "iteration 6200 / 10000: loss 1.963191\n",
      "iteration 6300 / 10000: loss 1.982720\n",
      "iteration 6400 / 10000: loss 1.963481\n",
      "iteration 6500 / 10000: loss 2.001998\n",
      "iteration 6600 / 10000: loss 2.031722\n",
      "iteration 6700 / 10000: loss 1.930236\n",
      "iteration 6800 / 10000: loss 2.000473\n",
      "iteration 6900 / 10000: loss 2.000131\n",
      "iteration 7000 / 10000: loss 2.007891\n",
      "iteration 7100 / 10000: loss 1.966855\n",
      "iteration 7200 / 10000: loss 1.989871\n",
      "iteration 7300 / 10000: loss 2.061225\n",
      "iteration 7400 / 10000: loss 2.088022\n",
      "iteration 7500 / 10000: loss 1.920398\n",
      "iteration 7600 / 10000: loss 2.003878\n",
      "iteration 7700 / 10000: loss 2.006695\n",
      "iteration 7800 / 10000: loss 2.004383\n",
      "iteration 7900 / 10000: loss 1.979164\n",
      "iteration 8000 / 10000: loss 1.945099\n",
      "iteration 8100 / 10000: loss 1.979530\n",
      "iteration 8200 / 10000: loss 2.025151\n",
      "iteration 8300 / 10000: loss 2.033282\n",
      "iteration 8400 / 10000: loss 2.064440\n",
      "iteration 8500 / 10000: loss 1.973339\n",
      "iteration 8600 / 10000: loss 2.062514\n",
      "iteration 8700 / 10000: loss 2.022594\n",
      "iteration 8800 / 10000: loss 2.074046\n",
      "iteration 8900 / 10000: loss 1.960192\n",
      "iteration 9000 / 10000: loss 2.024686\n",
      "iteration 9100 / 10000: loss 2.025741\n",
      "iteration 9200 / 10000: loss 2.034129\n",
      "iteration 9300 / 10000: loss 2.009951\n",
      "iteration 9400 / 10000: loss 2.046693\n",
      "iteration 9500 / 10000: loss 1.939961\n",
      "iteration 9600 / 10000: loss 1.959589\n",
      "iteration 9700 / 10000: loss 1.984196\n",
      "iteration 9800 / 10000: loss 1.970823\n",
      "iteration 9900 / 10000: loss 1.974449\n",
      "Validation accuracy:  0.451\n",
      "\n",
      "\n",
      "Starting iteration with rate 2.944200e-01, regstrength 9.102982e-02 and hidden layer size 46\n",
      "iteration 0 / 10000: loss 2.302592\n",
      "iteration 100 / 10000: loss 2.300680\n",
      "iteration 200 / 10000: loss 2.273989\n",
      "iteration 300 / 10000: loss 2.170119\n",
      "iteration 400 / 10000: loss 2.231319\n",
      "iteration 500 / 10000: loss 2.246659\n",
      "iteration 600 / 10000: loss 2.240325\n",
      "iteration 700 / 10000: loss 2.227617\n",
      "iteration 800 / 10000: loss 2.220506\n",
      "iteration 900 / 10000: loss 2.204403\n",
      "iteration 1000 / 10000: loss 2.246450\n",
      "iteration 1100 / 10000: loss 2.238983\n",
      "iteration 1200 / 10000: loss 2.214977\n",
      "iteration 1300 / 10000: loss 2.200016\n",
      "iteration 1400 / 10000: loss 2.194691\n",
      "iteration 1500 / 10000: loss 2.211039\n",
      "iteration 1600 / 10000: loss 2.216082\n",
      "iteration 1700 / 10000: loss 2.235309\n",
      "iteration 1800 / 10000: loss 2.165794\n",
      "iteration 1900 / 10000: loss 2.200646\n",
      "iteration 2000 / 10000: loss 2.184003\n",
      "iteration 2100 / 10000: loss 2.163093\n",
      "iteration 2200 / 10000: loss 2.235146\n",
      "iteration 2300 / 10000: loss 2.199755\n",
      "iteration 2400 / 10000: loss 2.209984\n",
      "iteration 2500 / 10000: loss 2.222079\n",
      "iteration 2600 / 10000: loss 2.216455\n",
      "iteration 2700 / 10000: loss 2.234809\n",
      "iteration 2800 / 10000: loss 2.229603\n",
      "iteration 2900 / 10000: loss 2.231497\n",
      "iteration 3000 / 10000: loss 2.248997\n",
      "iteration 3100 / 10000: loss 2.183056\n",
      "iteration 3200 / 10000: loss 2.210285\n",
      "iteration 3300 / 10000: loss 2.245385\n",
      "iteration 3400 / 10000: loss 2.228311\n",
      "iteration 3500 / 10000: loss 2.258330\n",
      "iteration 3600 / 10000: loss 2.196898\n",
      "iteration 3700 / 10000: loss 2.200587\n",
      "iteration 3800 / 10000: loss 2.269094\n",
      "iteration 3900 / 10000: loss 2.252678\n",
      "iteration 4000 / 10000: loss 2.262190\n",
      "iteration 4100 / 10000: loss 2.231875\n",
      "iteration 4200 / 10000: loss 2.210066\n",
      "iteration 4300 / 10000: loss 2.222485\n",
      "iteration 4400 / 10000: loss 2.261586\n",
      "iteration 4500 / 10000: loss 2.239377\n",
      "iteration 4600 / 10000: loss 2.224437\n",
      "iteration 4700 / 10000: loss 2.249057\n",
      "iteration 4800 / 10000: loss 2.209964\n",
      "iteration 4900 / 10000: loss 2.226118\n",
      "iteration 5000 / 10000: loss 2.243070\n",
      "iteration 5100 / 10000: loss 2.247251\n",
      "iteration 5200 / 10000: loss 2.245955\n",
      "iteration 5300 / 10000: loss 2.197652\n",
      "iteration 5400 / 10000: loss 2.215161\n",
      "iteration 5500 / 10000: loss 2.167412\n",
      "iteration 5600 / 10000: loss 2.219711\n",
      "iteration 5700 / 10000: loss 2.219058\n",
      "iteration 5800 / 10000: loss 2.248418\n",
      "iteration 5900 / 10000: loss 2.245561\n",
      "iteration 6000 / 10000: loss 2.225321\n",
      "iteration 6100 / 10000: loss 2.198739\n",
      "iteration 6200 / 10000: loss 2.215000\n",
      "iteration 6300 / 10000: loss 2.186456\n",
      "iteration 6400 / 10000: loss 2.250385\n",
      "iteration 6500 / 10000: loss 2.246003\n",
      "iteration 6600 / 10000: loss 2.220742\n",
      "iteration 6700 / 10000: loss 2.199222\n",
      "iteration 6800 / 10000: loss 2.211747\n",
      "iteration 6900 / 10000: loss 2.216872\n",
      "iteration 7000 / 10000: loss 2.212206\n",
      "iteration 7100 / 10000: loss 2.245169\n",
      "iteration 7200 / 10000: loss 2.264029\n",
      "iteration 7300 / 10000: loss 2.209061\n",
      "iteration 7400 / 10000: loss 2.270001\n",
      "iteration 7500 / 10000: loss 2.229590\n",
      "iteration 7600 / 10000: loss 2.210653\n",
      "iteration 7700 / 10000: loss 2.200543\n",
      "iteration 7800 / 10000: loss 2.224937\n",
      "iteration 7900 / 10000: loss 2.194648\n",
      "iteration 8000 / 10000: loss 2.222299\n",
      "iteration 8100 / 10000: loss 2.229793\n",
      "iteration 8200 / 10000: loss 2.184778\n",
      "iteration 8300 / 10000: loss 2.200530\n",
      "iteration 8400 / 10000: loss 2.199063\n",
      "iteration 8500 / 10000: loss 2.189186\n",
      "iteration 8600 / 10000: loss 2.247398\n",
      "iteration 8700 / 10000: loss 2.217897\n",
      "iteration 8800 / 10000: loss 2.250333\n",
      "iteration 8900 / 10000: loss 2.173084\n",
      "iteration 9000 / 10000: loss 2.221703\n",
      "iteration 9100 / 10000: loss 2.202850\n",
      "iteration 9200 / 10000: loss 2.191554\n",
      "iteration 9300 / 10000: loss 2.177683\n",
      "iteration 9400 / 10000: loss 2.209056\n",
      "iteration 9500 / 10000: loss 2.215870\n",
      "iteration 9600 / 10000: loss 2.195723\n",
      "iteration 9700 / 10000: loss 2.192580\n",
      "iteration 9800 / 10000: loss 2.199000\n",
      "iteration 9900 / 10000: loss 2.231123\n",
      "Validation accuracy:  0.323\n",
      "\n",
      "\n",
      "Starting iteration with rate 5.315225e-02, regstrength 4.714866e-03 and hidden layer size 13\n",
      "iteration 0 / 10000: loss 2.302585\n",
      "iteration 100 / 10000: loss 2.302121\n",
      "iteration 200 / 10000: loss 2.303022\n",
      "iteration 300 / 10000: loss 2.300760\n",
      "iteration 400 / 10000: loss 2.277845\n",
      "iteration 500 / 10000: loss 2.088311\n",
      "iteration 600 / 10000: loss 1.954930\n",
      "iteration 700 / 10000: loss 1.838454\n",
      "iteration 800 / 10000: loss 1.737909\n",
      "iteration 900 / 10000: loss 1.713867\n",
      "iteration 1000 / 10000: loss 1.681971\n",
      "iteration 1100 / 10000: loss 1.614890\n",
      "iteration 1200 / 10000: loss 1.546072\n",
      "iteration 1300 / 10000: loss 1.544013\n",
      "iteration 1400 / 10000: loss 1.634858\n",
      "iteration 1500 / 10000: loss 1.549976\n",
      "iteration 1600 / 10000: loss 1.665241\n",
      "iteration 1700 / 10000: loss 1.549913\n",
      "iteration 1800 / 10000: loss 1.444075\n",
      "iteration 1900 / 10000: loss 1.427936\n",
      "iteration 2000 / 10000: loss 1.425826\n",
      "iteration 2100 / 10000: loss 1.472803\n",
      "iteration 2200 / 10000: loss 1.483562\n",
      "iteration 2300 / 10000: loss 1.451594\n",
      "iteration 2400 / 10000: loss 1.415878\n",
      "iteration 2500 / 10000: loss 1.545487\n",
      "iteration 2600 / 10000: loss 1.386037\n",
      "iteration 2700 / 10000: loss 1.380795\n",
      "iteration 2800 / 10000: loss 1.502465\n",
      "iteration 2900 / 10000: loss 1.543876\n",
      "iteration 3000 / 10000: loss 1.500818\n",
      "iteration 3100 / 10000: loss 1.387784\n",
      "iteration 3200 / 10000: loss 1.470116\n",
      "iteration 3300 / 10000: loss 1.437603\n",
      "iteration 3400 / 10000: loss 1.646490\n",
      "iteration 3500 / 10000: loss 1.538815\n",
      "iteration 3600 / 10000: loss 1.449285\n",
      "iteration 3700 / 10000: loss 1.524625\n",
      "iteration 3800 / 10000: loss 1.545554\n",
      "iteration 3900 / 10000: loss 1.600028\n",
      "iteration 4000 / 10000: loss 1.583690\n",
      "iteration 4100 / 10000: loss 1.565885\n",
      "iteration 4200 / 10000: loss 1.489987\n",
      "iteration 4300 / 10000: loss 1.545230\n",
      "iteration 4400 / 10000: loss 1.461099\n",
      "iteration 4500 / 10000: loss 1.356160\n",
      "iteration 4600 / 10000: loss 1.398106\n",
      "iteration 4700 / 10000: loss 1.499893\n",
      "iteration 4800 / 10000: loss 1.490964\n",
      "iteration 4900 / 10000: loss 1.499625\n",
      "iteration 5000 / 10000: loss 1.470881\n",
      "iteration 5100 / 10000: loss 1.443016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5200 / 10000: loss 1.409056\n",
      "iteration 5300 / 10000: loss 1.377305\n",
      "iteration 5400 / 10000: loss 1.456592\n",
      "iteration 5500 / 10000: loss 1.418190\n",
      "iteration 5600 / 10000: loss 1.500392\n",
      "iteration 5700 / 10000: loss 1.547269\n",
      "iteration 5800 / 10000: loss 1.401784\n",
      "iteration 5900 / 10000: loss 1.616673\n",
      "iteration 6000 / 10000: loss 1.542195\n",
      "iteration 6100 / 10000: loss 1.318120\n",
      "iteration 6200 / 10000: loss 1.557149\n",
      "iteration 6300 / 10000: loss 1.418550\n",
      "iteration 6400 / 10000: loss 1.559462\n",
      "iteration 6500 / 10000: loss 1.429915\n",
      "iteration 6600 / 10000: loss 1.480463\n",
      "iteration 6700 / 10000: loss 1.521079\n",
      "iteration 6800 / 10000: loss 1.418369\n",
      "iteration 6900 / 10000: loss 1.541038\n",
      "iteration 7000 / 10000: loss 1.444900\n",
      "iteration 7100 / 10000: loss 1.396795\n",
      "iteration 7200 / 10000: loss 1.317361\n",
      "iteration 7300 / 10000: loss 1.550650\n",
      "iteration 7400 / 10000: loss 1.603707\n",
      "iteration 7500 / 10000: loss 1.472452\n",
      "iteration 7600 / 10000: loss 1.427621\n",
      "iteration 7700 / 10000: loss 1.525243\n",
      "iteration 7800 / 10000: loss 1.382771\n",
      "iteration 7900 / 10000: loss 1.531536\n",
      "iteration 8000 / 10000: loss 1.500264\n",
      "iteration 8100 / 10000: loss 1.534087\n",
      "iteration 8200 / 10000: loss 1.433482\n",
      "iteration 8300 / 10000: loss 1.428842\n",
      "iteration 8400 / 10000: loss 1.379474\n",
      "iteration 8500 / 10000: loss 1.487025\n",
      "iteration 8600 / 10000: loss 1.437959\n",
      "iteration 8700 / 10000: loss 1.496003\n",
      "iteration 8800 / 10000: loss 1.469258\n",
      "iteration 8900 / 10000: loss 1.530362\n",
      "iteration 9000 / 10000: loss 1.489825\n",
      "iteration 9100 / 10000: loss 1.557140\n",
      "iteration 9200 / 10000: loss 1.535731\n",
      "iteration 9300 / 10000: loss 1.459283\n",
      "iteration 9400 / 10000: loss 1.442254\n",
      "iteration 9500 / 10000: loss 1.466478\n",
      "iteration 9600 / 10000: loss 1.539770\n",
      "iteration 9700 / 10000: loss 1.471557\n",
      "iteration 9800 / 10000: loss 1.498230\n",
      "iteration 9900 / 10000: loss 1.406370\n",
      "Validation accuracy:  0.511\n",
      "\n",
      "\n",
      "Starting iteration with rate 1.250963e-01, regstrength 1.842070e-01 and hidden layer size 10\n",
      "iteration 0 / 10000: loss 2.302588\n",
      "iteration 100 / 10000: loss 2.302708\n",
      "iteration 200 / 10000: loss 2.303846\n",
      "iteration 300 / 10000: loss 2.303530\n",
      "iteration 400 / 10000: loss 2.302794\n",
      "iteration 500 / 10000: loss 2.303012\n",
      "iteration 600 / 10000: loss 2.303280\n",
      "iteration 700 / 10000: loss 2.303247\n",
      "iteration 800 / 10000: loss 2.302047\n",
      "iteration 900 / 10000: loss 2.303480\n",
      "iteration 1000 / 10000: loss 2.302387\n",
      "iteration 1100 / 10000: loss 2.302587\n",
      "iteration 1200 / 10000: loss 2.303106\n",
      "iteration 1300 / 10000: loss 2.301702\n",
      "iteration 1400 / 10000: loss 2.302891\n",
      "iteration 1500 / 10000: loss 2.302446\n",
      "iteration 1600 / 10000: loss 2.302710\n",
      "iteration 1700 / 10000: loss 2.302622\n",
      "iteration 1800 / 10000: loss 2.302400\n",
      "iteration 1900 / 10000: loss 2.301781\n",
      "iteration 2000 / 10000: loss 2.302954\n",
      "iteration 2100 / 10000: loss 2.303038\n",
      "iteration 2200 / 10000: loss 2.303310\n",
      "iteration 2300 / 10000: loss 2.302698\n",
      "iteration 2400 / 10000: loss 2.302424\n",
      "iteration 2500 / 10000: loss 2.302273\n",
      "iteration 2600 / 10000: loss 2.302615\n",
      "iteration 2700 / 10000: loss 2.302576\n",
      "iteration 2800 / 10000: loss 2.302435\n",
      "iteration 2900 / 10000: loss 2.303833\n",
      "iteration 3000 / 10000: loss 2.302192\n",
      "iteration 3100 / 10000: loss 2.302994\n",
      "iteration 3200 / 10000: loss 2.302575\n",
      "iteration 3300 / 10000: loss 2.302571\n",
      "iteration 3400 / 10000: loss 2.302771\n",
      "iteration 3500 / 10000: loss 2.302201\n",
      "iteration 3600 / 10000: loss 2.302722\n",
      "iteration 3700 / 10000: loss 2.302343\n",
      "iteration 3800 / 10000: loss 2.302531\n",
      "iteration 3900 / 10000: loss 2.303101\n",
      "iteration 4000 / 10000: loss 2.302817\n",
      "iteration 4100 / 10000: loss 2.301757\n",
      "iteration 4200 / 10000: loss 2.300934\n",
      "iteration 4300 / 10000: loss 2.302206\n",
      "iteration 4400 / 10000: loss 2.302694\n",
      "iteration 4500 / 10000: loss 2.301228\n",
      "iteration 4600 / 10000: loss 2.302431\n",
      "iteration 4700 / 10000: loss 2.301308\n",
      "iteration 4800 / 10000: loss 2.300976\n",
      "iteration 4900 / 10000: loss 2.303518\n",
      "iteration 5000 / 10000: loss 2.300713\n",
      "iteration 5100 / 10000: loss 2.304283\n",
      "iteration 5200 / 10000: loss 2.303264\n",
      "iteration 5300 / 10000: loss 2.303053\n",
      "iteration 5400 / 10000: loss 2.302587\n",
      "iteration 5500 / 10000: loss 2.303363\n",
      "iteration 5600 / 10000: loss 2.301240\n",
      "iteration 5700 / 10000: loss 2.302698\n",
      "iteration 5800 / 10000: loss 2.302841\n",
      "iteration 5900 / 10000: loss 2.302077\n",
      "iteration 6000 / 10000: loss 2.304502\n",
      "iteration 6100 / 10000: loss 2.302606\n",
      "iteration 6200 / 10000: loss 2.303462\n",
      "iteration 6300 / 10000: loss 2.299236\n",
      "iteration 6400 / 10000: loss 2.301816\n",
      "iteration 6500 / 10000: loss 2.307326\n",
      "iteration 6600 / 10000: loss 2.301534\n",
      "iteration 6700 / 10000: loss 2.305428\n",
      "iteration 6800 / 10000: loss 2.297543\n",
      "iteration 6900 / 10000: loss 2.307408\n",
      "iteration 7000 / 10000: loss 2.305819\n",
      "iteration 7100 / 10000: loss 2.302441\n",
      "iteration 7200 / 10000: loss 2.305694\n",
      "iteration 7300 / 10000: loss 2.300066\n",
      "iteration 7400 / 10000: loss 2.304703\n",
      "iteration 7500 / 10000: loss 2.303553\n",
      "iteration 7600 / 10000: loss 2.302934\n",
      "iteration 7700 / 10000: loss 2.299861\n",
      "iteration 7800 / 10000: loss 2.303651\n",
      "iteration 7900 / 10000: loss 2.308605\n",
      "iteration 8000 / 10000: loss 2.296292\n",
      "iteration 8100 / 10000: loss 2.307682\n",
      "iteration 8200 / 10000: loss 2.299160\n",
      "iteration 8300 / 10000: loss 2.300548\n",
      "iteration 8400 / 10000: loss 2.306450\n",
      "iteration 8500 / 10000: loss 2.299190\n",
      "iteration 8600 / 10000: loss 2.304091\n",
      "iteration 8700 / 10000: loss 2.299133\n",
      "iteration 8800 / 10000: loss 2.297589\n",
      "iteration 8900 / 10000: loss 2.305791\n",
      "iteration 9000 / 10000: loss 2.300756\n",
      "iteration 9100 / 10000: loss 2.302396\n",
      "iteration 9200 / 10000: loss 2.301429\n",
      "iteration 9300 / 10000: loss 2.302111\n",
      "iteration 9400 / 10000: loss 2.304655\n",
      "iteration 9500 / 10000: loss 2.304561\n",
      "iteration 9600 / 10000: loss 2.296045\n",
      "iteration 9700 / 10000: loss 2.305534\n",
      "iteration 9800 / 10000: loss 2.305328\n",
      "iteration 9900 / 10000: loss 2.300895\n",
      "Validation accuracy:  0.161\n",
      "\n",
      "\n",
      "Starting iteration with rate 4.801538e-01, regstrength 2.120951e-01 and hidden layer size 15\n",
      "iteration 0 / 10000: loss 2.302590\n",
      "iteration 100 / 10000: loss 2.302371\n",
      "iteration 200 / 10000: loss 2.305213\n",
      "iteration 300 / 10000: loss 2.304635\n",
      "iteration 400 / 10000: loss 2.304166\n",
      "iteration 500 / 10000: loss 2.302986\n",
      "iteration 600 / 10000: loss 2.304661\n",
      "iteration 700 / 10000: loss 2.303539\n",
      "iteration 800 / 10000: loss 2.302458\n",
      "iteration 900 / 10000: loss 2.302196\n",
      "iteration 1000 / 10000: loss 2.306063\n",
      "iteration 1100 / 10000: loss 2.303290\n",
      "iteration 1200 / 10000: loss 2.304629\n",
      "iteration 1300 / 10000: loss 2.302647\n",
      "iteration 1400 / 10000: loss 2.305211\n",
      "iteration 1500 / 10000: loss 2.304076\n",
      "iteration 1600 / 10000: loss 2.303888\n",
      "iteration 1700 / 10000: loss 2.302506\n",
      "iteration 1800 / 10000: loss 2.302523\n",
      "iteration 1900 / 10000: loss 2.303288\n",
      "iteration 2000 / 10000: loss 2.304316\n",
      "iteration 2100 / 10000: loss 2.303351\n",
      "iteration 2200 / 10000: loss 2.304605\n",
      "iteration 2300 / 10000: loss 2.302715\n",
      "iteration 2400 / 10000: loss 2.304742\n",
      "iteration 2500 / 10000: loss 2.302871\n",
      "iteration 2600 / 10000: loss 2.302673\n",
      "iteration 2700 / 10000: loss 2.302471\n",
      "iteration 2800 / 10000: loss 2.302934\n",
      "iteration 2900 / 10000: loss 2.303321\n",
      "iteration 3000 / 10000: loss 2.302101\n",
      "iteration 3100 / 10000: loss 2.303765\n",
      "iteration 3200 / 10000: loss 2.302825\n",
      "iteration 3300 / 10000: loss 2.304716\n",
      "iteration 3400 / 10000: loss 2.302033\n",
      "iteration 3500 / 10000: loss 2.300960\n",
      "iteration 3600 / 10000: loss 2.303182\n",
      "iteration 3700 / 10000: loss 2.303633\n",
      "iteration 3800 / 10000: loss 2.302989\n",
      "iteration 3900 / 10000: loss 2.303343\n",
      "iteration 4000 / 10000: loss 2.303650\n",
      "iteration 4100 / 10000: loss 2.301826\n",
      "iteration 4200 / 10000: loss 2.302817\n",
      "iteration 4300 / 10000: loss 2.303472\n",
      "iteration 4400 / 10000: loss 2.302706\n",
      "iteration 4500 / 10000: loss 2.301904\n",
      "iteration 4600 / 10000: loss 2.304068\n",
      "iteration 4700 / 10000: loss 2.303019\n",
      "iteration 4800 / 10000: loss 2.302321\n",
      "iteration 4900 / 10000: loss 2.304129\n",
      "iteration 5000 / 10000: loss 2.303806\n",
      "iteration 5100 / 10000: loss 2.302618\n",
      "iteration 5200 / 10000: loss 2.303310\n",
      "iteration 5300 / 10000: loss 2.302886\n",
      "iteration 5400 / 10000: loss 2.301759\n",
      "iteration 5500 / 10000: loss 2.302828\n",
      "iteration 5600 / 10000: loss 2.302240\n",
      "iteration 5700 / 10000: loss 2.303444\n",
      "iteration 5800 / 10000: loss 2.303120\n",
      "iteration 5900 / 10000: loss 2.303829\n",
      "iteration 6000 / 10000: loss 2.303080\n",
      "iteration 6100 / 10000: loss 2.302083\n",
      "iteration 6200 / 10000: loss 2.303299\n",
      "iteration 6300 / 10000: loss 2.303457\n",
      "iteration 6400 / 10000: loss 2.301105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6500 / 10000: loss 2.302040\n",
      "iteration 6600 / 10000: loss 2.302126\n",
      "iteration 6700 / 10000: loss 2.303502\n",
      "iteration 6800 / 10000: loss 2.303855\n",
      "iteration 6900 / 10000: loss 2.302974\n",
      "iteration 7000 / 10000: loss 2.302282\n",
      "iteration 7100 / 10000: loss 2.304099\n",
      "iteration 7200 / 10000: loss 2.304072\n",
      "iteration 7300 / 10000: loss 2.303150\n",
      "iteration 7400 / 10000: loss 2.302366\n",
      "iteration 7500 / 10000: loss 2.303002\n",
      "iteration 7600 / 10000: loss 2.303157\n",
      "iteration 7700 / 10000: loss 2.302601\n",
      "iteration 7800 / 10000: loss 2.302107\n",
      "iteration 7900 / 10000: loss 2.302781\n",
      "iteration 8000 / 10000: loss 2.304017\n",
      "iteration 8100 / 10000: loss 2.301678\n",
      "iteration 8200 / 10000: loss 2.302527\n",
      "iteration 8300 / 10000: loss 2.302459\n",
      "iteration 8400 / 10000: loss 2.303277\n",
      "iteration 8500 / 10000: loss 2.302137\n",
      "iteration 8600 / 10000: loss 2.302753\n",
      "iteration 8700 / 10000: loss 2.301985\n",
      "iteration 8800 / 10000: loss 2.302649\n",
      "iteration 8900 / 10000: loss 2.302600\n",
      "iteration 9000 / 10000: loss 2.303211\n",
      "iteration 9100 / 10000: loss 2.303259\n",
      "iteration 9200 / 10000: loss 2.302533\n",
      "iteration 9300 / 10000: loss 2.303189\n",
      "iteration 9400 / 10000: loss 2.302462\n",
      "iteration 9500 / 10000: loss 2.301920\n",
      "iteration 9600 / 10000: loss 2.302512\n",
      "iteration 9700 / 10000: loss 2.303039\n",
      "iteration 9800 / 10000: loss 2.302611\n",
      "iteration 9900 / 10000: loss 2.302701\n",
      "Validation accuracy:  0.119\n",
      "\n",
      "\n",
      "Starting iteration with rate 3.996921e-01, regstrength 4.094915e-03 and hidden layer size 18\n",
      "iteration 0 / 10000: loss 2.302585\n",
      "iteration 100 / 10000: loss 1.830274\n",
      "iteration 200 / 10000: loss 1.450056\n",
      "iteration 300 / 10000: loss 1.534385\n",
      "iteration 400 / 10000: loss 1.454234\n",
      "iteration 500 / 10000: loss 1.403609\n",
      "iteration 600 / 10000: loss 1.598287\n",
      "iteration 700 / 10000: loss 1.505224\n",
      "iteration 800 / 10000: loss 1.522863\n",
      "iteration 900 / 10000: loss 1.502335\n",
      "iteration 1000 / 10000: loss 1.494597\n",
      "iteration 1100 / 10000: loss 1.550525\n",
      "iteration 1200 / 10000: loss 1.561364\n",
      "iteration 1300 / 10000: loss 1.458538\n",
      "iteration 1400 / 10000: loss 1.477051\n",
      "iteration 1500 / 10000: loss 1.450747\n",
      "iteration 1600 / 10000: loss 1.459501\n",
      "iteration 1700 / 10000: loss 1.514160\n",
      "iteration 1800 / 10000: loss 1.382805\n",
      "iteration 1900 / 10000: loss 1.545830\n",
      "iteration 2000 / 10000: loss 1.375528\n",
      "iteration 2100 / 10000: loss 1.377376\n",
      "iteration 2200 / 10000: loss 1.440105\n",
      "iteration 2300 / 10000: loss 1.485138\n",
      "iteration 2400 / 10000: loss 1.369524\n",
      "iteration 2500 / 10000: loss 1.490226\n",
      "iteration 2600 / 10000: loss 1.365691\n",
      "iteration 2700 / 10000: loss 1.451250\n",
      "iteration 2800 / 10000: loss 1.483741\n",
      "iteration 2900 / 10000: loss 1.423632\n",
      "iteration 3000 / 10000: loss 1.431798\n",
      "iteration 3100 / 10000: loss 1.456967\n",
      "iteration 3200 / 10000: loss 1.461741\n",
      "iteration 3300 / 10000: loss 1.371055\n",
      "iteration 3400 / 10000: loss 1.461072\n",
      "iteration 3500 / 10000: loss 1.399560\n",
      "iteration 3600 / 10000: loss 1.418133\n",
      "iteration 3700 / 10000: loss 1.466928\n",
      "iteration 3800 / 10000: loss 1.423296\n",
      "iteration 3900 / 10000: loss 1.456263\n",
      "iteration 4000 / 10000: loss 1.539992\n",
      "iteration 4100 / 10000: loss 1.612493\n",
      "iteration 4200 / 10000: loss 1.464185\n",
      "iteration 4300 / 10000: loss 1.432075\n",
      "iteration 4400 / 10000: loss 1.447047\n",
      "iteration 4500 / 10000: loss 1.454182\n",
      "iteration 4600 / 10000: loss 1.309288\n",
      "iteration 4700 / 10000: loss 1.381018\n",
      "iteration 4800 / 10000: loss 1.524451\n",
      "iteration 4900 / 10000: loss 1.569953\n",
      "iteration 5000 / 10000: loss 1.519032\n",
      "iteration 5100 / 10000: loss 1.536022\n",
      "iteration 5200 / 10000: loss 1.394732\n",
      "iteration 5300 / 10000: loss 1.506248\n",
      "iteration 5400 / 10000: loss 1.382168\n",
      "iteration 5500 / 10000: loss 1.384875\n",
      "iteration 5600 / 10000: loss 1.349912\n",
      "iteration 5700 / 10000: loss 1.512370\n",
      "iteration 5800 / 10000: loss 1.398196\n",
      "iteration 5900 / 10000: loss 1.504778\n",
      "iteration 6000 / 10000: loss 1.453433\n",
      "iteration 6100 / 10000: loss 1.440988\n",
      "iteration 6200 / 10000: loss 1.486923\n",
      "iteration 6300 / 10000: loss 1.373232\n",
      "iteration 6400 / 10000: loss 1.427600\n",
      "iteration 6500 / 10000: loss 1.525438\n",
      "iteration 6600 / 10000: loss 1.426277\n",
      "iteration 6700 / 10000: loss 1.324274\n",
      "iteration 6800 / 10000: loss 1.461637\n",
      "iteration 6900 / 10000: loss 1.387940\n",
      "iteration 7000 / 10000: loss 1.441550\n",
      "iteration 7100 / 10000: loss 1.290432\n",
      "iteration 7200 / 10000: loss 1.440995\n",
      "iteration 7300 / 10000: loss 1.388657\n",
      "iteration 7400 / 10000: loss 1.429108\n",
      "iteration 7500 / 10000: loss 1.369335\n",
      "iteration 7600 / 10000: loss 1.412181\n",
      "iteration 7700 / 10000: loss 1.292850\n",
      "iteration 7800 / 10000: loss 1.353098\n",
      "iteration 7900 / 10000: loss 1.445617\n",
      "iteration 8000 / 10000: loss 1.399881\n",
      "iteration 8100 / 10000: loss 1.545839\n",
      "iteration 8200 / 10000: loss 1.473048\n",
      "iteration 8300 / 10000: loss 1.471253\n",
      "iteration 8400 / 10000: loss 1.458932\n",
      "iteration 8500 / 10000: loss 1.515136\n",
      "iteration 8600 / 10000: loss 1.419922\n",
      "iteration 8700 / 10000: loss 1.484964\n",
      "iteration 8800 / 10000: loss 1.467648\n",
      "iteration 8900 / 10000: loss 1.489116\n",
      "iteration 9000 / 10000: loss 1.402278\n",
      "iteration 9100 / 10000: loss 1.403235\n",
      "iteration 9200 / 10000: loss 1.479393\n",
      "iteration 9300 / 10000: loss 1.525697\n",
      "iteration 9400 / 10000: loss 1.348034\n",
      "iteration 9500 / 10000: loss 1.409370\n",
      "iteration 9600 / 10000: loss 1.383890\n",
      "iteration 9700 / 10000: loss 1.321371\n",
      "iteration 9800 / 10000: loss 1.406048\n",
      "iteration 9900 / 10000: loss 1.397958\n",
      "Validation accuracy:  0.544\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from cs231n.classifiers.neural_net import TwoLayerNet\n",
    "\n",
    "input_dim = X_train_feats.shape[1]\n",
    "hidden_dim = 500\n",
    "num_classes = 10\n",
    "\n",
    "net = TwoLayerNet(input_dim, hidden_dim, num_classes)\n",
    "best_net = None\n",
    "best_val = 0\n",
    "\n",
    "################################################################################\n",
    "# TODO: Train a two-layer neural network on image features. You may want to    #\n",
    "# cross-validate various parameters as in previous sections. Store your best   #\n",
    "# model in the best_net variable.                                              #\n",
    "################################################################################\n",
    "\n",
    "num_params=50\n",
    "\n",
    "learning_rates = np.log10([5e-2, 1e0])\n",
    "regularization_strengths = np.log10([0.001, 1])\n",
    "hidden_size = np.log10([10, 50])\n",
    "\n",
    "range_lr  = np.logspace(learning_rates[0],learning_rates[1],num_params)\n",
    "range_reg = np.logspace(regularization_strengths[0],regularization_strengths[1],num_params)\n",
    "range_hid = np.logspace(hidden_size[0], hidden_size[1], num_params).astype('int32')\n",
    "\n",
    "range_lr = range_lr[np.random.permutation(num_params)]\n",
    "range_reg = range_reg[np.random.permutation(num_params)]\n",
    "range_hid = range_hid[np.random.permutation(num_params)]\n",
    "\n",
    "for learning_rate, regularization_strength, nhidden in zip(range_lr, range_reg, range_hid):\n",
    "    net = TwoLayerNet(input_dim, nhidden, num_classes)\n",
    "    print(\"Starting iteration with rate %e, regstrength %e and hidden layer size %i\"%(learning_rate,regularization_strength,nhidden))\n",
    "    \n",
    "    # Train the network\n",
    "    stats = net.train(X_train_feats, y_train, X_val_feats, y_val,\n",
    "                num_iters=10000, batch_size=200,\n",
    "                learning_rate=learning_rate, learning_rate_decay=0.95,\n",
    "                reg=regularization_strength, verbose=True, num_epoch=20)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    train_acc = stats['train_acc_history'][-1]\n",
    "    val_acc = (net.predict(X_val_feats) == y_val).mean()\n",
    "    print('Validation accuracy: ', val_acc)\n",
    "    print('\\n')\n",
    "    \n",
    "    results[(learning_rate,regularization_strength)]=(train_acc,val_acc)\n",
    "    if val_acc > best_val:\n",
    "        print(\"prev best val: %e, next best val: %e\"%(best_val, val_acc))\n",
    "        best_val = val_acc\n",
    "        best_net = net\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.558\n"
     ]
    }
   ],
   "source": [
    "# Run your neural net classifier on the test set. You should be able to\n",
    "# get more than 55% accuracy.\n",
    "\n",
    "test_acc = (best_net.predict(X_test_feats) == y_test).mean()\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Design your own features!\n",
    "\n",
    "You have seen that simple image features can improve classification performance. So far we have tried HOG and color histograms, but other types of features may be able to achieve even better classification performance.\n",
    "\n",
    "For bonus points, design and implement a new type of feature and use it for image classification on CIFAR-10. Explain how your feature works and why you expect it to be useful for image classification. Implement it in this notebook, cross-validate any hyperparameters, and compare its performance to the HOG + Color histogram baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Do something extra!\n",
    "Use the material and code we have presented in this assignment to do something interesting. Was there another question we should have asked? Did any cool ideas pop into your head as you were working on the assignment? This is your chance to show off!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
